{
    "language_name": "English",
    "ui": {
        "sidebar": {
            "training": "Training",
            "console": "Console",
            "metadata": "Metadata",
            "updates": "Updates",
            "system": "System",
            "connecting": "Connecting...",
            "language": "Language"
        },
        "updates": {
            "title": "System Updates",
            "subtitle": "Keep your Onika installation up to date",
            "check_status_default": "Check for updates to see available changes.",
            "btn_check": "Check for Updates",
            "btn_checking": "Checking...",
            "btn_apply": "Apply Updates",
            "btn_applying": "Applying...",
            "details_title": "Update Details",
            "col_file": "File Path",
            "col_status": "Status",
            "log_title": "Update Log",
            "status_uptodate": "Your system is up to date.",
            "status_failed": "Failed to check for updates.",
            "status_error": "Error connecting to update server.",
            "status_found": "Found {count} update(s) available.",
            "checked_at": "Checked at: {time}",
            "confirm_apply": "Are you sure you want to apply {count} update(s)?",
            "apply_success": "Updates applied successfully!",
            "apply_partial": "Updates completed with some errors.",
            "apply_failed": "Failed to apply updates.",
            "apply_error": "Error applying updates."
        },
        "training": {
            "title": "Training",
            "subtitle": "Configure and start your LoRA/LyCORIS training",
            "preset": {
                "label": "Preset",
                "select_placeholder": "Select Preset...",
                "btn_load": "Load",
                "btn_save": "Save Preset",
                "btn_new": "New Preset",
                "btn_delete": "Delete",
                "help": "Loads a preset from <span class=\"mono\">presets/</span> and applies it to the form. Presets are a fast way to switch profiles. Save Preset overwrites the selected preset. New Preset creates a new one with a custom name. Delete removes the selected preset."
            },
            "tabs": {
                "model": "Model",
                "network": "Network",
                "dataset": "Dataset",
                "text_encoder": "Text Encoder",
                "aug": "Augmentation \u0026 Caching",
                "learning": "Learning",
                "advanced": "Advanced",
                "samples": "Samples",
                "metadata": "Metadata"
            },
            "model": {
                "base_model": "Base Model",
                "base_model_help": "The foundation model used for training. Selecting a model that closely matches your target style improves convergence speed. Ensure the architecture matches your training settings (e.g., SDXL base for SDXL training). Training on a base model like SDXL 1.0 is generally more stable than using a heavily fine-tuned \"baked\" model.",
                "architecture": "Model Architecture",
                "architecture_help": "Specifies the architecture logic (SDXL, SD1.5, Flux) required for weight loading and latent processing. Selecting an architecture that does not match the base model will result in shape mismatch errors.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-Optimize for My Hardware",
                "optimize_help": "Automatically adjusts precision, quantization, and memory settings based on your detected GPU VRAM.",
                "btn_adjust": "Auto Adjust (Dataset-Aware)",
                "adjust_help": "Analyzes your dataset (image count/sizes + captions) and recommends stable training settings for your selected architecture. Applies changes like a preset.",
                "quantization": "Model Quantization (Q-LoRA)",
                "quantization_none": "None (Standard fp16/bf16)",
                "quantization_8bit": "8-bit (Low VRAM)",
                "quantization_4bit": "4-bit (Extreme Low VRAM)",
                "quantization_help": "Reduces model precision to 8-bit or 4-bit to significantly lower VRAM requirements, enabling training on hardware with as little as 8GB VRAM. Note that lower precision may slightly impact accuracy and requires the `bitsandbytes` library.",
                "output_name": "Output Name",
                "output_name_placeholder": "e.g. character_lora_v1",
                "output_name_help": "The filename prefix for saved LoRA adapters. Use unique names to organize different versions and prevent overwriting previous results.",
                "output_dir": "Output Directory",
                "output_dir_help": "The destination folder for training results. Ensure sufficient disk space is available to prevent training interruptions.",
                "save_precision": "Save Precision",
                "save_precision_help": "The bit depth for saved model files. `float16` is the standard for a balance of size and precision, while `float32` provides maximum fidelity at the cost of significantly larger file sizes.",
                "save_precision_auto": "AUTO",
                "save_precision_fp16": "float16",
                "save_precision_bf16": "bf16",
                "save_precision_fp32": "float32",
                "save_format": "Save Format",
                "save_format_help": "The file format for the output. `safetensors` is recommended for its security and loading speed. `ckpt` is a legacy format, and `diffusers` saves the model as a directory structure.",
                "save_format_safetensors": "safetensors",
                "save_format_ckpt": "ckpt",
                "save_format_diffusers": "diffusers",
                "save_epochs": "Save Every N Epochs",
                "save_epochs_help": "Frequency of checkpoint saves during training. Regular saving allows for recovery from crashes and provides multiple versions to evaluate for overfitting.",
                "save_steps": "Save Every N Steps",
                "save_steps_placeholder": "Optional",
                "save_steps_help": "Alternative frequency control based on training steps rather than epochs.",
                "resume": "Resume from Checkpoint",
                "resume_placeholder": "e.g. latest or path/to/checkpoint-1000",
                "resume_help": "Continues training from a previously saved state. Note that changing core parameters like learning rate or rank when resuming can lead to training instability.",
                "save_best": "Save Only Best Models (Lowest Loss)",
                "save_best_help": "Retains only the checkpoint with the lowest recorded loss to save disk space. Note that the lowest loss does not always correlate with the best visual quality.",
                "checkpoints_limit": "Checkpoints Total Limit",
                "checkpoints_limit_placeholder": "Optional",
                "checkpoints_limit_help": "The maximum number of checkpoints to retain. Older checkpoints are automatically deleted to manage disk usage."
            },
            "network": {
                "title": "Network Settings",
                "type": "Network Type",
                "type_help": "The architecture of the adapter. LoRA is the industry standard. LoHa and LoKr offer higher expressiveness but may require more careful tuning. OFT is designed to preserve hypersphere energy, which is particularly effective for Flux models.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonal Finetuning)",
                "module": "Network Module",
                "module_help": "The internal Python module used for the adapter. This is an advanced setting and should generally not be modified.",
                "dim": "Network Dim (Rank)",
                "dim_help": "The capacity of the adapter. Higher values (e.g., 128) allow for more detailed learning but increase the risk of overfitting and result in larger file sizes. Lower values (e.g., 16) generalize better and produce smaller files.",
                "alpha": "Network Alpha",
                "alpha_help": "A scaling factor that prevents weight updates from being too aggressive. A common rule of thumb is to set Alpha to half of the Network Dim for stability, or equal to Dim for stronger effects.",
                "algo": "LyCORIS Algorithm",
                "algo_help": "LyCORIS algorithm variant (only relevant for LyCORIS/LoHa modes). Different algorithms trade off expressiveness vs stability. If you're not sure, start with <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span> and only switch when you can clearly measure a benefit.",
                "conv_dim": "Conv Rank (Dim)",
                "conv_dim_help": "Optional conv adapter rank for LoCon/LyCORIS. Low values add a small convolutional capacity boost; high values increase VRAM and can overfit texture/detail quickly. Leave empty if you're not explicitly using a conv-based method.",
                "conv_alpha": "Conv Alpha",
                "conv_alpha_help": "Optional conv adapter scaling (alpha). Lower alpha makes the conv part gentler; higher alpha makes it bite harder and can destabilize or overshoot. Typically <= conv dim; if conv effects look too strong, reduce alpha before reducing dim.",
                "dora_wd": "DoRA Weight Decay",
                "dora_wd_help": "Weight decay specifically applied to DoRA magnitude vectors.",
                "network_dropout": "Network Dropout",
                "network_dropout_help": "Randomly drops neuron outputs within the adapter to improve robustness.",
                "rank_dropout": "Rank Dropout",
                "rank_dropout_help": "Randomly drops individual rank dimensions as a form of regularization.",
                "module_dropout": "Module Dropout",
                "module_dropout_help": "Randomly disables entire modules during training to prevent overfitting and improve generalization.",
                "lora_blocks": "LoRA Blocks",
                "lora_blocks_help": "Allows targeting specific blocks (e.g., mid blocks) within the model architecture.",
                "lora_layers": "LoRA Layers",
                "lora_layers_help": "Allows targeting specific layers (e.g., attention layers) for surgical fine-tuning.",
                "advanced_lora": "Advanced LoRA Options",
                "lycoris_settings": "LyCORIS Settings",
                "args": "Network Args",
                "args_help": "Additional arguments for the network module, provided as comma-separated key=value pairs. Use this for specialized options like dropout or decomposition. Incorrect arguments may lead to training instability."
            },
            "dataset": {
                "dataset_type": "Dataset Type",
                "dataset_type_hub": "Hugging Face Hub",
                "dataset_type_local": "Local Files",
                "dataset_type_help": "Select the source of your dataset. Hugging Face Hub allows for easy sharing and downloading of datasets. Local Files lets you use files from your device.",
                "path": "Dataset Path",
                "path_help": "The directory containing training images and their corresponding caption files (e.g., .txt). Dataset quality is the most critical factor in training success; ensure captions accurately describe the images.",
                "repo_id": "Repository ID",
                "repo_id_placeholder": "e.g. username/repo",
                "repo_id_help": "The Hugging Face repository ID for downloading datasets. Leave blank to use local files.",
                "local_dir": "Local Directory",
                "local_dir_help": "The folder on your device containing the dataset files. Ensure the folder is accessible and contains the correct data format.",
                "image_folder": "Image Folder",
                "image_folder_help": "The subdirectory within the local directory that contains the image files for training.",
                "annotation_file": "Annotation File",
                "annotation_file_placeholder": "e.g. captions.txt",
                "annotation_file_help": "The file containing annotations or captions for the images. This is used for training models that require text input.",
                "resolution": "Resolution",
                "resolution_help": "The target resolution for training. Higher resolutions capture more detail but require more VRAM. Ensure the resolution is appropriate for the model architecture (e.g., 1024x1024 for SDXL, 512x512 for SD1.5).",
                "batch_size": "Batch Size",
                "batch_size_help": "The number of images processed simultaneously. Higher batch sizes can lead to faster training and smoother gradients but significantly increase VRAM usage.",
                "max_epochs": "Max Epochs",
                "max_epochs_help": "The total number of full passes through the dataset. For LoRA training, 10-20 epochs are typically sufficient. Excessive epochs can lead to overfitting and \"fried\" images.",
                "max_steps": "Max Steps",
                "max_steps_help": "An optional hard limit on the total number of training steps.",
                "max_steps_placeholder": "Optional",
                "bucketing": "Enable Aspect Ratio Bucketing",
                "bucketing_help": "Automatically groups images by aspect ratio to prevent unnecessary cropping and preserve the original composition of your training data.",
                "bucket_steps": "Bucket Resolution Step",
                "bucket_steps_help": "The grid size used for bucket dimensions. 64 is the standard value for most architectures.",
                "min_bucket": "Minimum Bucket Resolution",
                "min_bucket_help": "The minimum allowed resolution for an image bucket. This prevents the use of very small or blurry images during training.",
                "max_bucket": "Maximum Bucket Resolution",
                "max_bucket_help": "The maximum allowed resolution for an image bucket, helping to prevent out-of-memory (OOM) errors on exceptionally large images.",
                "center_crop": "Center Crop (Smart 1:1)",
                "center_crop_help": "If enabled, images that are close to square (e.g. 1210x1280) will be center-cropped to a perfect 1:1 ratio. This is recommended for character training to ensure consistent framing.",
                "no_upscale": "No Upscale",
                "no_upscale_help": "Prevents the upscaling of small images to fit bucket dimensions, avoiding potential artifacts.",
                "dreambooth": "DreamBooth \u0026 Prior Preservation",
                "prior_preservation": "Enable Prior Preservation (Reg Images)",
                "prior_preservation_help": "Uses regularization images to prevent the model from losing its original understanding of the class (e.g., \"person\") while learning a specific instance. This is essential for maintaining the model's general knowledge but increases training time.",
                "num_class_images": "Num Class Images",
                "num_class_images_help": "The target number of regularization images. A common recommendation is 100 images per instance image.",
                "instance_prompt": "Instance Prompt",
                "instance_prompt_help": "The combination of a unique trigger word and a class word (e.g., \"sks person\") that identifies the specific subject being trained.",
                "instance_prompt_placeholder": "e.g. a photo of sks person",
                "class_prompt": "Class Prompt",
                "class_prompt_help": "The generic class word (e.g., \"person\") used to identify regularization images.",
                "class_prompt_placeholder": "e.g. a photo of a person",
                "reg_dir": "Reg Images Directory",
                "reg_dir_help": "The folder containing regularization images, used only when prior preservation is enabled.",
                "reg_dir_placeholder": "Optional",
                "auto_gen": "Auto-generate Reg Images",
                "auto_gen_help": "Automatically generates regularization images if they are not already present in the specified directory.",
                "gen_settings": "Generation Settings",
                "neg_class_prompt": "Negative Class Prompt",
                "neg_class_prompt_help": "Negative prompt used when generating regularization images.",
                "neg_class_prompt_placeholder": "Optional negative prompt",
                "guidance": "Guidance Scale",
                "guidance_help": "CFG scale for generating regularization images.",
                "steps": "Reg Steps",
                "steps_help": "Number of sampling steps for generating regularization images.",
                "scheduler": "Reg Scheduler",
                "scheduler_help": "Sampler used for generating regularization images.",
                "seed": "Reg Seed",
                "seed_help": "Seed for generating regularization images. -1 for random.",
                "sample_warning": "Tip: If both “Sample Every N Steps” and “Sample Every N Epochs” are left empty, Onika will not generate sample images (saves a lot of time). On low and mid-range GPUs, generating samples during training is generally not recommended.",
                "train_val_split": "Train/Val Split",
                "train_val_split_help": "The ratio of training to validation data. A common split is 80% for training and 20% for validation.",
                "shuffle": "DataLoader Shuffle",
                "shuffle_help": "Randomizes the order of images in each epoch to prevent the model from learning the sequence of the dataset.",
                "workers": "DataLoader Workers",
                "workers_help": "The number of CPU threads dedicated to loading and preprocessing data. Higher values can feed the GPU faster but may cause system lag if set too high.",
                "num_workers": "Number of Workers",
                "num_workers_help": "The number of subprocesses to use for data loading. More workers can speed up data loading, but require more system resources.",
                "persistent_workers": "Persistent Data Loader Workers",
                "persistent_workers_help": "Keeps CPU data loading workers active between epochs to improve training speed, at the cost of increased RAM usage.",
                "pin_memory": "Pin Memory",
                "pin_memory_help": "If set, the data loader will copy Tensors into CUDA pinned memory. This can improve performance when transferring data to the GPU."
            },
            "text_encoder": {
                "title": "Text Encoder",
                "ti_title": "Textual Inversion (Pivotal Tuning)",
                "weighting_title": "Caption Weighting",
                "train": "Train Text Encoder",
                "train_help": "Enables training of the text encoder alongside the UNet, which can improve prompt adherence but may reduce the model's flexibility or editability.",
                "clip_skip": "Clip Skip",
                "clip_skip_help": "Skips the top N layers of the CLIP text encoder. SD1.5 typically uses a skip of 1, while SDXL usually uses 0. Incorrect values can lead to poor prompt understanding.",
                "train_ti": "Train Textual Inversion",
                "train_ti_help": "Trains new tokens (Textual Inversion) to add specific words or concepts to the model's vocabulary.",
                "ti_frac": "TI Training Fraction",
                "ti_frac_help": "The percentage of total training epochs during which Textual Inversion training is active.",
                "te_frac": "TE Training Fraction",
                "te_frac_help": "The percentage of total training epochs during which Text Encoder training is active.",
                "emphasis": "Default Emphasis",
                "emphasis_help": "The default multiplier applied to emphasized tags when no explicit weight is provided.",
                "de_emphasis": "Default De-Emphasis",
                "de_emphasis_help": "The default multiplier applied to de-emphasized tags when no explicit weight is provided.",
                "enable_weighted": "Enable Weighted Captions",
                "enable_weighted_help": "Enables the use of weighted syntax (e.g., \"(word:1.1)\") in captions for precise control over the importance of specific terms.",
                "new_tokens": "New Tokens Per Abstraction",
                "new_tokens_help": "The number of vectors assigned to each new token. More vectors can capture more detail but are more difficult to train effectively.",
                "token_abs": "Token Abstraction",
                "token_abs_help": "The placeholder string (e.g., \"TOK\") used to represent the new concept in captions.",
                "train_text_encoder": "Train Text Encoder",
                "train_text_encoder_help": "Whether to train the text encoder alongside the UNet. Recommended for better prompt adherence.",
                "text_encoder_lr": "Text Encoder Learning Rate",
                "text_encoder_lr_help": "The learning rate for the text encoder. Usually lower than the UNet learning rate."
            },
            "aug": {
                "aug_title": "Image Augmentation",
                "enable_augmentation": "Enable Augmentation",
                "enable_augmentation_help": "Enables data augmentation techniques during training to improve model generalization.",
                "aug_mode": "Augmentation Mode",
                "aug_mode_help": "Decides when augmentations kick in during training. <strong>Always:</strong> Every image gets augmented on every step — maximum variety, though it can slow things down a bit. <strong>Per Epoch Only:</strong> New augmentations are rolled once per epoch and stay consistent within that pass. Keeps things stable while still mixing it up between epochs. <strong>Random Probability:</strong> Each image might or might not get augmented on any given step, based on individual settings like <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>. Most of the time, \"Always\" or \"Per Epoch Only\" will do the trick.",
                "color_aug": "Color Aug Strength",
                "color_aug_help": "How aggressively to jitter colors (hue, saturation, brightness) during training. <strong>0.0:</strong> Colors stay exactly as-is — no tampering. <strong>0.3-0.5:</strong> Gentle variation, works great for most datasets and helps the model not memorize exact colors. <strong>0.7-1.0:</strong> Heavy color shifts. Can produce weird combos but really pushes generalization. This is especially handy for character or style LoRAs where you want the model to nail shapes and concepts without getting stuck on particular palettes. Turn it off if color fidelity matters (brand colors, specific outfits, etc.).",
                "flip_aug": "Flip Aug Probability",
                "flip_aug_help": "Chance of flipping each image horizontally. <strong>0.5</strong> = 50/50 coin flip, essentially doubling your dataset with mirrored versions. <strong>0.0</strong> = no flipping whatsoever. <strong>Heads up:</strong> Set this to 0 if your images have: • <strong>Text or logos</strong> (would be backwards) • <strong>Asymmetric stuff</strong> (cars, faces with a distinctive side) • <strong>Directional content</strong> (left vs right hands, specific poses) Works great for symmetric subjects like centered portraits, abstract art, or many anime characters. One of the best tricks against overfitting on small datasets.",
                "random_flip": "Random Flip",
                "random_flip_help": "Randomly flips images horizontally and/or vertically.",
                "crop_scale": "Random Crop Scale",
                "crop_scale_help": "Minimum zoom level for random cropping. <strong>1.0:</strong> No cropping — images stay at full size. <strong>0.8:</strong> Crops anywhere between 80-100%, giving subtle zoom variations. <strong>0.5:</strong> More dramatic — can zoom in up to 2×, showing the model wildly different framings. Cropping teaches the model to handle subjects at various scales and compositions. Super useful if your dataset is all \"perfectly centered headshots\" but you want more flexibility in outputs. Keep at 1.0 if exact framing matters.",
                "rotation_range": "Rotation Range",
                "rotation_range_help": "Randomly rotates images within the specified range (in degrees).",
                "width_shift_range": "Width Shift Range",
                "width_shift_range_help": "Randomly shifts images horizontally within the specified range (as a fraction of total width).",
                "height_shift_range": "Height Shift Range",
                "height_shift_range_help": "Randomly shifts images vertically within the specified range (as a fraction of total height).",
                "shear_range": "Shear Range",
                "shear_range_help": "Randomly applies shearing transformations within the specified range (in degrees).",
                "zoom_range": "Zoom Range",
                "zoom_range_help": "Randomly zooms in or out on images within the specified range.",
                "channel_shift_range": "Channel Shift Range",
                "channel_shift_range_help": "Randomly shifts the color channels of the images.",
                "brightness_range": "Brightness Range",
                "brightness_range_help": "Randomly adjusts the brightness of the images within the specified range.",
                "contrast_range": "Contrast Range",
                "contrast_range_help": "Randomly adjusts the contrast of the images within the specified range.",
                "saturation_range": "Saturation Range",
                "saturation_range_help": "Randomly adjusts the saturation of the images within the specified range.",
                "hue_range": "Hue Range",
                "hue_range_help": "Randomly adjusts the hue of the images within the specified range.",
                "cutout_size": "Cutout Size",
                "cutout_size_help": "The size of the cutout squares for CutOut augmentation. Set to 0 to disable.",
                "grid_mask_size": "Grid Mask Size",
                "grid_mask_size_help": "The size of the grid cells for Grid Mask augmentation. Set to 0 to disable.",
                "random_eraser": "Random Eraser",
                "random_eraser_help": "Applies random erasing augmentation to the images.",
                "mixup_alpha": "Mixup Alpha",
                "mixup_alpha_help": "The alpha parameter for Mixup augmentation. Set to 0 to disable.",
                "cutmix_alpha": "CutMix Alpha",
                "cutmix_alpha_help": "The alpha parameter for CutMix augmentation. Set to 0 to disable.",
                "label_smoothing": "Label Smoothing",
                "label_smoothing_help": "Applies label smoothing to the target labels during training.",
                "random_crop": "Random Crop",
                "random_crop_help": "Randomly crops the images to the specified target size.",
                "resize": "Resize",
                "resize_help": "Resizes the images to the specified target size.",
                "normalize": "Normalize",
                "normalize_help": "Normalizes the images to have zero mean and unit variance.",
                "denormalize": "Denormalize",
                "denormalize_help": "Denormalizes the images to restore original pixel values.",
                "caption_aug_title": "Caption Augmentation",
                "shuffle": "Shuffle Captions",
                "shuffle_help": "Randomizes the order of tags/words in captions each time they're used. Stops the model from memorizing fixed patterns like \"blue eyes always comes before long hair\" — instead, it learns that these tags can appear anywhere. Perfect for booru-style comma-separated tags where order is meaningless. Use <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a> if you need the trigger word to stay put. <strong>Don't use this</strong> for natural language captions where word order carries meaning (\"woman holding cat\" ≠ \"cat holding woman\").",
                "keep_tokens": "Keep Tokens",
                "keep_tokens_help": "How many tokens at the start of each caption are \"sacred\" and won't get shuffled. <strong>0:</strong> Everything's fair game for shuffling. <strong>1:</strong> First token (usually your trigger word) always stays first. <strong>2-3:</strong> Protects \"trigger, character_name\" style patterns. If your captions look like \"mytrigger, blonde hair, blue eyes, ...\", setting this to 1 keeps \"mytrigger\" anchored at the front while the rest gets randomized. Helps the model lock onto the trigger-concept association without memorizing exact tag orders. Only matters when <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a> is enabled.",
                "dropout": "Dropout Rate",
                "dropout_help": "Probability of completely dropping an image's caption (replacing it with nothing). When captions vanish, the model has to figure out the image purely from visuals — this trains unconditional generation. <strong>0.0:</strong> Captions always present. <strong>0.05-0.1:</strong> Light dropout, gently improves what happens when users give empty or weak prompts. <strong>0.15-0.2:</strong> More aggressive — really pushes unconditional quality. Dropout helps CFG (classifier-free guidance) work better at inference time, since the model actually knows what \"no prompt\" looks like. But for LoRA training where prompt adherence is king, too much dropout can backfire. A tiny bit (0.05) often helps; skip it entirely for very short training runs.",
                "dropout_epochs": "Dropout Every N Epochs",
                "dropout_epochs_help": "Instead of random dropout per step, this drops captions on specific epochs. <strong>0:</strong> Falls back to the normal random <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a> behavior. <strong>1:</strong> Every single epoch is captionless (pretty extreme). <strong>5:</strong> Epochs 5, 10, 15... go without captions. This gives you a more structured pattern — normal training most of the time, with periodic \"blind\" epochs. Useful for specific curricula, but random per-step dropout is more common in practice.",
                "noise_title": "Noise Settings",
                "noise_type": "Noise Offset Type",
                "noise_type_help": "The mathematical method used to calculate noise offset. \"Original\" is the standard implementation.",
                "noise_offset": "Noise Offset Strength",
                "noise_offset_help": "Adds an offset to the noise during training, which helps the model generate images with better dynamic range (deeper blacks and brighter whites). A value of 0.05-0.1 is generally recommended.",
                "adaptive_noise": "Adaptive Noise Scale",
                "adaptive_noise_help": "Scales the noise based on the model's prediction error during training.",
                "noise_random": "Noise Offset Random Strength",
                "noise_random_help": "Randomizes the strength of the noise offset for each training step.",
                "multires_iter": "Multires Noise Iter",
                "multires_iter_help": "Applies multi-resolution noise to improve the learning of fine textures and details.",
                "multires_discount": "Multires Noise Discount",
                "multires_discount_help": "The discount factor applied to multi-resolution noise iterations.",
                "edm": "EDM-Style Training (SDXL)",
                "edm_help": "Uses the EDM (Elucidating the Design Space of Diffusion-Based Generative Models) formulation, which can result in higher quality outputs for SDXL models.",
                "caching_title": "Caching",
                "cache_latents": "Cache Latents to RAM",
                "cache_latents_help": "Precomputes VAE latents to significantly increase training speed (often 2x-3x). When enabled, all latents are cached and remain in RAM throughout training. This disables certain real-time augmentations.",
                "cache_te": "Cache Text Embeddings",
                "cache_te_help": "Precomputes text embeddings to improve training speed. This setting is incompatible with active Text Encoder training.",
                "cache_disk": "Cache Latents to Disk",
                "cache_disk_help": "Saves precomputed latents to disk to conserve RAM. Note that slow disk I/O can negatively impact training performance. These latents will be reused in training sessions. If dataset changes, you have to delete the old latents manually from cache.",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "The batch size used for VAE encoding during the caching process. Higher values increase speed but require more VRAM.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Optimizer",
                "optimizer_help": "The algorithm that actually updates your weights based on gradients. Different optimizers have different strengths. <strong>AdamW8bit:</strong> Memory-efficient 8-bit version of the classic. Uses ~50% less optimizer state memory. Go-to choice for most LoRA training when VRAM is tight. Needs the bitsandbytes library. <strong>AdamW:</strong> The original, full precision. Most stable and battle-tested. Pick this if you've got VRAM to spare or hit issues with the 8-bit version. <strong>Lion / Lion8bit:</strong> Newer hotness — often needs much lower LR (3-10× lower than AdamW). Can give good results with less memory, but still somewhat experimental. <strong>DAdaptAdam:</strong> Self-adjusting optimizer. Set your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> to 1.0 and let it find the sweet spot. <strong>Prodigy:</strong> Another smart adaptive optimizer. Also use LR=1.0. Often produces excellent results hands-free. <strong>CAME:</strong> Experimental, decent memory efficiency. <strong>Adafactor:</strong> Super memory-friendly, originally built for huge language models. Last resort for extreme VRAM constraints. <strong>SGD:</strong> Basic gradient descent. Rarely used for diffusion stuff but it's here if you want to experiment. Fine-tune behavior with <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer_args\" data-xref-label=\"Optimizer Args\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer Args</a>.",
                "optimizer_adamw": "AdamW (Stable)",
                "optimizer_adamw8bit": "AdamW8bit (Memory-efficient)",
                "optimizer_lion": "Lion (Experimental)",
                "optimizer_prodigy": "Prodigy (Advanced)",
                "optimizer_dadaptation": "DAdaptAdam (Adaptive LR)",
                "optimizer_args": "Optimizer Args",
                "optimizer_args_help": "Extra tweaks passed straight to your <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>. Format: <code>key=value</code>, comma-separated. Some examples: <strong>weight_decay=0.01</strong> — L2 regularization to fight overfitting (common with AdamW). <strong>betas=(0.9,0.999)</strong> — Adam momentum coefficients. <strong>d_coef=1.0</strong> — D-coefficient for Prodigy/DAdaptAdam. <strong>eps=1e-8</strong> — Tiny number for numerical stability. Defaults are solid for most training. Only mess with these if you're following a specific guide or you really know what you're doing.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning Rate",
                "lr_help": "The base learning rate for training.",
                "learning_rate": "Learning Rate",
                "learning_rate_help": "The single most impactful setting in your entire training run. This determines how aggressively the model adjusts its weights each step. <strong>Too high</strong> (like 1e-3 or more) and you'll fry your model — expect artifacts, noise, or complete collapse. <strong>Too low</strong> (like 1e-6 or less) and training will crawl or go nowhere at all. <strong>Where to start:</strong> - <strong>LoRA/LyCORIS:</strong> 1e-4 to 5e-4 is the sweet spot - <strong>Full finetune:</strong> go much lower, around 1e-6 to 1e-5 - <strong>Prodigy/DAdaptAdam:</strong> just use 1.0 and let the optimizer figure it out The 'perfect' value depends on batch size, dataset, and your setup. Bigger effective batches (via <a href=\"#\" class=\"xref-link\" data-xref=\"grad_acc\" data-xref-label=\"Gradient Accumulation\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Gradient Accumulation</a>) usually handle higher learning rates gracefully.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Override the learning rate just for the UNet — that's the core image generation engine. Leave blank to use the global <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> setting. Splitting rates between UNet and Text Encoder gives you precise control over what learns faster. The UNet handles all your visuals (style, composition, structure), so this directly affects how quickly the \"look\" of your training subject gets baked in. Honestly, same rate for both usually works fine — only split if you know what you're doing.",
                "unet_lr_placeholder": "Optional",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Learning rate specifically for the Text Encoder (CLIP or T5, depending on your base model). This component translates your text prompts into something the image generator understands. Common wisdom: use about <strong>1/10th to 1/2</strong> of the UNet rate, or just turn it off entirely (set to 0). <strong>Training it:</strong> Helps the model recognize new concepts, names, and respond better to your style-specific prompts. <strong>Skipping it (0):</strong> Keeps the base model's language understanding intact — good when you only care about visual style. For LoRA work, half of <a href=\"#\" class=\"xref-link\" data-xref=\"unet_lr\" data-xref-label=\"UNet LR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet LR</a> is a solid starting point.",
                "te_lr_placeholder": "Optional",
                "lr_scheduler": "LR Scheduler",
                "lr_scheduler_help": "Dictates how your learning rate evolves over the course of training. This can make or break your results. <strong>constant:</strong> Stays flat the whole way. Simple, predictable — works great for short runs. <strong>cosine:</strong> Starts strong, then smoothly tapers off following a cosine curve. Popular choice because it pushes learning early and refines gently at the end. <strong>cosine_with_restarts:</strong> Same cosine idea, but periodically kicks the LR back up. Can help escape local minima. Set restart count with <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_num_cycles\" data-xref-label=\"LR Scheduler Cycles\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">LR Scheduler Cycles</a>. <strong>linear:</strong> Straight line from starting LR down to zero. Simple and effective. <strong>polynomial:</strong> Decays based on a power function. Tweak behavior with <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_power\" data-xref-label=\"LR Scheduler Power\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">LR Scheduler Power</a>. <strong>constant_with_warmup:</strong> Ramps up from zero, then holds steady. Pair with <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Steps</a> for smooth starts. For most LoRA jobs, <strong>constant</strong> or <strong>cosine</strong> will serve you well.",
                "lr_scheduler_constant": "constant",
                "lr_scheduler_linear": "linear",
                "lr_scheduler_cosine": "cosine",
                "lr_scheduler_cosine_with_restarts": "cosine_with_restarts",
                "lr_scheduler_polynomial": "polynomial",
                "lr_scheduler_constant_with_warmup": "constant_with_warmup",
                "scheduler": "LR Scheduler",
                "scheduler_help": "The strategy used to adjust the learning rate during training.",
                "warmup": "Warmup Steps",
                "warmup_help": "How many steps to spend gently ramping the learning rate from zero up to its target value. Think of it like warming up before exercise — prevents the model from getting shocked by sudden large gradients when weights are still way off from optimal. <strong>0</strong> = no warmup, jump straight to full LR. <strong>10-100 steps</strong> is usually plenty for LoRA training. <strong>100-500 steps</strong> might help with full finetuning or when using big batches. You can also use <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> to specify this as a percentage of total steps instead. Especially useful with high learning rates or adaptive optimizers like <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy or DAdaptAdam</a>.",
                "warmup_steps": "Warmup Steps",
                "warmup_steps_help": "How many steps to spend gently ramping the learning rate from zero up to its target value. Think of it like warming up before exercise — prevents the model from getting shocked by sudden large gradients when weights are still way off from optimal. <strong>0</strong> = no warmup, jump straight to full LR. <strong>10-100 steps</strong> is usually plenty for LoRA training. <strong>100-500 steps</strong> might help with full finetuning or when using big batches. You can also use <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> to specify this as a percentage of total steps instead. Especially useful with high learning rates or adaptive optimizers like <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy or DAdaptAdam</a>.",
                "warmup_ratio": "Warmup Ratio",
                "warmup_ratio_help": "Alternative to <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Steps</a> — specify warmup as a fraction of total training instead of an exact step count. <strong>0.0</strong> = no warmup. <strong>0.05</strong> = warmup over the first 5% of training. <strong>0.1</strong> = warmup over the first 10% of training. Way more convenient than counting steps manually since it scales automatically with your training length. If you set both this and warmup_steps, this one wins.",
                "cycles": "LR Cycles",
                "cycles_help": "How many complete up-and-down cycles to run when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a>. Each cycle drops the LR to minimum then kicks it back up (usually to a lower peak than before). <strong>1</strong> = one cycle across all training, basically the same as regular cosine. <strong>2-4</strong> cycles can shake the model out of local minima and explore the loss landscape better. More cycles = more 'restarts' = better exploration but potentially less stable near the end. Only matters if you're using cosine_with_restarts.",
                "scheduler_power": "Scheduler Power",
                "scheduler_power_help": "The exponent when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial scheduler</a>. Shapes how aggressively the LR drops off. <strong>1.0</strong> = linear decay, nothing fancy. <strong>> 1.0</strong> (like 2.0) = fast drop early, slows down later. <strong>< 1.0</strong> (like 0.5) = slow drop early, faster toward the end. Only does anything with polynomial scheduler selected. Default 1.0 is fine for most cases.",
                "total_steps": "Total Steps",
                "total_steps_help": "The total number of training steps.",
                "gradient_accumulation_steps": "Gradient Accumulation Steps",
                "gradient_accumulation_steps_help": "The number of steps to accumulate gradients over.",
                "ema_unet": "EMA for UNet",
                "ema_unet_help": "Keeps a running average of UNet weights during training. Instead of using the raw weights from the last step (which can be noisy), EMA gives you a \"smoothed\" version that typically looks better and generalizes more reliably. Think of it as noise reduction for your model weights — the EMA checkpoint often outperforms the raw one. The catch: it doubles VRAM usage for UNet weights (you're storing both sets). Smoothing rate controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a>. Strongly recommended for longer training runs if your VRAM can handle it.",
                "ema_te": "EMA for Text Encoder",
                "ema_te_help": "Same idea as <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, but for the text encoder instead. Less critical than UNet EMA since text encoder weights usually shift more gently during LoRA training. Still adds VRAM overhead for storing the extra weights. Turn this on if you're training the text encoder and want the absolute best quality.",
                "ema_decay": "EMA Decay",
                "ema_decay_help": "How much the EMA \"remembers\" old weights vs. new: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Higher (0.999-0.9999):</strong> More smoothing, slower to pick up changes. Good for long training runs. <strong>Lower (0.99-0.995):</strong> Less smoothing, reacts faster to new weights. Better for short runs. <strong>0.995</strong> works well for typical LoRA training (hundreds to a few thousand steps). Doing tens of thousands of steps? Try <strong>0.9999</strong>. Only matters if <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for UNet</a> or <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for Text Encoder</a> is on.",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Whether to use mixed precision training (FP16).",
                "mixed_precision_no": "no",
                "mixed_precision_fp16": "fp16",
                "mixed_precision_bf16": "bf16",
                "fp16_opt_level": "FP16 Opt Level",
                "fp16_opt_level_help": "The optimization level for FP16 training.",
                "loss_scale": "Loss Scale",
                "loss_scale_help": "The loss scaling factor for FP16 training.",
                "clip_grad": "Clip Grad",
                "clip_grad_help": "Whether to clip gradients during training.",
                "log_interval": "Log Interval",
                "log_interval_help": "The interval (in steps) at which to log training progress.",
                "save_interval": "Save Interval",
                "save_interval_help": "The interval (in steps) at which to save checkpoints.",
                "resume_from_checkpoint": "Resume from Checkpoint",
                "resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint.",
                "checkpoint_path": "Checkpoint Path",
                "checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "checkpoint_path_help": "The path to the checkpoint file to resume training from.",
                "use_8bit_adam": "Use 8-bit Adam",
                "use_8bit_adam_help": "Whether to use the 8-bit Adam optimizer.",
                "beta1": "Beta 1",
                "beta1_help": "The beta1 parameter for the Adam optimizer.",
                "beta2": "Beta 2",
                "beta2_help": "The beta2 parameter for the Adam optimizer.",
                "epsilon": "Epsilon",
                "epsilon_help": "The epsilon parameter for the Adam optimizer.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer.",
                "weight_decay": "Weight Decay",
                "weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer.",
                "max_grad_norm": "Max Grad Norm",
                "max_grad_norm_help": "The maximum norm for gradient clipping.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "The beta1 parameter for the Adam optimizer.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "The beta2 parameter for the Adam optimizer.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "The epsilon parameter for the Adam optimizer."
            },
            "advanced": {
                "attention": "Attention Backend",
                "attention_help": "Which implementation handles the attention mechanism — the most memory-hungry part of transformers. <strong>SDPA:</strong> PyTorch's built-in optimized attention (requires PyTorch 2.0+). Good balance of speed and memory, no extra installs. Recommended default. <strong>xFormers:</strong> Meta's attention library, often the fastest option on NVIDIA cards. Can slash VRAM by 20-40% compared to vanilla attention. Needs separate installation. <strong>None:</strong> Plain PyTorch attention — eats more memory but works everywhere. Only fall back to this if SDPA or xFormers are giving you trouble. Both SDPA and xFormers play nicely with <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Aggressive Memory Saving</a>.",
                "use_xformers": "Use Xformers",
                "use_xformers_help": "Whether to use Xformers for efficient attention computation.",
                "xformers_memory_efficient": "Memory Efficient Attention",
                "xformers_memory_efficient_help": "Whether to use memory-efficient attention with Xformers.",
                "gradient_checkpointing": "Gradient Checkpointing",
                "gradient_checkpointing_help": "Whether to use gradient checkpointing to save memory.",
                "grad_acc": "Gradient Accumulation",
                "grad_acc_help": "Stacks up gradients over multiple passes before updating weights — essentially fakes a bigger batch size. Your <strong>effective batch = batch_size × gradient_accumulation</strong>. So batch_size=2 with accumulation=4 acts like batch_size=8. This is a lifesaver for VRAM-starved systems: can't fit batch=8? Do batch=2 with accumulation=4 instead. Bigger effective batches mean more stable training, but you might need to tweak your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">learning rate</a>. Downside: slows things down proportionally (4× accumulation = 4× slower per effective step).",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Numerical precision for calculations — directly impacts VRAM and speed. <strong>fp16:</strong> Half-precision floats, cuts memory in half compared to full precision. Solid choice for GTX 10-series and RTX 20-series cards, though SDXL can sometimes throw NaN errors due to fp16's limited range. <strong>bf16:</strong> Also 16 bits but with better range for exponents — much more stable. Definitely use this on RTX 30-series and newer. Won't work on older hardware. <strong>fp8:</strong> 8-bit precision for extreme memory savings. Experimental and may hurt quality. Needs specific hardware. <strong>no:</strong> Full fp32 precision. Eats 2× the VRAM but rock-solid numerically. Only use for debugging NaN issues or if you've got VRAM to burn.",
                "mixed_precision_training": "Mixed Precision Training",
                "mixed_precision_training_help": "Whether to use mixed precision training (FP16).",
                "tf32": "Allow TF32",
                "tf32_help": "TensorFloat-32 — a compute mode exclusive to RTX 30-series and newer. Uses fp32 range but with less mantissa precision (10 bits instead of 23), giving you up to 8× speedup on some operations with barely any quality difference for ML workloads. <strong>Always leave this on for RTX 30-series and 40-series cards.</strong> Does nothing on older GPUs (GTX 10-series, RTX 20-series) since they don't have the hardware for it. Only turn off if you suspect TF32 is somehow tanking quality (super rare).",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "Separate batch size just for VAE encoding — when images get converted to latent space before the UNet sees them. Setting this lower than your main batch size can tame VRAM spikes during encoding. Leave blank to match training batch size. Set to <strong>1</strong> for minimum VRAM at the cost of slower encoding. Handy if you're OOMing specifically during the latent encoding phase.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max Token Length",
                "max_token_length_help": "Maximum tokens (roughly words/subwords) allowed in captions. CLIP's native context is <strong>77 tokens</strong> (75 usable + start/end markers). <strong>75:</strong> Standard, handles most captions without issue. <strong>150:</strong> Room for more detailed descriptions — good for complex scenes or thorough character breakdowns. <strong>225:</strong> Very long, super-detailed captions. Eats significantly more VRAM and slows down processing. Higher limits work through caption chunking techniques. Bump up to <strong>150</strong> or <strong>225</strong> if your captions regularly exceed ~60-70 words and truncation is cutting off important info.",
                "memory_saving": "Aggressive Memory Saving",
                "memory_saving_help": "Throws everything at VRAM savings: gradient checkpointing, CPU offloading, aggressive garbage collection. <strong>A must-have for 4GB-8GB cards</strong> — lets you train SDXL on hardware that would otherwise choke. Can cut VRAM usage by 30-50% depending on the model. The price? Training becomes 2-4× slower due to recomputation and shuffling data between CPU and GPU. Pairs well with a good <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention Backend</a> (SDPA or xFormers) for maximum savings. Got 16GB+ VRAM? Leave this off and enjoy faster training.",
                "seed": "Training Seed",
                "seed_help": "The random seed for all RNGs in training. Pick a specific seed and you get <strong>reproducibility</strong>: same config + same seed = identical results (assuming same hardware and software). Leave blank for random initialization — totally fine for everyday training. Useful when A/B testing settings, hunting bugs, or sharing reproducible configs with others. People often pick memorable numbers like <strong>42</strong>, <strong>1234</strong>, or <strong>0</strong>.",
                "seed_placeholder": "Random",
                "fp16_opt_level": "FP16 Opt Level",
                "fp16_opt_level_help": "The optimization level for FP16 training.",
                "loss_scale": "Loss Scale",
                "loss_scale_help": "The loss scaling factor for FP16 training.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "The beta1 parameter for the Adam optimizer.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "The beta2 parameter for the Adam optimizer.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "The epsilon parameter for the Adam optimizer.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer.",
                "weight_decay": "Weight Decay",
                "weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer.",
                "max_grad_norm": "Max Grad Norm",
                "max_grad_norm_help": "The maximum norm for gradient clipping.",
                "lr_scheduler": "LR Scheduler",
                "lr_scheduler_help": "The learning rate scheduler to use.",
                "warmup_steps": "Warmup Steps",
                "warmup_steps_help": "The number of steps to perform learning rate warmup.",
                "total_steps": "Total Steps",
                "total_steps_help": "The total number of training steps.",
                "gradient_accumulation_steps": "Gradient Accumulation Steps",
                "gradient_accumulation_steps_help": "The number of steps to accumulate gradients over.",
                "clip_grad": "Clip Grad",
                "clip_grad_help": "Whether to clip gradients during training.",
                "log_interval": "Log Interval",
                "log_interval_help": "The interval (in steps) at which to log training progress.",
                "save_interval": "Save Interval",
                "save_interval_help": "The interval (in steps) at which to save checkpoints.",
                "resume_from_checkpoint": "Resume from Checkpoint",
                "resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint.",
                "checkpoint_path": "Checkpoint Path",
                "checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "checkpoint_path_help": "The path to the checkpoint file to resume training from."
            },
            "flux": {
                "title": "Flux Specific Settings",
                "t5xxl": "T5XXL Model",
                "t5xxl_help": "Path to the T5XXL text encoder. Required for Flux training to process complex textual descriptions.",
                "ae": "Autoencoder (VAE)",
                "ae_help": "Path to the Flux Autoencoder. Handles the conversion between pixel space and latent space.",
                "clip_l": "CLIP-L Model",
                "clip_l_help": "Path to the CLIP-L text encoder, used alongside T5XXL for text processing in Flux.",
                "max_seq_len": "Max Sequence Length",
                "max_seq_len_help": "Max token length for T5 text encoder (Flux / SD3). Higher values allow longer prompts/captions, but increase VRAM use and slow down training. Common values: <strong>256</strong> (default) or <strong>512</strong>.",
                "guidance_scale": "Guidance Scale",
                "guidance_scale_help": "Classifier-free guidance for training Flux and SD3 models. Tells the model how hard to stick to the text prompt during training. <strong>3.5:</strong> Stability AI's recommended default for SD3, works great for Flux too. <strong>Lower (1-2):</strong> More creative/varied outputs but might wander from the prompt. <strong>Higher (5-7):</strong> Tighter prompt adherence but can look a bit stiff. For SD1.5/SDXL, this typically isn't used — guidance only kicks in at inference time for those.",
                "discrete_flow_shift": "Discrete Flow Shift",
                "discrete_flow_shift_help": "Adjusts the flow matching shift parameter. Higher values can improve detail but may require more sampling steps.",
                "model_prediction_type": "Prediction Type",
                "model_prediction_type_help": "Specifies what the model is predicting (e.g., raw noise or velocity). Usually set to \u0027raw\u0027 for Flux.",
                "split_mode": "Split Mode",
                "split_mode_help": "Advanced memory management for Flux. \u0027Symmetry\u0027 is standard; \u0027Asymmetry\u0027 can save VRAM at a slight performance cost.",
                "train_blocks": "Train Blocks",
                "train_blocks_help": "Specifies which blocks of the Flux model to train. \u0027all\u0027 is standard for full fine-tuning.",
                "weighting_scheme": "Weighting Scheme",
                "weighting_scheme_help": "How timesteps are sampled during training for flow-matching models (Flux, SD3). This decides which noise levels get more attention during training. <strong>None (Uniform):</strong> Equal love for all noise levels. Safe default when you don't have specific needs. <strong>Logit Normal:</strong> Bell-curve distribution centered on <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> with width set by <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Lets you dial in exactly which timesteps matter most. <strong>Mode:</strong> Focuses hard on a specific timestep, spread controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a>. These only affect Flux and SD3 — does nothing for SD1.5 or SDXL (epsilon-prediction models).",
                "logit_mean": "Logit Mean",
                "logit_mean_help": "Where the logit-normal distribution peaks when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>0.0:</strong> Centers on the middle timestep (t=0.5). <strong>Negative:</strong> Push focus toward noisier timesteps (coarse structure). <strong>Positive:</strong> Push focus toward cleaner timesteps (fine details). Tune this based on what part of the generation process matters most for your use case. Only kicks in when Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit Std",
                "logit_std_help": "How spread out the logit-normal distribution is when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>Lower (0.5-0.8):</strong> Tight focus around <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> — really zeros in on specific timesteps. <strong>Higher (1.5-2.0):</strong> Spreads out more, approaching uniform sampling. <strong>1.0:</strong> Balanced default. Only matters when Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode Scale",
                "mode_scale_help": "How tightly the mode distribution clusters when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode weighting</a>. <strong>Lower:</strong> Samples spread across more timesteps. <strong>Higher:</strong> Samples bunch up tighter around the mode. <strong>1.29:</strong> The value from the original SD3 paper. Stick with it unless experimenting. Only relevant when Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep Sampling",
                "type": "Sampling Type",
                "type_help": "The method used to sample timesteps during training. \u0027sigma_distribution\u0027 is recommended for Flux.",
                "min": "Min Timestep",
                "min_help": "Minimum timestep for sampling.",
                "max": "Max Timestep",
                "max_help": "Maximum timestep for sampling.",
                "sampling_type": "Sampling Type",
                "sampling_type_help": "The method used to distribute noise across timesteps. \u0027sigma\u0027 is modern and generally preferred for Flux/SD3.",
                "sampling_type_sigma": "sigma",
                "sampling_type_timestep": "timestep",
                "timestep_spacing": "Timestep Spacing",
                "timestep_spacing_help": "How timesteps are spaced during the diffusion process. \u0027linspace\u0027 is standard.",
                "timestep_spacing_linspace": "linspace",
                "timestep_spacing_leading": "leading",
                "timestep_spacing_trailing": "trailing",
                "min_timestep": "Min Timestep",
                "min_timestep_help": "Floor for timestep sampling — keeps training in higher-noise territory. Timesteps go from 0 (clean image) to ~1000 (pure noise). Setting something like <strong>100</strong> stops the model from training on near-clean images. Good for: - Focusing on rough structure when fine details don't matter - Testing specific noise regimes - Weird specialized training schedules Leave blank for normal full-range training. Pairs with <a href=\"#\" class=\"xref-link\" data-xref=\"max_timestep\" data-xref-label=\"Max Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Max Timestep</a> to carve out a custom range.",
                "max_timestep": "Max Timestep",
                "max_timestep_help": "Ceiling for timestep sampling — keeps training away from the noisiest levels. Setting something like <strong>800</strong> skips training on maximum-noise timesteps. Useful for: - Polishing fine details when coarse structure is already solid - Finetuning while preserving the base model's high-noise behavior - Experimental training approaches Leave blank for normal full-range training. Combined with <a href=\"#\" class=\"xref-link\" data-xref=\"min_timestep\" data-xref-label=\"Min Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min Timestep</a>, you can train on any subset of the noise schedule."
            },
            "caption": {
                "title": "Caption Settings",
                "extension": "Caption Extension",
                "extension_help": "File extension for your caption files. Default is <strong>.txt</strong> — so \"image001.png\" expects its caption in \"image001.txt\". Supports whatever extension you want (.caption, .captions, .tags, etc.). Caption file needs to live in the same folder as its image. Change this if your captioning tool outputs something different.",
                "prefix": "Caption Prefix",
                "prefix_help": "Text to prepend to every caption during training.",
                "suffix": "Caption Suffix",
                "suffix_help": "Text to append to every caption during training.",
                "dropout": "Caption Dropout",
                "dropout_help": "Probability of dropping the entire caption during training. Helps with classifier-free guidance.",
                "token_warmup": "Token Warmup",
                "token_warmup_help": "Gradually introduce tokens from the caption during the initial steps of training.",
                "weighted": "Weighted Captions",
                "weighted_help": "Parses A1111-style weights in captions like \"(important thing:1.3)\" or \"[less important:0.7]\". Lets you emphasize or de-emphasize specific words during training. <strong>Weight > 1.0:</strong> Model pays more attention to those tokens. <strong>Weight < 1.0:</strong> Model cares less about those tokens. Great for fine-tuning what aspects of your captions matter most. Turn this off if your captions use literal parentheses that aren't meant as weights.",
                "caption_dropout_rate": "Caption Dropout Rate",
                "caption_dropout_rate_help": "The probability of dropping a caption during training. This can help the model learn to generate images without relying solely on captions.",
                "caption_dropout_every_n_epochs": "Caption Dropout Every N Epochs",
                "caption_dropout_every_n_epochs_help": "The interval (in epochs) at which to apply caption dropout.",
                "caption_tag_dropout_rate": "Caption Tag Dropout Rate",
                "caption_tag_dropout_rate_help": "The probability of dropping individual tags within a caption during training."
            },
            "loss": {
                "title": "Loss \u0026 Optimization",
                "optimization_title": "Loss Optimization",
                "loss_type": "Loss Type",
                "loss_type_help": "The mathematical function used to calculate the error between predicted and actual noise. \u0027l2\u0027 is standard; \u0027huber\u0027 is more robust to outliers.",
                "type": "Loss Type",
                "type_help": "The mathematical function used to calculate the difference between predicted and target noise.",
                "loss_type_l2": "l2",
                "loss_type_l1": "l1",
                "loss_type_huber": "huber",
                "huber_delta": "Huber Delta",
                "huber_delta_help": "The threshold at which the Huber loss switches from quadratic to linear.",
                "huber_c": "Huber C",
                "huber_c_help": "The threshold parameter for Huber loss. Lower values make it more like L1.",
                "huber_schedule": "Huber Schedule",
                "huber_schedule_help": "How the Huber delta changes over time during training.",
                "huber_schedule_constant": "constant",
                "huber_schedule_exponential": "exponential",
                "huber_schedule_snr": "snr",
                "min_snr_gamma": "Min-SNR Gamma",
                "min_snr_gamma_help": "From the \"Efficient Diffusion Training via Min-SNR Weighting Strategy\" paper — fixes the problem where models overfit some timesteps while ignoring others. Without this, quality tends to be inconsistent across different denoising stages. The gamma value (<strong>5.0</strong> typically) sets the SNR clipping threshold. <strong>Lower (1-3):</strong> Emphasizes high-noise timesteps (early denoising, coarse structure). <strong>Higher (8-20):</strong> Emphasizes low-noise timesteps (final details, polish). <strong>0:</strong> Turns Min-SNR off entirely. Generally should be enabled — big quality boost with almost no performance cost.",
                "min_snr": "Min SNR Gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. Helps stabilize training by weighting loss based on noise levels.",
                "ip_noise_gamma": "IP Noise Gamma",
                "ip_noise_gamma_help": "Input Perturbation Noise adds additional random noise to the input during training as a regularization trick — kind of like dropout but for images. Helps prevent the model from memorizing specific details and improves generalization, especially on smaller datasets. <strong>0.05:</strong> Light regularization, won't mess with your training. <strong>Above 0.1:</strong> Might hurt quality by adding too much noise. Keep at <strong>0</strong> for normal training, or bump up slightly if your model is memorizing individual images instead of learning concepts. Independent from <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — you can stack them.",
                "ip_noise": "IP Noise Gamma",
                "ip_noise_help": "Strength of Input Perturbation noise. Helps with over-smoothing.",
                "ip_noise_gamma_random_range": "IP Noise Random Range",
                "ip_noise_gamma_random_range_help": "The range of random noise added when using IP Noise Gamma.",
                "noise_offset": "Noise Offset",
                "noise_offset_help": "Adds a tiny constant offset to noise during training so the model can actually generate true blacks and true whites instead of washed-out grays. Standard diffusion struggles with extreme dark/bright areas because the noise schedule doesn't cover them well. <strong>0.035-0.05:</strong> Safe defaults, improve dark/light handling without side effects. <strong>0.05-0.1:</strong> More aggressive push toward better extremes. <strong>Above 0.1:</strong> Might destabilize training or cause color shifts. Works best when paired with <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a> — they complement each other. Essential if your dataset has dark scenes, night shots, or high-contrast images.",
                "v_pred": "V-Pred Like Loss",
                "v_pred_help": "Blends velocity-prediction behavior into standard noise prediction training. Originally from v-prediction models (SD 2.x depth/inpainting) — can smooth out training and reduce artifacts. <strong>0:</strong> Pure noise prediction, the standard approach. <strong>1:</strong> Full velocity prediction mode. <strong>0.1-0.2:</strong> Subtle v-pred influence while staying compatible with noise-pred base models. Can help with smoother gradients and fewer artifacts in some cases. Leave at <strong>0</strong> if things are already working well or you're not sure.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Advanced Loss Options",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> Tweaks the noise scheduler so the final timestep is pure noise (SNR = 0), which unlocks true black generation. Essential companion to <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — auto-enabled when using the Auto toggle. Without it, noise offset alone won't fully fix dark image issues. From the paper \"Common Diffusion Noise Schedules and Sample Steps are Flawed\".<br><strong>Debiased Estimation:</strong> Fixes the bias in timestep sampling where some get oversampled. Reweights the loss so all timesteps train evenly, improving overall quality. Works alongside <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> but tackles a different problem.<br><strong>Scale V-Pred Loss:</strong> Normalizes v-prediction loss to match noise-prediction magnitude. Mainly useful for v-pred base models (SD 2.x variants) or when using <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a>. Doesn't do much for standard SD 1.5 or SDXL (epsilon-prediction).<br><strong>Masked Loss (Alpha):</strong> Uses PNG alpha channels as training masks — model only learns from non-transparent pixels. Perfect for character/object training where you want to ignore backgrounds. <strong>Needs PNGs with proper alpha channels!</strong>",
                "presets": "Loss Presets",
                "presets_help": "Quickly apply recommended loss settings for different training goals.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light"
            },
            "samples": {
                "title": "Sample Generation",
                "prompts_title": "Sample Prompts",
                "inference_title": "Inference Settings",
                "schedule_title": "Generation Schedule",
                "generated_title": "Generated Samples",
                "gallery_placeholder": "Generated samples will appear here during training...",
                "prompts": "Sample Prompts",
                "prompts_help": "Enter prompts to generate sample images during training. One prompt per line.",
                "neg_prompt": "Negative Prompt",
                "neg_prompt_help": "Negative prompt to use for sample generation.",
                "every_n_epochs": "Every N Epochs",
                "every_n_epochs_help": "Generate samples every N epochs.",
                "every_n_steps": "Every N Steps",
                "every_n_steps_help": "Generate samples every N steps.",
                "sampler": "Sampler",
                "sampler_help": "The sampler to use for generating sample images.",
                "steps": "Sample Steps",
                "steps_help": "Number of sampling steps for each image.",
                "inference_steps": "Inference Steps",
                "inference_steps_help": "Number of denoising steps for sample generation.",
                "cfg_scale": "CFG Scale",
                "cfg_scale_help": "Classifier Free Guidance scale for sample generation.",
                "width": "Width",
                "width_help": "Width of the sample image.",
                "height": "Height",
                "height_help": "Height of the sample image.",
                "num_images": "Images per Prompt",
                "num_images_help": "Number of images to generate for each prompt.",
                "seed": "Seed",
                "seed_help": "Seed for reproducible sample generation. -1 for random.",
                "format": "Sample Format",
                "format_help": "File format for saved samples.",
                "sample_every_n_steps": "Sample Every N Steps",
                "sample_every_n_steps_help": "Generates validation samples every N steps to provide visual feedback on training progress. Note that frequent sample generation can slightly increase total training time.",
                "sample_prompt": "Sample Prompt",
                "sample_prompt_help": "The prompts used for validation images to evaluate how well the model has learned the target concepts. Enter one prompt per line — each prompt will be rendered separately during sampling.",
                "sample_negative_prompt": "Sample Negative Prompt",
                "sample_negative_prompt_help": "The negative prompt used during sample generation to suppress unwanted artifacts or styles.",
                "sample_num_images": "Sample Num Images",
                "sample_num_images_help": "The number of sample images to generate at each sampling interval.",
                "sample_seed": "Sample Seed",
                "sample_seed_help": "The seed value for random number generation during sample image creation.",
                "sample_width": "Sample Width",
                "sample_width_help": "The width of the generated sample images.",
                "sample_height": "Sample Height",
                "sample_height_help": "The height of the generated sample images.",
                "sample_guidance_scale": "Sample Guidance Scale",
                "sample_guidance_scale_help": "The guidance scale to use for controlling the strength of the prompt during image generation.",
                "sample_steps": "Sample Steps",
                "sample_steps_help": "Number of denoising steps to use when generating validation samples. 20–30 steps are usually sufficient for evaluation; higher values can improve quality but increase generation time.",
                "sample_eta": "Sample Eta",
                "sample_eta_help": "The eta parameter for controlling the randomness of the diffusion process during image generation.",
                "sample_scheduler": "Sample Scheduler",
                "sample_scheduler_help": "The sampler used for validation image generation. `Euler a` is generally recommended for speed and visual quality, but choose whichever sampler fits your workflow.",
                "sample_lora": "Sample LoRA",
                "sample_lora_help": "The LoRA model to use for generating sample images.",
                "sample_lycoris": "Sample LyCORIS",
                "sample_lycoris_help": "The LyCORIS model to use for generating sample images.",
                "sample_oft": "Sample OFT",
                "sample_oft_help": "The OFT model to use for generating sample images.",
                "sample_use_8bit_adam": "Sample Use 8-bit Adam",
                "sample_use_8bit_adam_help": "Whether to use the 8-bit Adam optimizer for generating sample images.",
                "sample_fp16_opt_level": "Sample FP16 Opt Level",
                "sample_fp16_opt_level_help": "The optimization level for FP16 training when generating sample images.",
                "sample_loss_scale": "Sample Loss Scale",
                "sample_loss_scale_help": "The loss scaling factor for FP16 training when generating sample images.",
                "sample_adam_beta1": "Sample Adam Beta1",
                "sample_adam_beta1_help": "The beta1 parameter for the Adam optimizer when generating sample images.",
                "sample_adam_beta2": "Sample Adam Beta2",
                "sample_adam_beta2_help": "The beta2 parameter for the Adam optimizer when generating sample images.",
                "sample_adam_epsilon": "Sample Adam Epsilon",
                "sample_adam_epsilon_help": "The epsilon parameter for the Adam optimizer when generating sample images.",
                "sample_amsgrad": "Sample AMSGrad",
                "sample_amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer when generating sample images.",
                "sample_weight_decay": "Sample Weight Decay",
                "sample_weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer when generating sample images.",
                "sample_max_grad_norm": "Sample Max Grad Norm",
                "sample_max_grad_norm_help": "The maximum norm for gradient clipping when generating sample images.",
                "sample_lr_scheduler": "Sample LR Scheduler",
                "sample_lr_scheduler_help": "The learning rate scheduler to use when generating sample images.",
                "sample_warmup_steps": "Sample Warmup Steps",
                "sample_warmup_steps_help": "The number of steps to perform learning rate warmup when generating sample images.",
                "sample_total_steps": "Sample Total Steps",
                "sample_total_steps_help": "The total number of training steps when generating sample images.",
                "sample_gradient_accumulation_steps": "Sample Gradient Accumulation Steps",
                "sample_gradient_accumulation_steps_help": "The number of steps to accumulate gradients over when generating sample images.",
                "sample_clip_grad": "Sample Clip Grad",
                "sample_clip_grad_help": "Whether to clip gradients during training when generating sample images.",
                "sample_log_interval": "Sample Log Interval",
                "sample_log_interval_help": "The interval (in steps) at which to log training progress when generating sample images.",
                "sample_save_interval": "Sample Save Interval",
                "sample_save_interval_help": "The interval (in steps) at which to save checkpoints when generating sample images.",
                "sample_resume_from_checkpoint": "Sample Resume from Checkpoint",
                "sample_resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint when generating sample images.",
                "sample_checkpoint_path": "Sample Checkpoint Path",
                "sample_checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "sample_checkpoint_path_help": "The path to the checkpoint file to resume training from when generating sample images."
            },
            "metadata": {
                "title": "Model Metadata",
                "title_label": "Title",
                "title_help": "Public name of the LoRA.",
                "author": "Author",
                "author_help": "Your name.",
                "desc": "Description",
                "desc_help": "Public description.",
                "license": "License",
                "license_help": "Usage license.",
                "tags": "Tags",
                "tags_help": "Search tags.",
                "comment": "Training Comment",
                "comment_help": "Private notes saved in metadata."
            },
            "progress": {
                "title": "Training Progress",
                "status": "Status",
                "eta": "ETA",
                "vram": "VRAM",
                "steps": "Steps",
                "epochs": "Epochs",
                "epoch_prefix": "Epoch",
                "loss": "Loss",
                "loss_label": "LOSS",
                "start": "Start Training",
                "idle": "System Idle",
                "btn_stop": "Stop Training"
            },
            "conversion": {
                "title": "Model Conversion",
                "subtitle": "Convert LoRA models between formats (Diffusers \u0026harr; Kohya/LDM)",
                "card_title": "Conversion Tool",
                "help_text": "Use this tool to fix compatibility issues with AUTOMATIC1111/Forge. It converts \"Diffusers\" style keys to \"Kohya/LDM\" style keys and vice versa.",
                "input_model": "Input Model (.safetensors)",
                "btn_refresh": "Refresh",
                "model_architecture": "Model Architecture",
                "architecture_help": "Specifies the architecture logic required for correct key mapping. Selecting the wrong architecture will result in a non-functional model.",
                "target_format": "Target Format",
                "target_format_kohya": "Kohya / LDM (for A1111, Forge, ComfyUI)",
                "output_filename": "Output Filename (Optional)",
                "output_filename_placeholder": "Leave empty to auto-name (e.g. model_converted.safetensors)",
                "btn_convert": "Convert Model",
                "success": "Conversion Successful!"
            }
        },
        "console": {
            "title": "System Console",
            "subtitle": "Real-time training logs and system output",
            "output_title": "Console Output",
            "clear": "Clear Console",
            "copy": "Copy Logs",
            "auto_scroll": "Auto-scroll",
            "wrap_lines": "Wrap Lines",
            "status": "Status",
            "step": "Step",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU Load",
            "cpu_load": "CPU Load",
            "ram": "RAM",
            "waiting": "Waiting for training to start..."
        },
        "metadata_editor": {
            "title": "Metadata Editor",
            "subtitle": "Manage embedded metadata for LoRA and Checkpoint files",
            "select_file": "Select File",
            "select_placeholder": "Select a file...",
            "select_help": "Choose a model or metadata file to view and edit its internal metadata.",
            "btn_load": "Load Metadata",
            "load_help": "Load metadata from the selected file to edit.",
            "btn_save": "Save Metadata",
            "load_model": "Load Model",
            "save_metadata": "Save Metadata",
            "clear_fields": "Clear Fields"
        },
        "modals": {
            "save_preset_title": "Save Preset",
            "save_preset_placeholder": "Enter preset name...",
            "btn_confirm": "Confirm",
            "btn_cancel": "Cancel",
            "new_preset_title": "New Preset",
            "new_preset_placeholder": "Enter new preset name...",
            "preset_title": "Preset",
            "preset_name": "Preset Name",
            "preset_placeholder": "Enter preset name...",
            "btn_create": "Create Preset",
            "delete_preset_title": "Delete Preset",
            "delete_preset_confirm": "Are you sure you want to delete this preset?",
            "delete_title": "Confirm Deletion",
            "delete_confirm": "Are you sure you want to delete",
            "delete_warning": "This action cannot be undone. The file will be permanently removed from disk.",
            "btn_delete": "Delete",
            "stop_title": "Stop Training",
            "stop_confirm": "Are you sure you want to stop the training process?",
            "stop_warning": "Any unsaved progress will be lost. The current checkpoint will not be saved.",
            "btn_stop": "Stop Training"
        },
        "notifications": {
            "success": "Success",
            "error": "Error",
            "warning": "Warning",
            "info": "Information"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Select an option...",
            "loading": "Loading...",
            "searching": "Searching...",
            "no_results": "No results found",
            "all": "All",
            "none": "None"
        }
    }
}