{
    "language_name": "English",
    "ui": {
        "sidebar": {
            "training": "Training",
            "augmentation": "Augmentation",
            "console": "Console",
            "metadata": "Metadata",
            "updates": "Updates",
            "system": "System",
            "connecting": "Connecting...",
            "language": "Language"
        },
        "updates": {
            "title": "System Updates",
            "subtitle": "Keep your Onika installation up to date",
            "check_status_default": "Check for updates to see available changes.",
            "btn_check": "Check for Updates",
            "btn_apply": "Apply Updates",
            "details_title": "Update Details",
            "col_file": "File Path",
            "col_status": "Status",
            "log_title": "Update Log",
            "status_uptodate": "Your system is up to date.",
            "status_failed": "Failed to check for updates.",
            "status_error": "Error connecting to update server.",
            "status_applying": "Applying updates...",
            "status_checking": "Checking for updates...",
            "btn_checking": "Checking...",
            "found_updates": "Found {count} update(s) available.",
            "checked_at": "Checked at: {date}",
            "updated_count": "Updated {count} files.",
            "failed_count": "Failed to update {count} files.",
            "file_error": "Error on {path}: {error}",
            "apply_success": "Updates applied successfully!",
            "apply_partial": "Updates completed with some errors."
        },
        "training": {
            "title": "Training",
            "subtitle": "Configure and start your LoRA/LyCORIS training",
            "preset": {
                "label": "Preset",
                "select_placeholder": "Select Preset...",
                "btn_load": "Load",
                "btn_save": "Save Preset",
                "btn_new": "New Preset",
                "btn_delete": "Delete",
                "help": "Loads a preset from <span class=\"mono\">presets/</span> and applies it to the form. Presets are a fast way to switch profiles. Save Preset overwrites the selected preset. New Preset creates a new one with a custom name. Delete removes the selected preset."
            },
            "tabs": {
                "model": "Model",
                "network": "Network",
                "dataset": "Dataset",
                "text_encoder": "Text Encoder",
                "caching": "Caching",
                "learning": "Learning",
                "advanced": "Advanced",
                "samples": "Samples",
                "metadata": "Metadata"
            },
            "model": {
                "base_model": "Base Model",
                "base_model_help": "The foundation model used for training. Selecting a model that closely matches your target style improves convergence speed. Ensure the architecture matches your training settings (e.g., SDXL base for SDXL training). Training on a base model like SDXL 1.0 is generally more stable than using a heavily fine-tuned \"baked\" model.",
                "architecture": "Model Architecture",
                "architecture_help": "Specifies the architecture logic (SDXL, SD1.5, Flux) required for weight loading and latent processing. Selecting an architecture that does not match the base model will result in shape mismatch errors.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-Optimize for My Hardware",
                "optimize_help": "Automatically adjusts precision, quantization, and memory settings based on your detected GPU VRAM.",
                "btn_adjust": "Auto Adjust (Dataset-Aware)",
                "adjust_help": "Analyzes your dataset (image count/sizes + captions) and recommends stable training settings for your selected architecture. Applies changes like a preset.",
                "quantization": "Model Quantization (Q-LoRA)",
                "quantization_none": "None (Standard fp16/bf16)",
                "quantization_8bit": "8-bit (Low VRAM)",
                "quantization_4bit": "4-bit (Extreme Low VRAM)",
                "quantization_help": "Reduces model precision to 8-bit or 4-bit to significantly lower VRAM requirements, enabling training on hardware with as little as 8GB VRAM. Note that lower precision may slightly impact accuracy and requires the `bitsandbytes` library.",
                "output_name": "Output Name",
                "output_name_placeholder": "e.g. character_lora_v1",
                "output_name_help": "The filename prefix for saved LoRA adapters. Use unique names to organize different versions and prevent overwriting previous results.",
                "output_dir": "Output Directory",
                "output_dir_help": "The destination folder for training results. Ensure sufficient disk space is available to prevent training interruptions.",
                "save_precision": "Save Precision",
                "save_precision_help": "The bit depth for saved model files. `float16` is the standard for a balance of size and precision, while `float32` provides maximum fidelity at the cost of significantly larger file sizes.",
                "save_format": "Save Format",
                "save_format_help": "The file format for the output. `safetensors` is recommended for its security and loading speed. `ckpt` is a legacy format, and `diffusers` saves the model as a directory structure.",
                "save_epochs": "Save Every N Epochs",
                "save_epochs_help": "Frequency of checkpoint saves during training. Regular saving allows for recovery from crashes and provides multiple versions to evaluate for overfitting.",
                "save_steps": "Save Every N Steps",
                "save_steps_placeholder": "Optional",
                "save_steps_help": "Alternative frequency control based on training steps rather than epochs.",
                "resume": "Resume from Checkpoint",
                "resume_placeholder": "e.g. latest or path/to/checkpoint-1000",
                "resume_help": "Continues training from a previously saved state. Note that changing core parameters like learning rate or rank when resuming can lead to training instability.",
                "save_best": "Save Only Best Models (Lowest Loss)",
                "save_best_help": "Retains only the checkpoint with the lowest recorded loss to save disk space. Note that the lowest loss does not always correlate with the best visual quality.",
                "checkpoints_limit": "Checkpoints Total Limit",
                "checkpoints_limit_placeholder": "Optional",
                "checkpoints_limit_help": "The maximum number of checkpoints to retain. Older checkpoints are automatically deleted to manage disk usage."
            },
            "network": {
                "type": "Network Type",
                "type_help": "The architecture of the adapter. LoRA is the industry standard. LoHa and LoKr offer higher expressiveness but may require more careful tuning. OFT is designed to preserve hypersphere energy, which is particularly effective for Flux models.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonal Finetuning)",
                "module": "Network Module",
                "module_help": "The internal Python module used for the adapter. This is an advanced setting and should generally not be modified.",
                "dim": "Network Dim (Rank)",
                "dim_help": "The capacity of the adapter. Higher values (e.g., 128) allow for more detailed learning but increase the risk of overfitting and result in larger file sizes. Lower values (e.g., 16) generalize better and produce smaller files.",
                "alpha": "Network Alpha",
                "alpha_help": "A scaling factor that prevents weight updates from being too aggressive. A common rule of thumb is to set Alpha to half of the Network Dim for stability, or equal to Dim for stronger effects.",
                "algo": "LyCORIS Algorithm",
                "algo_help": "LyCORIS algorithm variant (only relevant for LyCORIS/LoHa modes). Different algorithms trade off expressiveness vs stability. If you're not sure, start with <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span> and only switch when you can clearly measure a benefit.",
                "conv_dim": "Conv Rank (Dim)",
                "conv_dim_help": "Optional conv adapter rank for LoCon/LyCORIS. Low values add a small convolutional capacity boost; high values increase VRAM and can overfit texture/detail quickly. Leave empty if you're not explicitly using a conv-based method.",
                "conv_alpha": "Conv Alpha",
                "conv_alpha_help": "Optional conv adapter scaling (alpha). Lower alpha makes the conv part gentler; higher alpha makes it bite harder and can destabilize or overshoot. Typically <= conv dim; if conv effects look too strong, reduce alpha before reducing dim.",
                "dora_wd": "DoRA Weight Decay",
                "dora_wd_help": "Weight decay specifically applied to DoRA magnitude vectors.",
                "network_dropout": "Network Dropout",
                "network_dropout_help": "Randomly drops neuron outputs within the adapter to improve robustness.",
                "rank_dropout": "Rank Dropout",
                "rank_dropout_help": "Randomly drops individual rank dimensions as a form of regularization.",
                "module_dropout": "Module Dropout",
                "module_dropout_help": "Randomly disables entire modules during training to prevent overfitting and improve generalization.",
                "lora_blocks": "LoRA Blocks",
                "lora_blocks_help": "Allows targeting specific blocks (e.g., mid blocks) within the model architecture.",
                "lora_layers": "LoRA Layers",
                "lora_layers_help": "Allows targeting specific layers (e.g., attention layers) for surgical fine-tuning.",
                "advanced_lora": "Advanced LoRA Options",
                "lycoris_settings": "LyCORIS Settings",
                "args": "Network Args",
                "args_help": "Additional arguments for the network module, provided as comma-separated key=value pairs. Use this for specialized options like dropout or decomposition. Incorrect arguments may lead to training instability.",
                "args_placeholder": "key=value, key2=value2",
                "conv_alpha_placeholder": "Optional",
                "conv_dim_placeholder": "Optional",
                "dora_wd_placeholder": "Optional",
                "lora_blocks_placeholder": "Optional",
                "lora_layers_placeholder": "Optional",
                "module_placeholder": "Optional"
            },
            "dataset": {
                "path": "Dataset Path",
                "path_help": "The directory containing training images and their corresponding caption files (e.g., .txt). Dataset quality is the most critical factor in training success; ensure captions accurately describe the images.",
                "resolution": "Resolution",
                "resolution_help": "The target resolution for training. Higher resolutions capture more detail but require more VRAM. Ensure the resolution is appropriate for the model architecture (e.g., 1024x1024 for SDXL, 512x512 for SD1.5).",
                "batch_size": "Batch Size",
                "batch_size_help": "The number of images processed simultaneously. Higher batch sizes can lead to faster training and smoother gradients but significantly increase VRAM usage.",
                "max_epochs": "Max Epochs",
                "max_epochs_help": "The total number of full passes through the dataset. For LoRA training, 10-20 epochs are typically sufficient. Excessive epochs can lead to overfitting and \"fried\" images.",
                "max_steps": "Max Steps",
                "max_steps_help": "An optional hard limit on the total number of training steps.",
                "max_steps_placeholder": "Optional",
                "bucketing": "Enable Aspect Ratio Bucketing",
                "bucketing_help": "Automatically groups images by aspect ratio to prevent unnecessary cropping and preserve the original composition of your training data.",
                "bucket_steps": "Bucket Resolution Step",
                "bucket_steps_help": "The grid size used for bucket dimensions. 64 is the standard value for most architectures.",
                "min_bucket": "Minimum Bucket Resolution",
                "min_bucket_help": "The minimum allowed resolution for an image bucket. This prevents the use of very small or blurry images during training.",
                "max_bucket": "Maximum Bucket Resolution",
                "max_bucket_help": "The maximum allowed resolution for an image bucket, helping to prevent out-of-memory (OOM) errors on exceptionally large images.",
                "center_crop": "Center Crop (Smart 1:1)",
                "center_crop_help": "If enabled, images that are close to square (e.g. 1210x1280) will be center-cropped to a perfect 1:1 ratio. This is recommended for character training to ensure consistent framing.",
                "no_upscale": "No Upscale",
                "no_upscale_help": "Prevents the upscaling of small images to fit bucket dimensions, avoiding potential artifacts.",
                "dreambooth": "DreamBooth & Prior Preservation",
                "prior_preservation": "Enable Prior Preservation (Reg Images)",
                "prior_preservation_help": "Uses regularization images to prevent the model from losing its original understanding of the class (e.g., \"person\") while learning a specific instance. This is essential for maintaining the model's general knowledge but increases training time.",
                "num_class_images": "Num Class Images",
                "num_class_images_help": "The target number of regularization images. A common recommendation is 100 images per instance image.",
                "instance_prompt": "Instance Prompt",
                "instance_prompt_help": "The combination of a unique trigger word and a class word (e.g., \"sks person\") that identifies the specific subject being trained.",
                "instance_prompt_placeholder": "e.g. a photo of sks person",
                "class_prompt": "Class Prompt",
                "class_prompt_help": "The generic class word (e.g., \"person\") used to identify regularization images.",
                "class_prompt_placeholder": "e.g. a photo of a person",
                "reg_dir": "Reg Images Directory",
                "reg_dir_help": "The folder containing regularization images, used only when prior preservation is enabled.",
                "reg_dir_placeholder": "Optional",
                "auto_gen": "Auto-generate Reg Images",
                "auto_gen_help": "Automatically generates regularization images if they are not already present in the specified directory.",
                "gen_settings": "Generation Settings",
                "neg_class_prompt": "Negative Class Prompt",
                "neg_class_prompt_help": "Negative prompt used when generating regularization images.",
                "neg_class_prompt_placeholder": "Optional negative prompt",
                "guidance": "Guidance Scale",
                "guidance_help": "CFG scale for generating regularization images.",
                "steps": "Reg Steps",
                "steps_help": "Number of sampling steps for generating regularization images.",
                "scheduler": "Reg Scheduler",
                "scheduler_help": "Sampler used for generating regularization images.",
                "seed": "Reg Seed",
                "seed_help": "Seed for generating regularization images. -1 for random.",
                "sample_warning": "Tip: If both “Sample Every N Steps” and “Sample Every N Epochs” are left empty, Onika will not generate sample images (saves a lot of time). On low and mid-range GPUs, generating samples during training is generally not recommended.",
                "shuffle": "DataLoader Shuffle",
                "shuffle_help": "Randomizes the order of images in each epoch to prevent the model from learning the sequence of the dataset.",
                "workers": "DataLoader Workers",
                "workers_help": "The number of CPU threads dedicated to loading and preprocessing data. Higher values can feed the GPU faster but may cause system lag if set too high.",
                "persistent_workers": "Persistent Data Loader Workers",
                "persistent_workers_help": "Keeps CPU data loading workers active between epochs to improve training speed, at the cost of increased RAM usage."
            },
            "text_encoder": {
                "ti_title": "Textual Inversion (Pivotal Tuning)",
                "weighting_title": "Caption Weighting",
                "train": "Train Text Encoder",
                "train_help": "Enables training of the text encoder alongside the UNet, which can improve prompt adherence but may reduce the model's flexibility or editability.",
                "clip_skip": "Clip Skip",
                "clip_skip_help": "Skips the top N layers of the CLIP text encoder. SD1.5 typically uses a skip of 1, while SDXL usually uses 0. Incorrect values can lead to poor prompt understanding.",
                "train_ti": "Train Textual Inversion",
                "train_ti_help": "Trains new tokens (Textual Inversion) to add specific words or concepts to the model's vocabulary.",
                "ti_frac": "TI Training Fraction",
                "ti_frac_help": "The percentage of total training epochs during which Textual Inversion training is active.",
                "te_frac": "TE Training Fraction",
                "te_frac_help": "The percentage of total training epochs during which Text Encoder training is active.",
                "emphasis": "Default Emphasis",
                "emphasis_help": "The default multiplier applied to emphasized tags when no explicit weight is provided.",
                "de_emphasis": "Default De-Emphasis",
                "de_emphasis_help": "The default multiplier applied to de-emphasized tags when no explicit weight is provided.",
                "enable_weighted": "Enable Weighted Captions",
                "enable_weighted_help": "Enables the use of weighted syntax (e.g., \"(word:1.1)\") in captions for precise control over the importance of specific terms.",
                "new_tokens": "New Tokens Per Abstraction",
                "new_tokens_help": "The number of vectors assigned to each new token. More vectors can capture more detail but are more difficult to train effectively.",
                "token_abs": "Token Abstraction",
                "token_abs_help": "The placeholder string (e.g., \"TOK\") used to represent the new concept in captions."
            },
            "caching": {
                "caching_title": "Caching",
                "cache_latents": "Cache Latents to RAM",
                "cache_latents_help": "Precomputes VAE latents to significantly increase training speed (often 2x-3x). When enabled, all latents are cached and remain in RAM throughout training. This disables certain real-time augmentations.",
                "cache_te": "Cache Text Embeddings",
                "cache_te_help": "Precomputes text embeddings to improve training speed. This setting is incompatible with active Text Encoder training.",
                "cache_disk": "Cache Latents to Disk",
                "cache_disk_help": "Saves precomputed latents to disk to conserve RAM. <strong>⚠️ Important:</strong> When using disk caching with augmentations enabled, latents are re-generated at the start of each epoch with new random augmentations. This provides variety while still benefiting from caching within each epoch. If no augmentations are set, cached latents are reused across sessions.",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "The batch size used for VAE encoding during the caching process. Higher values increase speed but require more VRAM.",
                "vae_batch_placeholder": "Auto",
                "noise_title": "Noise Settings",
                "noise_type": "Noise Offset Type",
                "noise_type_help": "The mathematical method used to calculate noise offset. \"Original\" is the standard implementation.",
                "noise_offset": "Noise Offset Strength",
                "noise_offset_help": "Adds an offset to the noise during training, which helps the model generate images with better dynamic range (deeper blacks and brighter whites). A value of 0.05-0.1 is generally recommended.",
                "adaptive_noise": "Adaptive Noise Scale",
                "adaptive_noise_help": "Scales the noise based on the model's prediction error during training.",
                "noise_random": "Noise Offset Random Strength",
                "noise_random_help": "Randomizes the strength of the noise offset for each training step.",
                "multires_iter": "Multires Noise Iter",
                "multires_iter_help": "Applies multi-resolution noise to improve the learning of fine textures and details.",
                "multires_discount": "Multires Noise Discount",
                "multires_discount_help": "The discount factor applied to multi-resolution noise iterations.",
                "edm": "EDM-Style Training (SDXL)",
                "edm_help": "Uses the EDM (Elucidating the Design Space of Diffusion-Based Generative Models) formulation, which can result in higher quality outputs for SDXL models."
            },
            "learning": {
                "optimizer": "Optimizer",
                "optimizer_help": "The algorithm that actually updates your weights based on gradients. Different optimizers have different strengths. <strong>AdamW8bit:</strong> Memory-efficient 8-bit version of the classic. Uses ~50% less optimizer state memory. Go-to choice for most LoRA training when VRAM is tight. Needs the bitsandbytes library. <strong>AdamW:</strong> The original, full precision. Most stable and battle-tested. Pick this if you've got VRAM to spare or hit issues with the 8-bit version. <strong>Lion / Lion8bit:</strong> Newer hotness — often needs much lower LR (3-10× lower than AdamW). Can give good results with less memory, but still somewhat experimental. <strong>DAdaptAdam:</strong> Self-adjusting optimizer. Set your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> to 1.0 and let it find the sweet spot. <strong>Prodigy:</strong> Another smart adaptive optimizer. Also use LR=1.0. Often produces excellent results hands-free. <strong>CAME:</strong> Experimental, decent memory efficiency. <strong>Adafactor:</strong> Super memory-friendly, originally built for huge language models. Last resort for extreme VRAM constraints. <strong>SGD:</strong> Basic gradient descent. Rarely used for diffusion stuff but it's here if you want to experiment. Fine-tune behavior with <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer_args\" data-xref-label=\"Optimizer Args\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer Args</a>.",
                "optimizer_args": "Optimizer Args",
                "optimizer_args_help": "Extra tweaks passed straight to your <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>. Format: <code>key=value</code>, comma-separated. Some examples: <strong>weight_decay=0.01</strong> — L2 regularization to fight overfitting (common with AdamW). <strong>betas=(0.9,0.999)</strong> — Adam momentum coefficients. <strong>d_coef=1.0</strong> — D-coefficient for Prodigy/DAdaptAdam. <strong>eps=1e-8</strong> — Tiny number for numerical stability. Defaults are solid for most training. Only mess with these if you're following a specific guide or you really know what you're doing.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning Rate",
                "lr_help": "The base learning rate for training.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Override the learning rate just for the UNet — that's the core image generation engine. Leave blank to use the global <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> setting. Splitting rates between UNet and Text Encoder gives you precise control over what learns faster. The UNet handles all your visuals (style, composition, structure), so this directly affects how quickly the \"look\" of your training subject gets baked in. Honestly, same rate for both usually works fine — only split if you know what you're doing.",
                "unet_lr_placeholder": "Optional",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Learning rate specifically for the Text Encoder (CLIP or T5, depending on your base model). This component translates your text prompts into something the image generator understands. Common wisdom: use about <strong>1/10th to 1/2</strong> of the UNet rate, or just turn it off entirely (set to 0). <strong>Training it:</strong> Helps the model recognize new concepts, names, and respond better to your style-specific prompts. <strong>Skipping it (0):</strong> Keeps the base model's language understanding intact — good when you only care about visual style. For LoRA work, half of <a href=\"#\" class=\"xref-link\" data-xref=\"unet_lr\" data-xref-label=\"UNet LR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet LR</a> is a solid starting point.",
                "te_lr_placeholder": "Optional",
                "scheduler": "LR Scheduler",
                "scheduler_help": "The strategy used to adjust the learning rate during training.",
                "warmup": "Warmup Steps",
                "warmup_help": "How many steps to spend gently ramping the learning rate from zero up to its target value. Think of it like warming up before exercise — prevents the model from getting shocked by sudden large gradients when weights are still way off from optimal. <strong>0</strong> = no warmup, jump straight to full LR. <strong>10-100 steps</strong> is usually plenty for LoRA training. <strong>100-500 steps</strong> might help with full finetuning or when using big batches. You can also use <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> to specify this as a percentage of total steps instead. Especially useful with high learning rates or adaptive optimizers like <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy or DAdaptAdam</a>.",
                "warmup_ratio": "Warmup Ratio",
                "warmup_ratio_help": "Alternative to <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Steps</a> — specify warmup as a fraction of total training instead of an exact step count. <strong>0.0</strong> = no warmup. <strong>0.05</strong> = warmup over the first 5% of training. <strong>0.1</strong> = warmup over the first 10% of training. Way more convenient than counting steps manually since it scales automatically with your training length. If you set both this and warmup_steps, this one wins.",
                "cycles": "LR Cycles",
                "cycles_help": "How many complete up-and-down cycles to run when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a>. Each cycle drops the LR to minimum then kicks it back up (usually to a lower peak than before). <strong>1</strong> = one cycle across all training, basically the same as regular cosine. <strong>2-4</strong> cycles can shake the model out of local minima and explore the loss landscape better. More cycles = more 'restarts' = better exploration but potentially less stable near the end. Only matters if you're using cosine_with_restarts.",
                "scheduler_power": "Scheduler Power",
                "scheduler_power_help": "The exponent when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial scheduler</a>. Shapes how aggressively the LR drops off. <strong>1.0</strong> = linear decay, nothing fancy. <strong>> 1.0</strong> (like 2.0) = fast drop early, slows down later. <strong>< 1.0</strong> (like 0.5) = slow drop early, faster toward the end. Only does anything with polynomial scheduler selected. Default 1.0 is fine for most cases.",
                "ema_unet": "EMA for UNet",
                "ema_unet_help": "Keeps a running average of UNet weights during training. Instead of using the raw weights from the last step (which can be noisy), EMA gives you a \"smoothed\" version that typically looks better and generalizes more reliably. Think of it as noise reduction for your model weights — the EMA checkpoint often outperforms the raw one. The catch: it doubles VRAM usage for UNet weights (you're storing both sets). Smoothing rate controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a>. Strongly recommended for longer training runs if your VRAM can handle it.",
                "ema_te": "EMA for Text Encoder",
                "ema_te_help": "Same idea as <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, but for the text encoder instead. Less critical than UNet EMA since text encoder weights usually shift more gently during LoRA training. Still adds VRAM overhead for storing the extra weights. Turn this on if you're training the text encoder and want the absolute best quality.",
                "ema_decay": "EMA Decay",
                "ema_decay_help": "How much the EMA \"remembers\" old weights vs. new: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Higher (0.999-0.9999):</strong> More smoothing, slower to pick up changes. Good for long training runs. <strong>Lower (0.99-0.995):</strong> Less smoothing, reacts faster to new weights. Better for short runs. <strong>0.995</strong> works well for typical LoRA training (hundreds to a few thousand steps). Doing tens of thousands of steps? Try <strong>0.9999</strong>. Only matters if <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for UNet</a> or <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for Text Encoder</a> is on."
            },
            "advanced": {
                "attention": "Attention Backend",
                "attention_help": "Which implementation handles the attention mechanism — the most memory-hungry part of transformers. <strong>SDPA:</strong> PyTorch's built-in optimized attention (requires PyTorch 2.0+). Good balance of speed and memory, no extra installs. Recommended default. <strong>xFormers:</strong> Meta's attention library, often the fastest option on NVIDIA cards. Can slash VRAM by 20-40% compared to vanilla attention. Needs separate installation. <strong>None:</strong> Plain PyTorch attention — eats more memory but works everywhere. Only fall back to this if SDPA or xFormers are giving you trouble. Both SDPA and xFormers play nicely with <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Aggressive Memory Saving</a>.",
                "grad_acc": "Gradient Accumulation",
                "grad_acc_help": "Stacks up gradients over multiple passes before updating weights — essentially fakes a bigger batch size. Your <strong>effective batch = batch_size × gradient_accumulation</strong>. So batch_size=2 with accumulation=4 acts like batch_size=8. This is a lifesaver for VRAM-starved systems: can't fit batch=8? Do batch=2 with accumulation=4 instead. Bigger effective batches mean more stable training, but you might need to tweak your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">learning rate</a>. Downside: slows things down proportionally (4× accumulation = 4× slower per effective step).",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Numerical precision for calculations — directly impacts VRAM and speed. <strong>fp16:</strong> Half-precision floats, cuts memory in half compared to full precision. Solid choice for GTX 10-series and RTX 20-series cards, though SDXL can sometimes throw NaN errors due to fp16's limited range. <strong>bf16:</strong> Also 16 bits but with better range for exponents — much more stable. Definitely use this on RTX 30-series and newer. Won't work on older hardware. <strong>fp8:</strong> 8-bit precision for extreme memory savings. Experimental and may hurt quality. Needs specific hardware. <strong>no:</strong> Full fp32 precision. Eats 2× the VRAM but rock-solid numerically. Only use for debugging NaN issues or if you've got VRAM to burn.",
                "tf32": "Allow TF32",
                "tf32_help": "TensorFloat-32 — a compute mode exclusive to RTX 30-series and newer. Uses fp32 range but with less mantissa precision (10 bits instead of 23), giving you up to 8× speedup on some operations with barely any quality difference for ML workloads. <strong>Always leave this on for RTX 30-series and 40-series cards.</strong> Does nothing on older GPUs (GTX 10-series, RTX 20-series) since they don't have the hardware for it. Only turn off if you suspect TF32 is somehow tanking quality (super rare).",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "Separate batch size just for VAE encoding — when images get converted to latent space before the UNet sees them. Setting this lower than your main batch size can tame VRAM spikes during encoding. Leave blank to match training batch size. Set to <strong>1</strong> for minimum VRAM at the cost of slower encoding. Handy if you're OOMing specifically during the latent encoding phase.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max Token Length",
                "max_token_length_help": "Maximum tokens (roughly words/subwords) allowed in captions. CLIP's native context is <strong>77 tokens</strong> (75 usable + start/end markers). <strong>75:</strong> Standard, handles most captions without issue. <strong>150:</strong> Room for more detailed descriptions — good for complex scenes or thorough character breakdowns. <strong>225:</strong> Very long, super-detailed captions. Eats significantly more VRAM and slows down processing. Higher limits work through caption chunking techniques. Bump up to <strong>150</strong> or <strong>225</strong> if your captions regularly exceed ~60-70 words and truncation is cutting off important info.",
                "memory_saving": "Aggressive Memory Saving",
                "memory_saving_help": "Throws everything at VRAM savings: gradient checkpointing, CPU offloading, aggressive garbage collection. <strong>A must-have for 4GB-8GB cards</strong> — lets you train SDXL on hardware that would otherwise choke. Can cut VRAM usage by 30-50% depending on the model. The price? Training becomes 2-4× slower due to recomputation and shuffling data between CPU and GPU. Pairs well with a good <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention Backend</a> (SDPA or xFormers) for maximum savings. Got 16GB+ VRAM? Leave this off and enjoy faster training.",
                "seed": "Training Seed",
                "seed_help": "The random seed for all RNGs in training. Pick a specific seed and you get <strong>reproducibility</strong>: same config + same seed = identical results (assuming same hardware and software). Leave blank for random initialization — totally fine for everyday training. Useful when A/B testing settings, hunting bugs, or sharing reproducible configs with others. People often pick memorable numbers like <strong>42</strong>, <strong>1234</strong>, or <strong>0</strong>.",
                "seed_placeholder": "Random"
            },
            "flux": {
                "title": "Flux Specific Settings",
                "max_seq_len": "Max Sequence Length",
                "max_seq_len_help": "Max token length for T5 text encoder (Flux / SD3). Higher values allow longer prompts/captions, but increase VRAM use and slow down training. Common values: <strong>256</strong> (default) or <strong>512</strong>.",
                "guidance_scale": "Guidance Scale",
                "guidance_scale_help": "Classifier-free guidance for training Flux and SD3 models. Tells the model how hard to stick to the text prompt during training. <strong>3.5:</strong> Stability AI's recommended default for SD3, works great for Flux too. <strong>Lower (1-2):</strong> More creative/varied outputs but might wander from the prompt. <strong>Higher (5-7):</strong> Tighter prompt adherence but can look a bit stiff. For SD1.5/SDXL, this typically isn't used — guidance only kicks in at inference time for those.",
                "weighting_scheme": "Weighting Scheme",
                "weighting_scheme_help": "How timesteps are sampled during training for flow-matching models (Flux, SD3). This decides which noise levels get more attention during training. <strong>None (Uniform):</strong> Equal love for all noise levels. Safe default when you don't have specific needs. <strong>Logit Normal:</strong> Bell-curve distribution centered on <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> with width set by <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Lets you dial in exactly which timesteps matter most. <strong>Mode:</strong> Focuses hard on a specific timestep, spread controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a>. These only affect Flux and SD3 — does nothing for SD1.5 or SDXL (epsilon-prediction models).",
                "logit_mean": "Logit Mean",
                "logit_mean_help": "Where the logit-normal distribution peaks when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>0.0:</strong> Centers on the middle timestep (t=0.5). <strong>Negative:</strong> Push focus toward noisier timesteps (coarse structure). <strong>Positive:</strong> Push focus toward cleaner timesteps (fine details). Tune this based on what part of the generation process matters most for your use case. Only kicks in when Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit Std",
                "logit_std_help": "How spread out the logit-normal distribution is when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>Lower (0.5-0.8):</strong> Tight focus around <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> — really zeros in on specific timesteps. <strong>Higher (1.5-2.0):</strong> Spreads out more, approaching uniform sampling. <strong>1.0:</strong> Balanced default. Only matters when Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode Scale",
                "mode_scale_help": "How tightly the mode distribution clusters when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode weighting</a>. <strong>Lower:</strong> Samples spread across more timesteps. <strong>Higher:</strong> Samples bunch up tighter around the mode. <strong>1.29:</strong> The value from the original SD3 paper. Stick with it unless experimenting. Only relevant when Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep Sampling",
                "min": "Min Timestep",
                "min_help": "Minimum timestep for sampling.",
                "max": "Max Timestep",
                "max_help": "Maximum timestep for sampling.",
                "max_placeholder": "Optional",
                "min_placeholder": "Optional"
            },
            "caption": {
                "title": "Caption Settings",
                "extension": "Caption Extension",
                "extension_help": "File extension for your caption files. Default is <strong>.txt</strong> — so \"image001.png\" expects its caption in \"image001.txt\". Supports whatever extension you want (.caption, .captions, .tags, etc.). Caption file needs to live in the same folder as its image. Change this if your captioning tool outputs something different.",
                "weighted": "Weighted Captions",
                "weighted_help": "Parses A1111-style weights in captions like \"(important thing:1.3)\" or \"[less important:0.7]\". Lets you emphasize or de-emphasize specific words during training. <strong>Weight > 1.0:</strong> Model pays more attention to those tokens. <strong>Weight < 1.0:</strong> Model cares less about those tokens. Great for fine-tuning what aspects of your captions matter most. Turn this off if your captions use literal parentheses that aren't meant as weights."
            },
            "loss": {
                "title": "Loss & Optimization",
                "optimization_title": "Loss Optimization",
                "type": "Loss Type",
                "type_help": "The mathematical function used to calculate the difference between predicted and target noise.",
                "huber_c": "Huber C",
                "huber_c_help": "The threshold parameter for Huber loss. Lower values make it more like L1.",
                "min_snr": "Min SNR Gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. Helps stabilize training by weighting loss based on noise levels.",
                "ip_noise": "IP Noise Gamma",
                "ip_noise_help": "Strength of Input Perturbation noise. Helps with over-smoothing.",
                "noise_offset": "Noise Offset",
                "noise_offset_help": "Adds a tiny constant offset to noise during training so the model can actually generate true blacks and true whites instead of washed-out grays. Standard diffusion struggles with extreme dark/bright areas because the noise schedule doesn't cover them well. <strong>0.035-0.05:</strong> Safe defaults, improve dark/light handling without side effects. <strong>0.05-0.1:</strong> More aggressive push toward better extremes. <strong>Above 0.1:</strong> Might destabilize training or cause color shifts. Works best when paired with <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a> — they complement each other. Essential if your dataset has dark scenes, night shots, or high-contrast images.",
                "v_pred": "V-Pred Like Loss",
                "v_pred_help": "Blends velocity-prediction behavior into standard noise prediction training. Originally from v-prediction models (SD 2.x depth/inpainting) — can smooth out training and reduce artifacts. <strong>0:</strong> Pure noise prediction, the standard approach. <strong>1:</strong> Full velocity prediction mode. <strong>0.1-0.2:</strong> Subtle v-pred influence while staying compatible with noise-pred base models. Can help with smoother gradients and fewer artifacts in some cases. Leave at <strong>0</strong> if things are already working well or you're not sure.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Advanced Loss Options",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> Tweaks the noise scheduler so the final timestep is pure noise (SNR = 0), which unlocks true black generation. Essential companion to <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — auto-enabled when using the Auto toggle. Without it, noise offset alone won't fully fix dark image issues. From the paper \"Common Diffusion Noise Schedules and Sample Steps are Flawed\".<br><strong>Debiased Estimation:</strong> Fixes the bias in timestep sampling where some get oversampled. Reweights the loss so all timesteps train evenly, improving overall quality. Works alongside <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> but tackles a different problem.<br><strong>Scale V-Pred Loss:</strong> Normalizes v-prediction loss to match noise-prediction magnitude. Mainly useful for v-pred base models (SD 2.x variants) or when using <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a>. Doesn't do much for standard SD 1.5 or SDXL (epsilon-prediction).<br><strong>Masked Loss (Alpha):</strong> Uses PNG alpha channels as training masks — model only learns from non-transparent pixels. Perfect for character/object training where you want to ignore backgrounds. <strong>Needs PNGs with proper alpha channels!</strong>",
                "presets": "Loss Presets",
                "presets_help": "Quickly apply recommended loss settings for different training goals.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light",
                "loss_type": {
                    "label": "Loss Type",
                    "help": "Determines how the error between the generated image and the target is calculated. 'L2' (MSE) is the standard, penalizing large errors heavily for smooth convergence. 'Huber' is a hybrid that is less sensitive to outliers, while 'L1' (MAE) can provide sharper results but may be less stable. L2 is recommended for most training scenarios.",
                    "options": {
                        "l2": "L2 (MSE)",
                        "huber": "Huber",
                        "smooth_l1": "Smooth L1",
                        "l1": "L1 (MAE)"
                    }
                },
                "debiased_estimation_loss": {
                    "label": "Debiased Estimation Loss",
                    "help": "Corrects for the bias introduced by the Min-SNR weighting strategy, keeping the training mathematically grounded. Note that this can slightly affect convergence speed or the effective learning rate balance."
                },
                "min_snr_gamma": {
                    "label": "Min SNR Gamma",
                    "help": "Balances training focus across different noise levels. This is essential for zero-terminal-SNR models like SDXL to learn structural details. A value of 5.0 is recommended for SDXL; setting this to 0 on v-prediction models may result in noisy or broken images."
                },
                "snr_gamma": {
                    "label": "SNR Gamma (Alias)",
                    "help": "Alternative name for Min SNR Gamma used by some legacy scripts. Same effect as above."
                },
                "prior_loss_weight": {
                    "label": "Prior Loss Weight",
                    "help": "Controls the relative importance of regularization images versus training images. Higher values (e.g., 1.0) strongly preserve the original class (e.g., 'person'), while lower values allow the model to drift more towards the target style. Setting this too low can lead to catastrophic forgetting of the base concept."
                },
                "scheduled_huber_schedule": {
                    "label": "Scheduled Huber Schedule",
                    "help": "Defines how the Huber loss threshold evolves over time, allowing for a transition between robust and precise loss calculations. 'Constant' is the most stable option.",
                    "options": {
                        "constant": "Constant",
                        "exponential": "Exponential",
                        "snr": "SNR-based"
                    }
                },
                "scheduled_huber_c": {
                    "label": "Scheduled Huber C",
                    "help": "The threshold where Huber loss transitions from quadratic (L2) to linear (L1), controlling sensitivity to outliers. Lower values increase robustness, while higher values behave more like standard L2 loss."
                },
                "scheduled_huber_scale": {
                    "label": "Scheduled Huber Scale",
                    "help": "Global multiplier for the Huber loss. Used to balance the loss magnitude against other loss components."
                },
                "scale_weight_norms": {
                    "label": "Scale Weight Norms",
                    "help": "Rescales network weights to maintain them within a specific range, which can prevent exploding gradients in deep networks or when using high learning rates. This is typically not required for standard LoRA training."
                }
            },
            "optim": {
                "gradient_checkpointing": {
                    "label": "Gradient Checkpointing",
                    "help": "Reduces VRAM usage by re-calculating parts of the model during the backward pass instead of storing them in memory. This enables larger batch sizes at the cost of a 20-30% reduction in training speed."
                },
                "max_grad_norm": {
                    "label": "Max Grad Norm",
                    "help": "Clips gradient spikes to a specified value to prevent 'exploding gradients,' which can cause the model to produce NaN or black images. A value of 1.0 is the industry standard."
                },
                "no_half_vae": {
                    "label": "No Half VAE",
                    "help": "Forces the VAE to operate in float32 precision to prevent NaN errors (black squares), which are common with the SDXL VAE in lower precision. This improves stability but increases VRAM consumption."
                }
            },
            "samples": {
                "prompts_title": "Sample Prompts",
                "inference_title": "Inference Settings",
                "schedule_title": "Generation Schedule",
                "generated_title": "Generated Samples",
                "gallery_placeholder": "Generated samples will appear here during training...",
                "prompts": "Sample Prompts",
                "prompts_help": "Enter prompts to generate sample images during training. One prompt per line.",
                "neg_prompt": "Negative Prompt",
                "neg_prompt_help": "Negative prompt to use for sample generation.",
                "every_n_epochs": "Every N Epochs",
                "every_n_epochs_help": "Generate samples every N epochs.",
                "every_n_steps": "Every N Steps",
                "every_n_steps_help": "Generate samples every N steps.",
                "sampler": "Sampler",
                "sampler_help": "The sampler to use for generating sample images.",
                "inference_steps": "Inference Steps",
                "inference_steps_help": "Number of denoising steps for sample generation.",
                "cfg_scale": "CFG Scale",
                "cfg_scale_help": "Classifier Free Guidance scale for sample generation.",
                "num_images": "Images per Prompt",
                "num_images_help": "Number of images to generate for each prompt.",
                "seed": "Seed",
                "seed_help": "Seed for reproducible sample generation. -1 for random.",
                "every_n_epochs_placeholder": "Optional",
                "every_n_steps_placeholder": "Optional",
                "neg_prompt_placeholder": "Optional negative prompt",
                "prompts_placeholder": "One prompt per line..."
            },
            "metadata": {
                "title": "Model Metadata",
                "title_help": "Public name of the LoRA.",
                "author": "Author",
                "author_help": "Your name.",
                "desc": "Description",
                "desc_help": "Public description.",
                "license": "License",
                "license_help": "Usage license.",
                "tags": "Tags",
                "tags_help": "Search tags.",
                "comment": "Training Comment",
                "comment_help": "Private notes saved in metadata.",
                "comment_placeholder": "e.g. Trained on 50 images of character X",
                "desc_placeholder": "Describe your LoRA...",
                "license_placeholder": "e.g. CreativeML Open RAIL-M",
                "tags_placeholder": "e.g. character, style, anime"
            },
            "progress": {
                "epoch_prefix": "Epoch",
                "loss_label": "LOSS",
                "start": "Start Training",
                "stop": "Stop Training",
                "idle": "System Idle",
                "caching_latents": "Caching Latents",
                "latents_cache": "Latents Cache"
            },
            "conversion": {
                "title": "Model Conversion",
                "subtitle": "Convert LoRA models between formats (Diffusers &harr; Kohya/LDM)",
                "card_title": "Conversion Tool",
                "help_text": "Use this tool to fix compatibility issues with AUTOMATIC1111/Forge. It converts \"Diffusers\" style keys to \"Kohya/LDM\" style keys and vice versa.",
                "input_model": "Input Model (.safetensors)",
                "btn_refresh": "Refresh",
                "model_architecture": "Model Architecture",
                "architecture_help": "Specifies the architecture logic required for correct key mapping. Selecting the wrong architecture will result in a non-functional model.",
                "target_format": "Target Format",
                "target_format_kohya": "Kohya / LDM (for A1111, Forge, ComfyUI)",
                "output_filename": "Output Filename (Optional)",
                "output_filename_placeholder": "Leave empty to auto-name (e.g. model_converted.safetensors)",
                "btn_convert": "Convert Model",
                "success": "Conversion Successful!"
            }
        },
        "augmentation": {
            "title": "Augmentation",
            "subtitle": "Preview and configure image augmentation settings",
            "tabs": {
                "preview": "Preview",
                "settings": "Settings"
            },
            "btn_preview": "Generate Preview",
            "btn_preview_loading": "Processing...",
            "preview_page": "Page",
            "preview_of": "of",
            "preview_images": "images",
            "preview_original": "Original",
            "preview_augmented": "Augmented",
            "preview_no_images": "No images found in dataset",
            "preview_error": "Failed to generate preview",
            "label_flipped": "Flipped",
            "label_cropped": "Cropped",
            "label_color_aug": "Color Aug",
            "settings": {
                "crop_jitter": "Crop Jitter",
                "crop_jitter_help": "Randomly crops the image by this amount. <strong>0.0:</strong> No cropping. <strong>0.1:</strong> Randomly crops between 90-100% of the image size. <strong>Note:</strong> With disk caching, new crops are generated each epoch.",
                "random_flip": "Random Flip",
                "random_flip_help": "Chance of flipping each image horizontally. <strong>0.5:</strong> 50/50 coin flip. <strong>0.0:</strong> No flipping. <strong>Note:</strong> With disk caching, flip decisions are made once per epoch.",
                "random_brightness": "Random Brightness",
                "random_brightness_help": "Randomly adjusts image brightness. <strong>0.0:</strong> No change. <strong>0.1:</strong> +/- 10% variation. <strong>Note:</strong> With disk caching, brightness is randomized once per epoch.",
                "random_contrast": "Random Contrast",
                "random_contrast_help": "Randomly adjusts image contrast. <strong>0.0:</strong> No change. <strong>0.1:</strong> +/- 10% variation.",
                "random_saturation": "Random Saturation",
                "random_saturation_help": "Randomly adjusts image saturation. <strong>0.0:</strong> No change. <strong>0.1:</strong> +/- 10% variation.",
                "random_hue": "Random Hue",
                "random_hue_help": "Randomly shifts image hue. <strong>0.0:</strong> No change. <strong>0.1:</strong> Significant color shift.",
                "caption_aug_title": "Caption Augmentation",
                "shuffle": "Shuffle Captions",
                "shuffle_help": "Randomizes the order of tags/words in captions each time they're used. Perfect for booru-style comma-separated tags where order is meaningless.",
                "keep_tokens": "Keep Tokens",
                "keep_tokens_help": "How many tokens at the start of each caption are \"sacred\" and won't get shuffled. <strong>1:</strong> First token (usually trigger word) always stays first.",
                "dropout": "Dropout Rate",
                "dropout_help": "Probability of completely dropping an image's caption. <strong>0.0:</strong> Captions always present. <strong>0.05-0.1:</strong> Light dropout.",
                "dropout_epochs": "Dropout Every N Epochs",
                "dropout_epochs_help": "Instead of random dropout per step, this drops captions on specific epochs. <strong>0:</strong> Falls back to normal random dropout behavior."
            }
        },
        "console": {
            "title": "System Console",
            "subtitle": "Real-time training logs and system output",
            "output_title": "Console Output",
            "clear": "Clear Console",
            "status": "Status",
            "step": "Step",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU Load",
            "cpu_load": "CPU Load",
            "ram": "RAM",
            "waiting": "Waiting for training to start...",
            "dataset_images": "Images",
            "dataset_tags": "Total Tags",
            "dataset_no_tags": "No Tags",
            "dataset_dims": "Dimensions (Min-Max)",
            "btn_scan_dataset": "Scan Dataset"
        },
        "metadata_editor": {
            "title": "Metadata Editor",
            "subtitle": "Manage embedded metadata for LoRA and Checkpoint files",
            "select_file": "Select File",
            "select_placeholder": "Select a file...",
            "select_help": "Choose a model or metadata file to view and edit its internal metadata.",
            "btn_load": "Load Metadata",
            "load_help": "Load metadata from the selected file to edit.",
            "btn_save": "Save Metadata"
        },
        "modals": {
            "btn_cancel": "Cancel",
            "preset_title": "Preset",
            "preset_name": "Preset Name",
            "preset_placeholder": "Enter preset name...",
            "btn_create": "Create Preset",
            "delete_title": "Confirm Deletion",
            "delete_confirm": "Are you sure you want to delete",
            "delete_warning": "This action cannot be undone. The file will be permanently removed from disk.",
            "btn_delete": "Delete",
            "stop_title": "Stop Training",
            "stop_confirm": "Are you sure you want to stop the training process?",
            "stop_warning": "Any unsaved progress will be lost. The current checkpoint will not be saved.",
            "btn_stop": "Stop Training",
            "update_title": "Apply Updates",
            "update_confirm": "Are you sure you want to apply {count} update(s)?",
            "update_warning": "Please restart the application manually after the update is complete."
        },
        "notifications": {
            "updates_available": "Updates available!",
            "no_updates": "No updates found.",
            "update_check_failed": "Update check failed",
            "connection_error": "Connection error",
            "update_complete": "Update complete!",
            "update_partial": "Update partial",
            "update_failed": "Update failed",
            "select_preset_load": "Please select a preset to load.",
            "preset_loaded": "Loaded preset: {name}",
            "preset_load_failed": "Failed to load preset",
            "config_saved": "Configuration saved!",
            "config_save_failed": "Failed to save configuration.",
            "config_save_error": "Error saving config: {error}",
            "hardware_optimized": "Applied hardware optimizations!",
            "hardware_optimize_failed": "Failed to get hardware optimizations",
            "auto_adjust_failed": "Auto Adjust failed",
            "auto_adjust_no_changes": "Auto Adjust: no changes suggested (dataset missing?)",
            "auto_adjust_success": "Auto Adjust applied.",
            "auto_adjust_success_count": "Auto Adjust applied (dataset: {count} images).",
            "select_preset_save": "Please select a preset to save to.",
            "preset_saved": "Preset saved: {name}",
            "preset_created": "Preset created: {name}",
            "preset_save_failed": "Failed to save preset.",
            "preset_save_error": "Error saving preset: {error}",
            "preset_name_empty": "Preset name cannot be empty.",
            "preset_name_invalid": "Preset name can only contain letters, numbers, underscores, and hyphens.",
            "preset_create_failed": "Failed to create preset.",
            "preset_create_error": "Error creating preset: {error}",
            "select_preset_delete": "Please select a preset to delete.",
            "preset_delete_failed": "Failed to delete preset.",
            "preset_deleted": "Preset deleted: {name}",
            "preset_delete_error": "Error deleting preset: {error}",
            "training_started": "Training started! Job ID: {job_id}",
            "training_start_error": "Error: {error}",
            "training_start_failed": "Error starting training: {error}",
            "metadata_saved": "Metadata saved successfully!",
            "metadata_load_failed": "Failed to load metadata",
            "metadata_save_failed": "Failed to save metadata",
            "metadata_save_error": "Error saving metadata: {error}",
            "select_input_file": "Please select an input file",
            "conversion_success": "Conversion successful!",
            "conversion_failed": "Conversion failed: {error}",
            "conversion_error": "Conversion error: {error}",
            "output_load_failed": "Failed to load output files: {error}",
            "dataset_scan_success": "Dataset scan complete!",
            "dataset_scan_error": "Failed to scan dataset."
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Select an option...",
            "unknown_error": "Unknown error",
            "loading": "Loading...",
            "starting": "Starting...",
            "stopped": "Stopped",
            "converting": "Converting...",
            "no_files_found": "No files found",
            "select_file": "-- Select a file --",
            "select_output_file": "Select a file from outputs..."
        }
    }
}