{
    "language_name": "Deutsch",
    "ui": {
        "sidebar": {
            "training": "Training",
            "console": "Konsole",
            "metadata": "Metadaten",
            "updates": "Updates",
            "system": "System",
            "connecting": "Verbinde...",
            "language": "Sprache"
        },
        "updates": {
            "title": "System-Updates",
            "subtitle": "Halte deine Onika-Installation auf dem neuesten Stand",
            "check_status_default": "Prüfe auf Updates, um verfügbare Änderungen zu sehen.",
            "btn_check": "Nach Updates suchen",
            "btn_apply": "Updates anwenden",
            "details_title": "Update-Details",
            "col_file": "Dateipfad",
            "col_status": "Status",
            "log_title": "Update-Log",
            "status_uptodate": "Dein System ist auf dem neuesten Stand.",
            "status_failed": "Fehler beim Prüfen auf Updates.",
            "status_error": "Fehler beim Verbinden mit dem Update-Server.",
            "status_applying": "Updates werden angewendet...",
            "status_checking": "Suche nach Updates...",
            "btn_checking": "Prüfe...",
            "found_updates": "{count} Update(s) verfügbar.",
            "checked_at": "Geprüft am: {date}",
            "updated_count": "{count} Dateien aktualisiert.",
            "failed_count": "Aktualisierung von {count} Dateien fehlgeschlagen.",
            "file_error": "Fehler bei {path}: {error}",
            "apply_success": "Updates erfolgreich angewendet!",
            "apply_partial": "Updates mit einigen Fehlern abgeschlossen."
        },
        "training": {
            "title": "Training",
            "subtitle": "Konfiguriere und starte dein LoRA/LyCORIS-Training",
            "preset": {
                "label": "Voreinstellung",
                "select_placeholder": "Voreinstellung wählen...",
                "btn_load": "Laden",
                "btn_save": "Voreinstellung speichern",
                "btn_new": "Neue Voreinstellung",
                "btn_delete": "Löschen",
                "help": "Lädt eine Voreinstellung aus <span class=\"mono\">presets/</span> und wendet sie auf das Formular an. Voreinstellungen sind eine schnelle Möglichkeit, Profile zu wechseln. Voreinstellung speichern überschreibt die ausgewählte Voreinstellung. Neue Voreinstellung erstellt eine neue mit einem benutzerdefinierten Namen. Löschen entfernt die ausgewählte Voreinstellung."
            },
            "tabs": {
                "model": "Modell",
                "network": "Netzwerk",
                "dataset": "Datensatz",
                "text_encoder": "Text-Encoder",
                "aug": "Augmentierung & Caching",
                "learning": "Lernen",
                "advanced": "Erweitert",
                "samples": "Samples",
                "metadata": "Metadaten"
            },
            "model": {
                "base_model": "Basismodell",
                "base_model_help": "Das Grundmodell für das Training. Die Auswahl eines Modells, das deinem Zielstil nahekommt, verbessert die Konvergenzgeschwindigkeit. Stelle sicher, dass die Architektur zu deinen Trainingseinstellungen passt (z.B. SDXL-Basis für SDXL-Training). Training auf einem Basismodell wie SDXL 1.0 ist generell stabiler als die Verwendung eines stark feinabgestimmten \"gebackenen\" Modells.",
                "architecture": "Modellarchitektur",
                "architecture_help": "Gibt die Architekturlogik (SDXL, SD1.5, Flux) an, die für das Laden der Gewichte und die Latent-Verarbeitung erforderlich ist. Die Auswahl einer Architektur, die nicht zum Basismodell passt, führt zu Form-Mismatch-Fehlern.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-Optimierung für meine Hardware",
                "optimize_help": "Passt automatisch Präzision, Quantisierung und Speichereinstellungen basierend auf deinem erkannten GPU-VRAM an.",
                "btn_adjust": "Auto-Anpassung (Dataset-bewusst)",
                "adjust_help": "Analysiert deinen Datensatz (Bildanzahl/Größen + Beschriftungen) und empfiehlt stabile Trainingseinstellungen für deine gewählte Architektur. Wendet Änderungen wie eine Voreinstellung an.",
                "quantization": "Modellquantisierung (Q-LoRA)",
                "quantization_none": "Keine (Standard fp16/bf16)",
                "quantization_8bit": "8-Bit (Niedriger VRAM)",
                "quantization_4bit": "4-Bit (Extrem niedriger VRAM)",
                "quantization_help": "Reduziert die Modellpräzision auf 8-Bit oder 4-Bit, um VRAM-Anforderungen erheblich zu senken und Training auf Hardware mit nur 8GB VRAM zu ermöglichen. Beachte, dass niedrigere Präzision die Genauigkeit leicht beeinträchtigen kann und die `bitsandbytes`-Bibliothek erfordert.",
                "output_name": "Ausgabename",
                "output_name_placeholder": "z.B. character_lora_v1",
                "output_name_help": "Das Dateinamenpräfix für gespeicherte LoRA-Adapter. Verwende eindeutige Namen, um verschiedene Versionen zu organisieren und das Überschreiben vorheriger Ergebnisse zu verhindern.",
                "output_dir": "Ausgabeverzeichnis",
                "output_dir_help": "Der Zielordner für Trainingsergebnisse. Stelle sicher, dass ausreichend Speicherplatz verfügbar ist, um Trainingsunterbrechungen zu vermeiden.",
                "save_precision": "Speicherpräzision",
                "save_precision_help": "Die Bittiefe für gespeicherte Modelldateien. `float16` ist der Standard für ein Gleichgewicht von Größe und Präzision, während `float32` maximale Genauigkeit auf Kosten deutlich größerer Dateigrößen bietet.",
                "save_format": "Speicherformat",
                "save_format_help": "Das Dateiformat für die Ausgabe. `safetensors` wird wegen seiner Sicherheit und Ladegeschwindigkeit empfohlen. `ckpt` ist ein Legacy-Format, und `diffusers` speichert das Modell als Verzeichnisstruktur.",
                "save_epochs": "Speichern alle N Epochen",
                "save_epochs_help": "Häufigkeit der Checkpoint-Speicherungen während des Trainings. Regelmäßiges Speichern ermöglicht Wiederherstellung nach Abstürzen und bietet mehrere Versionen zur Bewertung von Overfitting.",
                "save_steps": "Speichern alle N Schritte",
                "save_steps_placeholder": "Optional",
                "save_steps_help": "Alternative Häufigkeitskontrolle basierend auf Trainingsschritten statt Epochen.",
                "resume": "Von Checkpoint fortsetzen",
                "resume_placeholder": "z.B. latest oder path/to/checkpoint-1000",
                "resume_help": "Setzt das Training von einem zuvor gespeicherten Zustand fort. Beachte, dass das Ändern von Kernparametern wie Lernrate oder Rang beim Fortsetzen zu Trainingsinstabilität führen kann.",
                "save_best": "Nur beste Modelle speichern (niedrigster Loss)",
                "save_best_help": "Behält nur den Checkpoint mit dem niedrigsten aufgezeichneten Loss, um Speicherplatz zu sparen. Beachte, dass der niedrigste Loss nicht immer mit der besten visuellen Qualität korreliert.",
                "checkpoints_limit": "Checkpoints Gesamtlimit",
                "checkpoints_limit_placeholder": "Optional",
                "checkpoints_limit_help": "Die maximale Anzahl beizubehaltender Checkpoints. Ältere Checkpoints werden automatisch gelöscht, um die Speichernutzung zu verwalten."
            },
            "network": {
                "type": "Netzwerktyp",
                "type_help": "Die Architektur des Adapters. LoRA ist der Industriestandard. LoHa und LoKr bieten höhere Ausdruckskraft, erfordern aber möglicherweise sorgfältigere Abstimmung. OFT ist darauf ausgelegt, Hypersphärenenergie zu erhalten, was besonders für Flux-Modelle effektiv ist.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonales Finetuning)",
                "module": "Netzwerkmodul",
                "module_help": "Das interne Python-Modul für den Adapter. Dies ist eine erweiterte Einstellung und sollte normalerweise nicht geändert werden.",
                "dim": "Network Dim (Rang)",
                "dim_help": "Die Kapazität des Adapters. Höhere Werte (z.B. 128) ermöglichen detaillierteres Lernen, erhöhen aber das Risiko von Overfitting und führen zu größeren Dateigrößen. Niedrigere Werte (z.B. 16) generalisieren besser und erzeugen kleinere Dateien.",
                "alpha": "Network Alpha",
                "alpha_help": "Ein Skalierungsfaktor, der verhindert, dass Gewichtsaktualisierungen zu aggressiv sind. Eine gängige Faustregel ist, Alpha auf die Hälfte von Network Dim für Stabilität zu setzen oder gleich Dim für stärkere Effekte.",
                "algo": "LyCORIS-Algorithmus",
                "algo_help": "LyCORIS-Algorithmusvariante (nur relevant für LyCORIS/LoHa-Modi). Verschiedene Algorithmen tauschen Ausdruckskraft gegen Stabilität aus. Wenn du dir nicht sicher bist, beginne mit <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span> und wechsle nur, wenn du einen klaren Vorteil messen kannst.",
                "conv_dim": "Conv Rang (Dim)",
                "conv_dim_help": "Optionaler Conv-Adapter-Rang für LoCon/LyCORIS. Niedrige Werte fügen einen kleinen konvolutionalen Kapazitätsschub hinzu; hohe Werte erhöhen VRAM und können Textur/Details schnell überanpassen. Lasse es leer, wenn du keine conv-basierte Methode explizit verwendest.",
                "conv_alpha": "Conv Alpha",
                "conv_alpha_help": "Optionale Conv-Adapter-Skalierung (Alpha). Niedrigeres Alpha macht den Conv-Teil sanfter; höheres Alpha macht ihn stärker und kann destabilisieren oder überschießen. Typischerweise <= Conv Dim; wenn Conv-Effekte zu stark aussehen, reduziere Alpha vor Dim.",
                "dora_wd": "DoRA Weight Decay",
                "dora_wd_help": "Weight Decay speziell auf DoRA-Magnitudenvektoren angewendet.",
                "network_dropout": "Network Dropout",
                "network_dropout_help": "Lässt zufällig Neuronenausgaben innerhalb des Adapters fallen, um die Robustheit zu verbessern.",
                "rank_dropout": "Rank Dropout",
                "rank_dropout_help": "Lässt zufällig einzelne Rangdimensionen als Form der Regularisierung fallen.",
                "module_dropout": "Module Dropout",
                "module_dropout_help": "Deaktiviert zufällig ganze Module während des Trainings, um Overfitting zu verhindern und die Generalisierung zu verbessern.",
                "lora_blocks": "LoRA-Blöcke",
                "lora_blocks_help": "Ermöglicht das Anvisieren spezifischer Blöcke (z.B. Mid-Blöcke) innerhalb der Modellarchitektur.",
                "lora_layers": "LoRA-Layer",
                "lora_layers_help": "Ermöglicht das Anvisieren spezifischer Layer (z.B. Attention-Layer) für chirurgisches Finetuning.",
                "advanced_lora": "Erweiterte LoRA-Optionen",
                "lycoris_settings": "LyCORIS-Einstellungen",
                "args": "Netzwerkargumente",
                "args_help": "Zusätzliche Argumente für das Netzwerkmodul, als kommagetrennte Schlüssel=Wert-Paare angegeben. Verwende dies für spezialisierte Optionen wie Dropout oder Dekomposition. Falsche Argumente können zu Trainingsinstabilität führen.",
                "args_placeholder": "key=value, key2=value2",
                "conv_alpha_placeholder": "Optional",
                "conv_dim_placeholder": "Optional",
                "dora_wd_placeholder": "Optional",
                "lora_blocks_placeholder": "Optional",
                "lora_layers_placeholder": "Optional",
                "module_placeholder": "Optional"
            },
            "dataset": {
                "path": "Dataset Path",
                "path_help": "Das Verzeichnis mit Trainingsbildern und den zugehörigen Beschriftungsdateien (z.B. .txt). Die Qualität des Datensatzes ist der kritischste Faktor für den Trainingserfolg; stelle sicher, dass Beschriftungen die Bilder genau beschreiben.",
                "resolution": "Auflösung",
                "resolution_help": "Die Zielauflösung für das Training. Höhere Auflösungen erfassen mehr Details, benötigen aber mehr VRAM. Stelle sicher, dass die Auflösung für die Modellarchitektur geeignet ist (z.B. 1024x1024 für SDXL, 512x512 für SD1.5).",
                "batch_size": "Batch-Größe",
                "batch_size_help": "Die Anzahl gleichzeitig verarbeiteter Bilder. Größere Batch-Größen können zu schnellerem Training und glatteren Gradienten führen, erhöhen aber den VRAM-Verbrauch deutlich.",
                "max_epochs": "Max. Epochen",
                "max_epochs_help": "Die Gesamtzahl vollständiger Durchläufe durch den Datensatz. Für LoRA-Training sind 10-20 Epochen typischerweise ausreichend. Zu viele Epochen können zu Overfitting und \"frittierten\" Bildern führen.",
                "max_steps": "Max. Schritte",
                "max_steps_help": "Ein optionales hartes Limit für die Gesamtzahl der Trainingsschritte.",
                "max_steps_placeholder": "Optional",
                "bucketing": "Seitenverhältnis-Bucketing aktivieren",
                "bucketing_help": "Gruppiert Bilder automatisch nach Seitenverhältnis, um unnötiges Zuschneiden zu vermeiden und die Originalkomposition deiner Trainingsdaten zu bewahren.",
                "bucket_steps": "Bucket-Auflösungsschritt",
                "bucket_steps_help": "Die Rastergröße für Bucket-Dimensionen. 64 ist der Standardwert für die meisten Architekturen.",
                "min_bucket": "Minimale Bucket-Auflösung",
                "min_bucket_help": "Die minimal erlaubte Auflösung für einen Bild-Bucket. Verhindert die Verwendung sehr kleiner oder unscharfer Bilder während des Trainings.",
                "max_bucket": "Maximale Bucket-Auflösung",
                "max_bucket_help": "Die maximal erlaubte Auflösung für einen Bild-Bucket, hilft Out-of-Memory (OOM)-Fehler bei außergewöhnlich großen Bildern zu vermeiden.",
                "center_crop": "Center Crop (Smart 1:1)",
                "center_crop_help": "Wenn aktiviert, werden Bilder, die fast quadratisch sind (z.B. 1210x1280), auf ein perfektes 1:1-Verhältnis zentriert zugeschnitten. Empfohlen für Charaktertraining, um konsistente Bildausschnitte sicherzustellen.",
                "no_upscale": "Keine Hochskalierung",
                "no_upscale_help": "Verhindert die Hochskalierung kleiner Bilder auf Bucket-Dimensionen und vermeidet potenzielle Artefakte.",
                "dreambooth": "DreamBooth & Prior Preservation",
                "prior_preservation": "Prior Preservation aktivieren (Reg Images)",
                "prior_preservation_help": "Verwendet Regularisierungsbilder, um zu verhindern, dass das Modell sein ursprüngliches Verständnis der Klasse (z.B. \"Person\") verliert, während es eine spezifische Instanz lernt. Essentiell für die Beibehaltung des allgemeinen Wissens des Modells, erhöht aber die Trainingszeit.",
                "num_class_images": "Anzahl Klassenbilder",
                "num_class_images_help": "Die Zielanzahl von Regularisierungsbildern. Eine gängige Empfehlung sind 100 Bilder pro Instanzbild.",
                "instance_prompt": "Instanz-Prompt",
                "instance_prompt_help": "Die Kombination aus einem eindeutigen Triggerwort und einem Klassenwort (z.B. \"sks person\"), das das spezifische trainierte Subjekt identifiziert.",
                "instance_prompt_placeholder": "e.g. a photo of sks person",
                "class_prompt": "Klassen-Prompt",
                "class_prompt_help": "Das generische Klassenwort (z.B. \"person\"), das zur Identifikation von Regularisierungsbildern verwendet wird.",
                "class_prompt_placeholder": "e.g. a photo of a person",
                "reg_dir": "Reg-Images-Verzeichnis",
                "reg_dir_help": "Der Ordner mit Regularisierungsbildern, wird nur verwendet, wenn Prior Preservation aktiviert ist.",
                "reg_dir_placeholder": "Optional",
                "auto_gen": "Reg-Images automatisch generieren",
                "auto_gen_help": "Generiert automatisch Regularisierungsbilder, wenn sie noch nicht im angegebenen Verzeichnis vorhanden sind.",
                "gen_settings": "Generierungseinstellungen",
                "neg_class_prompt": "Negativer Klassen-Prompt",
                "neg_class_prompt_help": "Negativer Prompt für die Generierung von Regularisierungsbildern.",
                "neg_class_prompt_placeholder": "Optional negative prompt",
                "guidance": "Guidance Scale",
                "guidance_help": "CFG-Skala für die Generierung von Regularisierungsbildern.",
                "steps": "Reg-Schritte",
                "steps_help": "Anzahl der Sampling-Schritte für die Generierung von Regularisierungsbildern.",
                "scheduler": "Reg-Scheduler",
                "scheduler_help": "Sampler für die Generierung von Regularisierungsbildern.",
                "seed": "Reg-Seed",
                "seed_help": "Seed für die Generierung von Regularisierungsbildern. -1 für zufällig.",
                "sample_warning": "Tipp: Wenn sowohl \"Sample Every N Steps\" als auch \"Sample Every N Epochs\" leer gelassen werden, generiert Onika keine Sample-Bilder (spart viel Zeit). Auf Low- und Mid-Range-GPUs wird das Generieren von Samples während des Trainings generell nicht empfohlen.",
                "shuffle": "DataLoader Shuffle",
                "shuffle_help": "Randomisiert die Reihenfolge der Bilder in jeder Epoche, um zu verhindern, dass das Modell die Sequenz des Datensatzes lernt.",
                "workers": "DataLoader Workers",
                "workers_help": "Die Anzahl der CPU-Threads für das Laden und Vorverarbeiten von Daten. Höhere Werte können die GPU schneller füttern, aber zu Systemverzögerungen führen, wenn zu hoch gesetzt.",
                "persistent_workers": "Persistente DataLoader-Workers",
                "persistent_workers_help": "Hält CPU-Datenlade-Workers zwischen Epochen aktiv, um die Trainingsgeschwindigkeit zu verbessern, auf Kosten erhöhter RAM-Nutzung."
            },
            "text_encoder": {
                "ti_title": "Textual Inversion (Pivotal Tuning)",
                "weighting_title": "Caption-Gewichtung",
                "train": "Text-Encoder trainieren",
                "train_help": "Aktiviert das Training des Text-Encoders zusammen mit dem UNet, was die Prompt-Treue verbessern kann, aber die Flexibilität oder Editierbarkeit des Modells reduzieren könnte.",
                "clip_skip": "Clip Skip",
                "clip_skip_help": "Überspringt die obersten N Layer des CLIP Text-Encoders. SD1.5 verwendet typischerweise einen Skip von 1, während SDXL normalerweise 0 verwendet. Falsche Werte können zu schlechtem Prompt-Verständnis führen.",
                "train_ti": "Textual Inversion trainieren",
                "train_ti_help": "Trainiert neue Tokens (Textual Inversion), um spezifische Wörter oder Konzepte zum Vokabular des Modells hinzuzufügen.",
                "ti_frac": "TI-Trainingsanteil",
                "ti_frac_help": "Der Prozentsatz der gesamten Trainingsepochen, während denen Textual Inversion Training aktiv ist.",
                "te_frac": "TE-Trainingsanteil",
                "te_frac_help": "Der Prozentsatz der gesamten Trainingsepochen, während denen Text-Encoder-Training aktiv ist.",
                "emphasis": "Standard-Emphasis",
                "emphasis_help": "Der Standard-Multiplikator für betonte Tags, wenn kein explizites Gewicht angegeben ist.",
                "de_emphasis": "Standard-De-Emphasis",
                "de_emphasis_help": "Der Standard-Multiplikator für entbetonte Tags, wenn kein explizites Gewicht angegeben ist.",
                "enable_weighted": "Gewichtete Captions aktivieren",
                "enable_weighted_help": "Aktiviert die Verwendung gewichteter Syntax (z.B. \"(word:1.1)\") in Captions für präzise Kontrolle über die Wichtigkeit spezifischer Begriffe.",
                "new_tokens": "Neue Tokens pro Abstraktion",
                "new_tokens_help": "Die Anzahl der Vektoren, die jedem neuen Token zugewiesen werden. Mehr Vektoren können mehr Details erfassen, sind aber schwieriger effektiv zu trainieren.",
                "token_abs": "Token-Abstraktion",
                "token_abs_help": "Der Platzhalter-String (z.B. \"TOK\"), der verwendet wird, um das neue Konzept in Captions darzustellen."
            },
            "aug": {
                "aug_title": "Bildaugmentierung",
                "aug_mode": "Augmentierungsmodus",
                "aug_mode_help": "Entscheidet, wann Augmentierungen während des Trainings einsetzen. <strong>Immer:</strong> Jedes Bild wird bei jedem Schritt augmentiert — maximale Vielfalt, kann aber etwas verlangsamen. <strong>Nur pro Epoche:</strong> Neue Augmentierungen werden einmal pro Epoche gewürfelt und bleiben in diesem Durchgang konsistent. Hält die Dinge stabil, während es zwischen Epochen variiert. <strong>Zufällige Wahrscheinlichkeit:</strong> Jedes Bild kann bei jedem Schritt augmentiert werden oder nicht, basierend auf individuellen Einstellungen wie <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>. Meistens funktionieren \"Immer\" oder \"Nur pro Epoche\" gut.",
                "color_aug": "Farbaugmentierungsstärke",
                "color_aug_help": "Wie aggressiv Farben (Farbton, Sättigung, Helligkeit) während des Trainings variiert werden. <strong>0.0:</strong> Farben bleiben genau wie sie sind — keine Manipulation. <strong>0.3-0.5:</strong> Sanfte Variation, funktioniert super für die meisten Datensätze und hilft dem Modell, exakte Farben nicht auswendig zu lernen. <strong>0.7-1.0:</strong> Starke Farbverschiebungen. Kann seltsame Kombinationen erzeugen, pusht aber wirklich die Generalisierung. Besonders praktisch für Charakter- oder Stil-LoRAs, wo du willst, dass das Modell Formen und Konzepte beherrscht, ohne an bestimmten Paletten hängenzubleiben. Abschalten, wenn Farbtreue wichtig ist (Markenfarben, spezifische Outfits usw.).",
                "flip_aug": "Flip-Aug-Wahrscheinlichkeit",
                "flip_aug_help": "Chance, jedes Bild horizontal zu spiegeln. <strong>0.5</strong> = 50/50 Münzwurf, verdoppelt im Wesentlichen deinen Datensatz mit gespiegelten Versionen. <strong>0.0</strong> = überhaupt keine Spiegelung. <strong>Achtung:</strong> Setze dies auf 0, wenn deine Bilder haben: • <strong>Text oder Logos</strong> (wären rückwärts) • <strong>Asymmetrische Sachen</strong> (Autos, Gesichter mit einer markanten Seite) • <strong>Richtungsbezogene Inhalte</strong> (linke vs. rechte Hände, spezifische Posen). Funktioniert super für symmetrische Subjekte wie zentrierte Porträts, abstrakte Kunst oder viele Anime-Charaktere. Einer der besten Tricks gegen Overfitting bei kleinen Datensätzen.",
                "crop_scale": "Zufällige Crop-Skala",
                "crop_scale_help": "Minimales Zoom-Level für zufälliges Zuschneiden. <strong>1.0:</strong> Kein Zuschneiden — Bilder bleiben in voller Größe. <strong>0.8:</strong> Schneidet irgendwo zwischen 80-100% zu, gibt subtile Zoom-Variationen. <strong>0.5:</strong> Dramatischer — kann bis zu 2× hineinzoomen und dem Modell völlig unterschiedliche Bildausschnitte zeigen. Zuschneiden lehrt das Modell, Subjekte in verschiedenen Maßstäben und Kompositionen zu handhaben. Super nützlich, wenn dein Datensatz nur \"perfekt zentrierte Kopfschüsse\" hat, du aber mehr Flexibilität in Ausgaben willst. Bei 1.0 lassen, wenn exakte Bildausschnitte wichtig sind.",
                "caption_aug_title": "Caption-Augmentierung",
                "shuffle": "Captions mischen",
                "shuffle_help": "Randomisiert die Reihenfolge von Tags/Wörtern in Captions jedes Mal, wenn sie verwendet werden. Hindert das Modell daran, feste Muster wie \"blaue Augen kommen immer vor langen Haaren\" auswendig zu lernen — stattdessen lernt es, dass diese Tags überall erscheinen können. Perfekt für Booru-Stil kommagetrennten Tags, wo die Reihenfolge bedeutungslos ist. Verwende <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a>, wenn du das Triggerwort festhalten musst. <strong>Nicht verwenden</strong> für natürlichsprachliche Captions, wo Wortreihenfolge Bedeutung trägt (\"Frau hält Katze\" ≠ \"Katze hält Frau\").",
                "keep_tokens": "Tokens behalten",
                "keep_tokens_help": "Wie viele Tokens am Anfang jeder Caption \"heilig\" sind und nicht gemischt werden. <strong>0:</strong> Alles ist zum Mischen freigegeben. <strong>1:</strong> Erstes Token (normalerweise dein Triggerwort) bleibt immer zuerst. <strong>2-3:</strong> Schützt \"Trigger, Charaktername\"-Stil-Muster. Wenn deine Captions aussehen wie \"mytrigger, blonde Haare, blaue Augen, ...\", hält das Setzen auf 1 \"mytrigger\" vorne verankert, während der Rest randomisiert wird. Hilft dem Modell, sich auf die Trigger-Konzept-Assoziation zu fixieren, ohne exakte Tag-Reihenfolen auswendig zu lernen. Relevant nur, wenn <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a> aktiviert ist.",
                "dropout": "Dropout-Rate",
                "dropout_help": "Wahrscheinlichkeit, die Caption eines Bildes vollständig zu löschen (durch nichts ersetzen). Wenn Captions verschwinden, muss das Modell das Bild rein aus Visuellem herausfinden — dies trainiert bedingungslose Generierung. <strong>0.0:</strong> Captions immer vorhanden. <strong>0.05-0.1:</strong> Leichter Dropout, verbessert sanft, was passiert, wenn Benutzer leere oder schwache Prompts geben. <strong>0.15-0.2:</strong> Aggressiver — pusht wirklich die bedingungslose Qualität. Dropout hilft CFG (Classifier-Free Guidance) zur Inferenzzeit besser zu funktionieren, da das Modell tatsächlich weiß, wie \"kein Prompt\" aussieht. Aber für LoRA-Training, wo Prompt-Treue König ist, kann zu viel Dropout nach hinten losgehen. Ein kleines bisschen (0.05) hilft oft; komplett überspringen für sehr kurze Trainingsläufe.",
                "dropout_epochs": "Dropout alle N Epochen",
                "dropout_epochs_help": "Statt zufälligen Dropouts pro Schritt, löscht dies Captions bei spezifischen Epochen. <strong>0:</strong> Fällt zurück auf das normale zufällige <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a>-Verhalten. <strong>1:</strong> Jede einzelne Epoche ist ohne Captions (ziemlich extrem). <strong>5:</strong> Epochen 5, 10, 15... laufen ohne Captions. Das gibt dir ein strukturierteres Muster — normales Training die meiste Zeit, mit periodischen \"blinden\" Epochen. Nützlich für spezifische Curricula, aber zufälliger Dropout pro Schritt ist in der Praxis häufiger.",
                "noise_title": "Rausch-Einstellungen",
                "noise_type": "Noise-Offset-Typ",
                "noise_type_help": "Die mathematische Methode zur Berechnung des Noise-Offsets. \"Original\" ist die Standardimplementierung.",
                "noise_offset": "Noise-Offset-Stärke",
                "noise_offset_help": "Fügt einen Offset zum Rauschen während des Trainings hinzu, was dem Modell hilft, Bilder mit besserem dynamischen Bereich (tiefere Schwarztöne und hellere Weißtöne) zu generieren. Ein Wert von 0.05-0.1 wird generell empfohlen.",
                "adaptive_noise": "Adaptive Noise-Skala",
                "adaptive_noise_help": "Skaliert das Rauschen basierend auf dem Vorhersagefehler des Modells während des Trainings.",
                "noise_random": "Noise-Offset zufällige Stärke",
                "noise_random_help": "Randomisiert die Stärke des Noise-Offsets für jeden Trainingsschritt.",
                "multires_iter": "Multires-Noise-Iterationen",
                "multires_iter_help": "Wendet Multi-Resolution-Rauschen an, um das Lernen feiner Texturen und Details zu verbessern.",
                "multires_discount": "Multires-Noise-Discount",
                "multires_discount_help": "Der Diskontfaktor, der auf Multi-Resolution-Noise-Iterationen angewendet wird.",
                "edm": "EDM-Stil-Training (SDXL)",
                "edm_help": "Verwendet die EDM-Formulierung (Elucidating the Design Space of Diffusion-Based Generative Models), was zu höherer Qualität für SDXL-Modelle führen kann.",
                "caching_title": "Caching",
                "cache_latents": "Latents in RAM cachen",
                "cache_latents_help": "Vorberechnet VAE-Latents, um die Trainingsgeschwindigkeit signifikant zu erhöhen (oft 2x-3x). Wenn aktiviert, werden alle Latents gecacht und bleiben während des gesamten Trainings im RAM. Dies deaktiviert bestimmte Echtzeit-Augmentierungen.",
                "cache_te": "Text-Embeddings cachen",
                "cache_te_help": "Vorberechnet Text-Embeddings zur Verbesserung der Trainingsgeschwindigkeit. Diese Einstellung ist inkompatibel mit aktivem Text-Encoder-Training.",
                "cache_disk": "Latents auf Disk cachen",
                "cache_disk_help": "Speichert vorberechnete Latents auf der Festplatte, um RAM zu sparen. Beachte, dass langsame Disk-I/O die Trainingsleistung negativ beeinflussen kann. Diese Latents werden in Trainingssessions wiederverwendet. Wenn sich der Datensatz ändert, musst du die alten Latents manuell aus dem Cache löschen.",
                "vae_batch": "VAE-Batch-Größe",
                "vae_batch_help": "Die Batch-Größe für VAE-Encoding während des Caching-Prozesses. Höhere Werte erhöhen die Geschwindigkeit, benötigen aber mehr VRAM.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Optimizer",
                "optimizer_help": "Der Algorithmus, der deine Gewichte basierend auf Gradienten aktualisiert. <strong>AdamW8bit:</strong> Speichereffiziente 8-Bit-Version des Klassikers. Verbraucht ~50% weniger Speicher für den Optimizer-Status. Die erste Wahl für LoRA-Training bei knappem VRAM. Benötigt die bitsandbytes-Bibliothek. <strong>AdamW:</strong> Das Original in voller Präzision. Am stabilsten und bewährtesten. Wähle dies, wenn du genug VRAM hast. <strong>Lion / Lion8bit:</strong> Neuerer Algorithmus — benötigt oft eine viel niedrigere Lernrate (3-10x niedriger als AdamW). <strong>DAdaptAdam:</strong> Selbstjustierender Optimizer. Setze die <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Lernrate</a> auf 1.0 und lass ihn den optimalen Punkt finden. <strong>Prodigy:</strong> Ein weiterer intelligenter adaptiver Optimizer. Verwende ebenfalls LR=1.0. Erzeugt oft exzellente Ergebnisse ohne manuelles Tuning. <strong>Adafactor:</strong> Sehr speicherschonend, ursprünglich für riesige Sprachmodelle entwickelt. Letzte Rettung bei extremen VRAM-Einschränkungen.",
                "optimizer_args": "Optimizer-Argumente",
                "optimizer_args_help": "Zusätzliche Parameter, die direkt an den <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer</a> übergeben werden. Format: <code>key=value</code>, kommagetrennt. Beispiele: <strong>weight_decay=0.01</strong> — L2-Regularisierung gegen Overfitting. <strong>betas=(0.9,0.999)</strong> — Adam-Momentum-Koeffizienten. <strong>d_coef=1.0</strong> — D-Koeffizient für Prodigy/DAdaptAdam. Die Standardwerte sind für die meisten Trainingsläufe solide.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Lernrate",
                "lr_help": "Die Basis-Lernrate für das Training.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Überschreibt die Lernrate nur für das UNet — das Herzstück der Bildgenerierung. Leer lassen, um die globale <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Lernrate</a> zu verwenden. Das Aufteilen der Raten zwischen UNet und Text-Encoder ermöglicht eine präzise Steuerung. Das UNet ist für die gesamte Optik (Stil, Komposition, Struktur) verantwortlich. Normalerweise funktioniert die gleiche Rate für beide gut.",
                "unet_lr_placeholder": "Optional",
                "te_lr": "Text-Encoder LR",
                "te_lr_help": "Lernrate speziell für den Text-Encoder (CLIP oder T5). Diese Komponente übersetzt deine Text-Prompts in etwas, das der Bildgenerator versteht. Faustregel: Verwende etwa <strong>1/10 bis 1/2</strong> der UNet-Rate oder schalte sie ganz aus (0). <strong>Training:</strong> Hilft dem Modell, neue Konzepte und Namen zu erkennen. <strong>Überspringen (0):</strong> Bewahrt das Sprachverständnis des Basismodells — gut, wenn dich nur der visuelle Stil interessiert.",
                "te_lr_placeholder": "Optional",
                "scheduler": "LR-Scheduler",
                "scheduler_help": "Die Strategie zur Anpassung der Lernrate während des Trainings.",
                "warmup": "Warmup-Schritte",
                "warmup_help": "Anzahl der Schritte, in denen die Lernrate sanft von Null auf den Zielwert hochgefahren wird. Verhindert, dass das Modell zu Beginn durch zu große Gradienten geschockt wird. <strong>10-100 Schritte</strong> sind für LoRA-Training meist ausreichend. Kann auch als <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> (Prozentsatz) angegeben werden.",
                "warmup_ratio": "Warmup-Verhältnis",
                "warmup_ratio_help": "Alternative zu <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup-Schritten</a> — gibt das Warmup als Bruchteil des gesamten Trainings an. <strong>0.1</strong> = Warmup über die ersten 10% des Trainings. Skaliert automatisch mit der Trainingslänge.",
                "cycles": "LR-Zyklen",
                "cycles_help": "Anzahl der vollständigen Auf-und-Ab-Zyklen bei Verwendung von <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a>. Mehrere Zyklen können dem Modell helfen, aus lokalen Minima auszubrechen.",
                "scheduler_power": "Scheduler-Power",
                "scheduler_power_help": "Der Exponent bei Verwendung des <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref_label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomialen Schedulers</a>. Steuert, wie aggressiv die Lernrate abfällt. 1.0 ist linearer Abfall.",
                "ema_unet": "EMA für UNet",
                "ema_unet_help": "Hält einen gleitenden Durchschnitt der UNet-Gewichte bereit. EMA liefert oft eine \"glattere\" Version des Modells, die besser generalisiert. Verdoppelt jedoch den VRAM-Bedarf für die UNet-Gewichte. Die Glättungsrate wird durch <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a> gesteuert.",
                "ema_te": "EMA für Text-Encoder",
                "ema_te_help": "Gleiches Prinzip wie <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, aber für den Text-Encoder. Weniger kritisch als UNet EMA, da sich die Gewichte des Text-Encoders meist sanfter verschieben.",
                "ema_decay": "EMA Decay",
                "ema_decay_help": "Wie stark das EMA alte Gewichte gegenüber neuen \"vergisst\". <strong>Höher (0.999-0.9999):</strong> Mehr Glättung, reagiert langsamer auf Änderungen. <strong>Niedriger (0.99-0.995):</strong> Weniger Glättung, reagiert schneller. 0.995 ist ein guter Standard für LoRA."
            },
            "advanced": {
                "attention": "Attention-Backend",
                "attention_help": "Welche Implementierung den Attention-Mechanismus verarbeitet — der speicherhungrigste Teil von Transformern. <strong>SDPA:</strong> PyTorchs integrierte optimierte Attention (erfordert PyTorch 2.0+). Gute Balance zwischen Geschwindigkeit und Speicher. <strong>xFormers:</strong> Metas Attention-Bibliothek, oft die schnellste Option auf NVIDIA-Karten. Kann den VRAM-Bedarf um 20-40% senken. <strong>None:</strong> Standard-PyTorch-Attention — verbraucht mehr Speicher, funktioniert aber überall.",
                "grad_acc": "Gradienten-Akkumulation",
                "grad_acc_help": "Stapelt Gradienten über mehrere Durchgänge, bevor die Gewichte aktualisiert werden — simuliert eine größere Batch-Größe. <strong>Effektive Batch-Größe = Batch-Größe × Akkumulation</strong>. Hilft bei VRAM-Mangel, stabilere Trainingsergebnisse zu erzielen, verlangsamt aber den Prozess proportional.",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Numerische Präzision für Berechnungen — beeinflusst VRAM und Geschwindigkeit direkt. <strong>fp16:</strong> Halbe Präzision, halbiert den Speicherbedarf. Gute Wahl für ältere Karten. <strong>bf16:</strong> Ebenfalls 16 Bit, aber stabiler. Unbedingt auf RTX 30er-Serie und neuer verwenden. <strong>fp8:</strong> 8-Bit-Präzision für extreme Ersparnis. Experimentell. <strong>no:</strong> Volle fp32-Präzision. Verbraucht doppelt so viel VRAM, ist aber numerisch absolut stabil.",
                "tf32": "TF32 erlauben",
                "tf32_help": "TensorFloat-32 — ein Rechenmodus exklusiv für RTX 30er-Serie und neuer. Bietet bis zu 8-fache Beschleunigung bei ML-Workloads mit kaum merklichem Qualitätsunterschied. <strong>Sollte auf RTX 30/40-Karten immer aktiviert bleiben.</strong>",
                "vae_batch": "VAE-Batch-Größe",
                "vae_batch_help": "Separate Batch-Größe nur für das VAE-Encoding. Ein niedrigerer Wert kann VRAM-Spitzen während des Encodings abfangen. Leer lassen, um die Trainings-Batch-Größe zu verwenden. Setze auf 1 für minimalen VRAM-Verbrauch.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max. Token-Länge",
                "max_token_length_help": "Maximale Anzahl an Token (Wörter/Teilwörter) in Captions. Der native Kontext von CLIP beträgt <strong>77 Token</strong> (75 usable + start/end markers). <strong>75:</strong> Standard, handles most captions without issue. <strong>150:</strong> Room for more detailed descriptions — good for complex scenes or thorough character breakdowns. <strong>225:</strong> Very long, super-detailed captions. Eats significantly more VRAM and slows down processing. Higher limits work through caption chunking techniques. Bump up to <strong>150</strong> or <strong>225</strong> if your captions regularly exceed ~60-70 words and truncation is cutting off important info.",
                "memory_saving": "Aggressive Memory Saving",
                "memory_saving_help": "Throws everything at VRAM savings: gradient checkpointing, CPU offloading, aggressive garbage collection. <strong>A must-have for 4GB-8GB cards</strong> — lets you train SDXL on hardware that would otherwise choke. Can cut VRAM usage by 30-50% depending on the model. The price? Training becomes 2-4× slower due to recomputation and shuffling data between CPU and GPU. Pairs well with a good <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref_label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention Backend</a> (SDPA or xFormers) for maximum savings. Got 16GB+ VRAM? Leave this off and enjoy faster training.",
                "seed": "Training Seed",
                "seed_help": "The random seed for all RNGs in training. Pick a specific seed and you get <strong>reproducibility</strong>: same config + same seed = identical results (assuming same hardware and software). Leave blank for random initialization — totally fine for everyday training. Useful when A/B testing settings, hunting bugs, or sharing reproducible configs with others. People often pick memorable numbers like <strong>42</strong>, <strong>1234</strong>, or <strong>0</strong>.",
                "seed_placeholder": "Random"
            },
            "flux": {
                "title": "Flux-spezifische Einstellungen",
                "max_seq_len": "Max. Sequenzlänge",
                "max_seq_len_help": "Maximale Token-Länge für den T5-Text-Encoder (Flux / SD3). Höhere Werte ermöglichen längere Prompts/Captions, erhöhen aber den VRAM-Verbrauch und verlangsamen das Training. Übliche Werte: <strong>256</strong> (Standard) oder <strong>512</strong>.",
                "guidance_scale": "Guidance-Skala",
                "guidance_scale_help": "Classifier-free Guidance für das Training von Flux- und SD3-Modellen. Gibt dem Modell vor, wie stark es sich während des Trainings an den Text-Prompt halten soll. <strong>3.5:</strong> Empfohlener Standard von Stability AI für SD3, funktioniert auch hervorragend für Flux. <strong>Niedriger (1-2):</strong> Kreativere/vielfältigere Ergebnisse, kann aber vom Prompt abweichen. <strong>Höher (5-7):</strong> Engere Prompt-Einhaltung, kann aber etwas steif wirken. Bei SD1.5/SDXL wird dies normalerweise nicht verwendet — dort greift die Guidance erst bei der Inferenz.",
                "weighting_scheme": "Gewichtungsschema",
                "weighting_scheme_help": "Wie Zeitschritte während des Trainings für Flow-Matching-Modelle (Flux, SD3) abgetastet werden. Dies entscheidet, welche Rauschpegel während des Trainings mehr Aufmerksamkeit erhalten. <strong>Keines (Uniform):</strong> Alle Rauschpegel werden gleich behandelt. Sicherer Standard. <strong>Logit Normal:</strong> Glockenkurven-Verteilung, zentriert auf <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> mit einer Breite, die durch <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a> festgelegt wird. <strong>Mode:</strong> Konzentriert sich stark auf einen bestimmten Zeitschritt, Streuung gesteuert durch <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a>. Diese Einstellungen betreffen nur Flux und SD3.",
                "logit_mean": "Logit Mean",
                "logit_mean_help": "Wo die Logit-Normal-Verteilung ihren Höhepunkt hat, wenn <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> verwendet wird. <strong>0.0:</strong> Zentriert auf den mittleren Zeitschritt (t=0.5). <strong>Negativ:</strong> Fokus auf rauschreichere Zeitschritte (grobe Struktur). <strong>Positiv:</strong> Fokus auf sauberere Zeitschritte (feine Details). Nur relevant, wenn Gewichtungsschema = \"Logit Normal\".",
                "logit_std": "Logit Std",
                "logit_std_help": "Wie weit die Logit-Normal-Verteilung gestreut ist, wenn <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> verwendet wird. <strong>Niedriger (0.5-0.8):</strong> Enger Fokus um <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a>. <strong>Höher (1.5-2.0):</strong> Breitere Streuung, nähert sich der gleichmäßigen Abtastung an. Nur relevant, wenn Gewichtungsschema = \"Logit Normal\".",
                "mode_scale": "Mode Scale",
                "mode_scale_help": "Wie stark sich die Mode-Verteilung konzentriert, wenn <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode</a> verwendet wird. <strong>Niedriger:</strong> Abtastung über mehr Zeitschritte verteilt. <strong>Höher:</strong> Abtastung konzentriert sich enger um den Modus. <strong>1.29:</strong> Der Wert aus dem originalen SD3-Paper. Nur relevant, wenn Gewichtungsschema = \"Mode\"."
            },
            "timestep": {
                "title": "Zeitschritt-Abtastung",
                "min": "Min. Zeitschritt",
                "min_help": "Minimaler Zeitschritt für die Abtastung.",
                "max": "Max. Zeitschritt",
                "max_help": "Maximaler Zeitschritt für die Abtastung.",
                "max_placeholder": "Optional",
                "min_placeholder": "Optional"
            },
            "caption": {
                "title": "Caption-Einstellungen",
                "extension": "Caption-Erweiterung",
                "extension_help": "Dateierweiterung für deine Caption-Dateien. Standard ist <strong>.txt</strong> — \"image001.png\" erwartet seine Caption also in \"image001.txt\". Unterstützt jede beliebige Erweiterung (.caption, .captions, .tags, etc.). Die Caption-Datei muss im selben Ordner wie das Bild liegen.",
                "weighted": "Gewichtete Captions",
                "weighted_help": "Analysiert Gewichte im A1111-Stil in Captions wie \"(wichtig:1.3)\" oder \"[weniger wichtig:0.7]\". Ermöglicht es, bestimmte Wörter während des Trainings zu betonen oder abzuschwächen. <strong>Gewicht > 1.0:</strong> Modell achtet mehr auf diese Token. <strong>Gewicht < 1.0:</strong> Modell achtet weniger darauf. Deaktiviere dies, wenn deine Captions wörtliche Klammern enthalten, die keine Gewichte darstellen."
            },
            "loss": {
                "title": "Loss & Optimierung",
                "optimization_title": "Loss-Optimierung",
                "type": "Loss-Typ",
                "type_help": "Die mathematische Funktion zur Berechnung der Differenz zwischen vorhergesagtem und Ziel-Rauschen.",
                "huber_c": "Huber C",
                "huber_c_help": "Schwellenwert-Parameter für Huber-Loss. Niedrigere Werte verhalten sich eher wie L1.",
                "min_snr": "Min SNR Gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio Gamma. Gewichtete Loss-Berechnung basierend auf Rauschpegeln zur Stabilisierung des Trainings.",
                "ip_noise": "IP Noise Gamma",
                "ip_noise_help": "Stärke des Input Perturbation Rauschens. Hilft gegen Überglättung.",
                "noise_offset": "Noise Offset",
                "noise_offset_help": "Fügt während des Trainings einen kleinen, konstanten Offset zum Rauschen hinzu, damit das Modell echtes Schwarz und echtes Weiß anstelle von verwaschenem Grau generieren kann. Standard-Diffusion hat Schwierigkeiten mit extrem dunklen/hellen Bereichen. <strong>0.035–0.05:</strong> Sichere Basiswerte. <strong>0.05–0.1:</strong> Aggressivere Korrektur. Am besten in Kombination mit <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a>.",
                "v_pred": "V-Pred-ähnlicher Loss",
                "v_pred_help": "Mischt Geschwindigkeitsvorhersage (Velocity) in das Standard-Rauschvorhersage-Training. Kann das Training glätten und Artefakte reduzieren. <strong>0:</strong> Reines Rauschen (Standard). <strong>1:</strong> Volle Velocity-Vorhersage. <strong>0.1–0.2:</strong> Subtile Wirkung bei Kompatibilität mit Standardmodellen.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Erweiterte Loss-Optionen",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> Passt den Noise-Scheduler so an, dass der letzte Zeitschritt reines Rauschen ist (SNR = 0), was bei echten Schwarztönen hilft. Wichtiger Partner für <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a>.<br><strong>Debiased Estimation:</strong> Korrigiert Verzerrungen bei der Zeitschritt-Abtastung. Gewichtete Loss-Berechnung für gleichmäßiges Lernen über alle Zeitschritte.<br><strong>Scale V-Pred Loss:</strong> Normalisiert den V-Vorhersage-Loss auf die Größenordnung des Rausch-Loss. Hauptsächlich für V-Pred-Basismodelle (SD 2.x) nützlich.<br><strong>Masked Loss (Alpha):</strong> Verwendet den PNG-Alpha-Kanal als Trainingsmaske — das Modell lernt nur von nicht-transparenten Pixeln. Perfekt für Charakter-Training.",
                "presets": "Loss-Voreinstellungen",
                "presets_help": "Schnelles Anwenden empfohlener Loss-Einstellungen für verschiedene Trainingsziele.",
                "preset_default": "Standard",
                "preset_balanced": "Ausgewogen",
                "preset_quality": "Qualität",
                "preset_dark_light": "Dunkel/Hell"
            },
            "samples": {
                "prompts_title": "Beispiel-Prompts",
                "inference_title": "Inferenz-Einstellungen",
                "schedule_title": "Generierungs-Zeitplan",
                "generated_title": "Generierte Beispiele",
                "gallery_placeholder": "Generierte Beispiele während des Trainings werden hier angezeigt...",
                "prompts": "Beispiel-Prompts",
                "prompts_help": "Gib Prompts für Beispielbilder während des Trainings an. Ein Prompt pro Zeile.",
                "neg_prompt": "Negativer Prompt",
                "neg_prompt_help": "Der für die Beispielgenerierung verwendete negative Prompt.",
                "every_n_epochs": "Alle N Epochen",
                "every_n_epochs_help": "Generiere Beispiele nach jeder N-ten Epoche.",
                "every_n_steps": "Alle N Schritte",
                "every_n_steps_help": "Generiere Beispiele nach jeder N-ten Schritt.",
                "sampler": "Sampler",
                "sampler_help": "Der Sampler, der für die Generierung von Beispielbildern verwendet werden soll.",
                "inference_steps": "Inferenz-Schritte",
                "inference_steps_help": "Anzahl der Denoising-Schritte für die Beispielgenerierung.",
                "cfg_scale": "CFG-Skala",
                "cfg_scale_help": "Classifier Free Guidance Skala für die Beispielgenerierung.",
                "num_images": "Bilder pro Prompt",
                "num_images_help": "Anzahl der zu generierenden Bilder für jeden Prompt.",
                "seed": "Seed",
                "seed_help": "Seed für reproduzierbare Beispielgenerierung. -1 für Zufall.",
                "every_n_epochs_placeholder": "Optional",
                "every_n_steps_placeholder": "Optional",
                "neg_prompt_placeholder": "Optionaler negativer Prompt",
                "prompts_placeholder": "Ein Prompt pro Zeile..."
            },
            "metadata": {
                "title": "Modell-Metadaten",
                "title_help": "Öffentlicher Name der LoRA.",
                "author": "Autor",
                "author_help": "Dein Name.",
                "desc": "Beschreibung",
                "desc_help": "Öffentliche Beschreibung.",
                "license": "Lizenz",
                "license_help": "Nutzungslizenz.",
                "tags": "Tags",
                "tags_help": "Such-Tags.",
                "comment": "Trainings-Kommentar",
                "comment_help": "Private Notizen, die in den Metadaten gespeichert werden.",
                "comment_placeholder": "z.B. Trainiert auf 50 Bildern von Charakter X",
                "desc_placeholder": "Beschreibe deine LoRA...",
                "license_placeholder": "z.B. CreativeML Open RAIL-M",
                "tags_placeholder": "z.B. Charakter, Stil, Anime"
            },
            "progress": {
                "epoch_prefix": "Epoche",
                "loss_label": "LOSS",
                "start": "Training starten",
                "stop": "Training stoppen",
                "idle": "System bereit",
                "caching_latents": "Latents werden gecached",
                "latents_cache": "Latents-Cache"
            },
            "conversion": {
                "title": "Modell-Konvertierung",
                "subtitle": "LoRA-Modelle zwischen Formaten konvertieren (Diffusers &harr; Kohya/LDM)",
                "card_title": "Konvertierungs-Tool",
                "help_text": "Verwende dieses Tool, um Kompatibilitätsprobleme mit AUTOMATIC1111/Forge zu beheben. Es konvertiert Schlüssel im \"Diffusers\"-Stil in den \"Kohya/LDM\"-Stil und umgekehrt.",
                "input_model": "Eingabemodell (.safetensors)",
                "btn_refresh": "Aktualisieren",
                "model_architecture": "Modellarchitektur",
                "architecture_help": "Gibt die Architekturlogik an, die für das korrekte Key-Mapping erforderlich ist. Die Auswahl der falschen Architektur führt zu einem nicht funktionsfähigen Modell.",
                "target_format": "Zielformat",
                "target_format_kohya": "Kohya / LDM (für A1111, Forge, ComfyUI)",
                "output_filename": "Ausgabedateiname (Optional)",
                "output_filename_placeholder": "Leer lassen für automatische Benennung (z.B. model_converted.safetensors)",
                "btn_convert": "Modell konvertieren",
                "success": "Konvertierung erfolgreich!"
            }
        },
        "console": {
            "title": "Systemkonsole",
            "subtitle": "Echtzeit-Trainingslogs und Systemausgabe",
            "output_title": "Konsolenausgabe",
            "clear": "Konsole leeren",
            "status": "Status",
            "step": "Schritt",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU-Last",
            "cpu_load": "CPU-Last",
            "ram": "RAM",
            "waiting": "Warte auf Trainingsstart..."
        },
        "metadata_editor": {
            "title": "Metadaten-Editor",
            "subtitle": "Verwalte eingebettete Metadaten für LoRA- und Checkpoint-Dateien",
            "select_file": "Datei auswählen",
            "select_placeholder": "Datei auswählen...",
            "select_help": "Wähle eine Modell- oder Metadatendatei, um ihre internen Metadaten anzuzeigen und zu bearbeiten.",
            "btn_load": "Metadaten laden",
            "load_help": "Lade Metadaten aus der ausgewählten Datei zum Bearbeiten.",
            "btn_save": "Metadaten speichern"
        },
        "modals": {
            "btn_cancel": "Abbrechen",
            "preset_title": "Voreinstellung",
            "preset_name": "Voreinstellungsname",
            "preset_placeholder": "Voreinstellungsnamen eingeben...",
            "btn_create": "Voreinstellung erstellen",
            "delete_title": "Löschung bestätigen",
            "delete_confirm": "Bist du sicher, dass du löschen möchtest",
            "delete_warning": "Diese Aktion kann nicht rückgängig gemacht werden. Die Datei wird permanent von der Festplatte entfernt.",
            "btn_delete": "Löschen",
            "stop_title": "Training stoppen",
            "stop_confirm": "Bist du sicher, dass du den Trainingsprozess stoppen möchtest?",
            "stop_warning": "Jeder nicht gespeicherte Fortschritt geht verloren. Der aktuelle Checkpoint wird nicht gespeichert.",
            "btn_stop": "Training stoppen",
            "update_title": "Updates anwenden",
            "update_confirm": "Bist du sicher, dass du {count} Update(s) anwenden möchtest?",
            "update_warning": "Bitte starten Sie die Anwendung nach Abschluss des Updates manuell neu."
        },
        "notifications": {
            "updates_available": "Updates verfügbar!",
            "no_updates": "Keine Updates gefunden.",
            "update_check_failed": "Update-Prüfung fehlgeschlagen",
            "connection_error": "Verbindungsfehler",
            "update_complete": "Update abgeschlossen!",
            "update_partial": "Update teilweise abgeschlossen",
            "update_failed": "Update fehlgeschlagen",
            "select_preset_load": "Bitte wähle eine Voreinstellung zum Laden aus.",
            "preset_loaded": "Voreinstellung geladen: {name}",
            "preset_load_failed": "Voreinstellung konnte nicht geladen werden",
            "config_saved": "Konfiguration gespeichert!",
            "config_save_failed": "Konfiguration konnte nem gespeichert werden.",
            "config_save_error": "Fehler beim Speichern der Konfiguration: {error}",
            "hardware_optimized": "Hardware-Optimierungen angewendet!",
            "hardware_optimize_failed": "Hardware-Optimierungen konnten nicht abgerufen werden",
            "auto_adjust_failed": "Auto-Anpassung fehlgeschlagen",
            "auto_adjust_no_changes": "Auto-Anpassung: keine Änderungen vorgeschlagen (Datensatz fehlt?)",
            "auto_adjust_success": "Auto-Anpassung angewendet.",
            "auto_adjust_success_count": "Auto-Anpassung angewendet (Datensatz: {count} Bilder).",
            "select_preset_save": "Bitte wähle eine Voreinstellung zum Speichern aus.",
            "preset_saved": "Voreinstellung gespeichert: {name}",
            "preset_created": "Voreinstellung erstellt: {name}",
            "preset_save_failed": "Voreinstellung konnte nicht gespeichert werden.",
            "preset_save_error": "Fehler beim Speichern der Voreinstellung: {error}",
            "preset_name_empty": "Voreinstellungsname darf nicht leer sein.",
            "preset_name_invalid": "Voreinstellungsname darf csak Buchstaben, Zahlen, Unterstriche und Bindestriche enthalten.",
            "preset_create_failed": "Voreinstellung konnte nicht erstellt werden.",
            "preset_create_error": "Fehler beim Erstellen der Voreinstellung: {error}",
            "select_preset_delete": "Bitte wähle eine Voreinstellung zum Löschen aus.",
            "preset_delete_failed": "Voreinstellung konnte nicht gelöscht werden.",
            "preset_deleted": "Voreinstellung gelöscht: {name}",
            "preset_delete_error": "Fehler beim Löschen der Voreinstellung: {error}",
            "training_started": "Training gestartet! Job-ID: {job_id}",
            "training_start_error": "Fehler: {error}",
            "training_start_failed": "Fehler beim Starten des Trainings: {error}",
            "metadata_saved": "Metadaten erfolgreich gespeichert!",
            "metadata_load_failed": "Metadaten konnten nicht geladen werden",
            "metadata_save_failed": "Metadaten konnten nicht gespeichert werden",
            "metadata_save_error": "Fehler beim Speichern der Metadaten: {error}",
            "select_input_file": "Bitte wähle eine Eingabedatei aus",
            "conversion_success": "Konvertierung erfolgreich!",
            "conversion_failed": "Konvertierung fehlgeschlagen: {error}",
            "conversion_error": "Konvertierungsfehler: {error}",
            "output_load_failed": "Ausgabedateien konnten nicht geladen werden: {error}"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Option auswählen...",
            "unknown_error": "Unbekannter Fehler",
            "loading": "Lädt...",
            "starting": "Startet...",
            "stopped": "Gestoppt",
            "converting": "Konvertiert...",
            "no_files_found": "Keine Dateien gefunden",
            "select_file": "-- Datei auswählen --",
            "select_output_file": "Datei aus Ausgaben auswählen..."
        }
    }
}