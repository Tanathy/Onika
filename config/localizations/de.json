{
    "language_name": "Deutsch",
    "ui": {
        "sidebar": {
            "training": "Training",
            "console": "Konsole",
            "metadata": "Metadaten",
            "updates": "Updates",
            "system": "System",
            "connecting": "Verbinde...",
            "language": "Sprache"
        },
        "updates": {
            "title": "System-Updates",
            "subtitle": "Halte deine Onika-Installation auf dem neuesten Stand",
            "check_status_default": "Prüfe auf Updates, um verfügbare Änderungen zu sehen.",
            "btn_check": "Nach Updates suchen",
            "btn_checking": "Prüfe...",
            "btn_apply": "Updates anwenden",
            "btn_applying": "Wende an...",
            "details_title": "Update-Details",
            "col_file": "Dateipfad",
            "col_status": "Status",
            "log_title": "Update-Log",
            "status_uptodate": "Dein System ist auf dem neuesten Stand.",
            "status_failed": "Fehler beim Prüfen auf Updates.",
            "status_error": "Fehler bei der Verbindung zum Update-Server.",
            "status_found": "{count} Update(s) verfügbar gefunden.",
            "checked_at": "Geprüft um: {time}",
            "confirm_apply": "Bist du sicher, dass du {count} Update(s) anwenden möchtest?",
            "apply_success": "Updates erfolgreich angewendet!",
            "apply_partial": "Updates mit einigen Fehlern abgeschlossen.",
            "apply_failed": "Fehler beim Anwenden der Updates.",
            "apply_error": "Fehler beim Anwenden der Updates."
        },
        "training": {
            "title": "Training",
            "subtitle": "Konfiguriere und starte dein LoRA/LyCORIS-Training",
            "preset": {
                "label": "Voreinstellung",
                "select_placeholder": "Voreinstellung wählen...",
                "btn_load": "Laden",
                "btn_save": "Voreinstellung speichern",
                "btn_new": "Neue Voreinstellung",
                "btn_delete": "Löschen",
                "help": "Lädt eine Voreinstellung aus <span class=\"mono\">presets/</span> und wendet sie auf das Formular an. Voreinstellungen sind eine schnelle Möglichkeit, Profile zu wechseln. Voreinstellung speichern überschreibt die ausgewählte Voreinstellung. Neue Voreinstellung erstellt eine neue mit einem benutzerdefinierten Namen. Löschen entfernt die ausgewählte Voreinstellung."
            },
            "tabs": {
                "model": "Modell",
                "network": "Netzwerk",
                "dataset": "Datensatz",
                "text_encoder": "Text-Encoder",
                "aug": "Augmentierung & Caching",
                "learning": "Lernen",
                "advanced": "Erweitert",
                "samples": "Samples",
                "metadata": "Metadaten"
            },
            "model": {
                "base_model": "Basismodell",
                "base_model_help": "Das Grundmodell für das Training. Die Auswahl eines Modells, das deinem Zielstil nahekommt, verbessert die Konvergenzgeschwindigkeit. Stelle sicher, dass die Architektur zu deinen Trainingseinstellungen passt (z.B. SDXL-Basis für SDXL-Training). Training auf einem Basismodell wie SDXL 1.0 ist generell stabiler als die Verwendung eines stark feinabgestimmten \"gebackenen\" Modells.",
                "architecture": "Modellarchitektur",
                "architecture_help": "Gibt die Architekturlogik (SDXL, SD1.5, Flux) an, die für das Laden der Gewichte und die Latent-Verarbeitung erforderlich ist. Die Auswahl einer Architektur, die nicht zum Basismodell passt, führt zu Form-Mismatch-Fehlern.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-Optimierung für meine Hardware",
                "optimize_help": "Passt automatisch Präzision, Quantisierung und Speichereinstellungen basierend auf deinem erkannten GPU-VRAM an.",
                "btn_adjust": "Auto-Anpassung (Dataset-bewusst)",
                "adjust_help": "Analysiert deinen Datensatz (Bildanzahl/Größen + Beschriftungen) und empfiehlt stabile Trainingseinstellungen für deine gewählte Architektur. Wendet Änderungen wie eine Voreinstellung an.",
                "quantization": "Modellquantisierung (Q-LoRA)",
                "quantization_none": "Keine (Standard fp16/bf16)",
                "quantization_8bit": "8-Bit (Niedriger VRAM)",
                "quantization_4bit": "4-Bit (Extrem niedriger VRAM)",
                "quantization_help": "Reduziert die Modellpräzision auf 8-Bit oder 4-Bit, um VRAM-Anforderungen erheblich zu senken und Training auf Hardware mit nur 8GB VRAM zu ermöglichen. Beachte, dass niedrigere Präzision die Genauigkeit leicht beeinträchtigen kann und die `bitsandbytes`-Bibliothek erfordert.",
                "output_name": "Ausgabename",
                "output_name_placeholder": "z.B. character_lora_v1",
                "output_name_help": "Das Dateinamenpräfix für gespeicherte LoRA-Adapter. Verwende eindeutige Namen, um verschiedene Versionen zu organisieren und das Überschreiben vorheriger Ergebnisse zu verhindern.",
                "output_dir": "Ausgabeverzeichnis",
                "output_dir_help": "Der Zielordner für Trainingsergebnisse. Stelle sicher, dass ausreichend Speicherplatz verfügbar ist, um Trainingsunterbrechungen zu vermeiden.",
                "save_precision": "Speicherpräzision",
                "save_precision_help": "Die Bittiefe für gespeicherte Modelldateien. `float16` ist der Standard für ein Gleichgewicht von Größe und Präzision, während `float32` maximale Genauigkeit auf Kosten deutlich größerer Dateigrößen bietet.",
                "save_precision_auto": "AUTO",
                "save_precision_fp16": "float16",
                "save_precision_bf16": "bf16",
                "save_precision_fp32": "float32",
                "save_format": "Speicherformat",
                "save_format_help": "Das Dateiformat für die Ausgabe. `safetensors` wird wegen seiner Sicherheit und Ladegeschwindigkeit empfohlen. `ckpt` ist ein Legacy-Format, und `diffusers` speichert das Modell als Verzeichnisstruktur.",
                "save_format_safetensors": "safetensors",
                "save_format_ckpt": "ckpt",
                "save_format_diffusers": "diffusers",
                "save_epochs": "Speichern alle N Epochen",
                "save_epochs_help": "Häufigkeit der Checkpoint-Speicherungen während des Trainings. Regelmäßiges Speichern ermöglicht Wiederherstellung nach Abstürzen und bietet mehrere Versionen zur Bewertung von Overfitting.",
                "save_steps": "Speichern alle N Schritte",
                "save_steps_placeholder": "Optional",
                "save_steps_help": "Alternative Häufigkeitskontrolle basierend auf Trainingsschritten statt Epochen.",
                "resume": "Von Checkpoint fortsetzen",
                "resume_placeholder": "z.B. latest oder path/to/checkpoint-1000",
                "resume_help": "Setzt das Training von einem zuvor gespeicherten Zustand fort. Beachte, dass das Ändern von Kernparametern wie Lernrate oder Rang beim Fortsetzen zu Trainingsinstabilität führen kann.",
                "save_best": "Nur beste Modelle speichern (niedrigster Loss)",
                "save_best_help": "Behält nur den Checkpoint mit dem niedrigsten aufgezeichneten Loss, um Speicherplatz zu sparen. Beachte, dass der niedrigste Loss nicht immer mit der besten visuellen Qualität korreliert.",
                "checkpoints_limit": "Checkpoints Gesamtlimit",
                "checkpoints_limit_placeholder": "Optional",
                "checkpoints_limit_help": "Die maximale Anzahl beizubehaltender Checkpoints. Ältere Checkpoints werden automatisch gelöscht, um die Speichernutzung zu verwalten."
            },
            "network": {
                "title": "Netzwerkeinstellungen",
                "type": "Netzwerktyp",
                "type_help": "Die Architektur des Adapters. LoRA ist der Industriestandard. LoHa und LoKr bieten höhere Ausdruckskraft, erfordern aber möglicherweise sorgfältigere Abstimmung. OFT ist darauf ausgelegt, Hypersphärenenergie zu erhalten, was besonders für Flux-Modelle effektiv ist.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonales Finetuning)",
                "module": "Netzwerkmodul",
                "module_help": "Das interne Python-Modul für den Adapter. Dies ist eine erweiterte Einstellung und sollte normalerweise nicht geändert werden.",
                "dim": "Network Dim (Rang)",
                "dim_help": "Die Kapazität des Adapters. Höhere Werte (z.B. 128) ermöglichen detaillierteres Lernen, erhöhen aber das Risiko von Overfitting und führen zu größeren Dateigrößen. Niedrigere Werte (z.B. 16) generalisieren besser und erzeugen kleinere Dateien.",
                "alpha": "Network Alpha",
                "alpha_help": "Ein Skalierungsfaktor, der verhindert, dass Gewichtsaktualisierungen zu aggressiv sind. Eine gängige Faustregel ist, Alpha auf die Hälfte von Network Dim für Stabilität zu setzen oder gleich Dim für stärkere Effekte.",
                "algo": "LyCORIS-Algorithmus",
                "algo_help": "LyCORIS-Algorithmusvariante (nur relevant für LyCORIS/LoHa-Modi). Verschiedene Algorithmen tauschen Ausdruckskraft gegen Stabilität aus. Wenn du dir nicht sicher bist, beginne mit <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span> und wechsle nur, wenn du einen klaren Vorteil messen kannst.",
                "conv_dim": "Conv Rang (Dim)",
                "conv_dim_help": "Optionaler Conv-Adapter-Rang für LoCon/LyCORIS. Niedrige Werte fügen einen kleinen konvolutionalen Kapazitätsschub hinzu; hohe Werte erhöhen VRAM und können Textur/Details schnell überanpassen. Lasse es leer, wenn du keine conv-basierte Methode explizit verwendest.",
                "conv_alpha": "Conv Alpha",
                "conv_alpha_help": "Optionale Conv-Adapter-Skalierung (Alpha). Niedrigeres Alpha macht den Conv-Teil sanfter; höheres Alpha macht ihn stärker und kann destabilisieren oder überschießen. Typischerweise <= Conv Dim; wenn Conv-Effekte zu stark aussehen, reduziere Alpha vor Dim.",
                "dora_wd": "DoRA Weight Decay",
                "dora_wd_help": "Weight Decay speziell auf DoRA-Magnitudenvektoren angewendet.",
                "network_dropout": "Network Dropout",
                "network_dropout_help": "Lässt zufällig Neuronenausgaben innerhalb des Adapters fallen, um die Robustheit zu verbessern.",
                "rank_dropout": "Rank Dropout",
                "rank_dropout_help": "Lässt zufällig einzelne Rangdimensionen als Form der Regularisierung fallen.",
                "module_dropout": "Module Dropout",
                "module_dropout_help": "Deaktiviert zufällig ganze Module während des Trainings, um Overfitting zu verhindern und die Generalisierung zu verbessern.",
                "lora_blocks": "LoRA-Blöcke",
                "lora_blocks_help": "Ermöglicht das Anvisieren spezifischer Blöcke (z.B. Mid-Blöcke) innerhalb der Modellarchitektur.",
                "lora_layers": "LoRA-Layer",
                "lora_layers_help": "Ermöglicht das Anvisieren spezifischer Layer (z.B. Attention-Layer) für chirurgisches Finetuning.",
                "advanced_lora": "Erweiterte LoRA-Optionen",
                "lycoris_settings": "LyCORIS-Einstellungen",
                "args": "Netzwerkargumente",
                "args_help": "Zusätzliche Argumente für das Netzwerkmodul, als kommagetrennte Schlüssel=Wert-Paare angegeben. Verwende dies für spezialisierte Optionen wie Dropout oder Dekomposition. Falsche Argumente können zu Trainingsinstabilität führen."
            },
            "dataset": {
                "dataset_type": "Datensatztyp",
                "dataset_type_hub": "Hugging Face Hub",
                "dataset_type_local": "Lokale Dateien",
                "dataset_type_help": "Select the source of your dataset. Hugging Face Hub allows for easy sharing and downloading of datasets. Local Files lets you use files from your device.",
                "path": "Dataset Path",
                "path_help": "Das Verzeichnis mit Trainingsbildern und den zugehörigen Beschriftungsdateien (z.B. .txt). Die Qualität des Datensatzes ist der kritischste Faktor für den Trainingserfolg; stelle sicher, dass Beschriftungen die Bilder genau beschreiben.",
                "repo_id": "Repository-ID",
                "repo_id_placeholder": "z.B. username/repo",
                "repo_id_help": "Die Hugging Face Repository-ID zum Herunterladen von Datensätzen. Leer lassen, um lokale Dateien zu verwenden.",
                "local_dir": "Lokales Verzeichnis",
                "local_dir_help": "Der Ordner auf deinem Gerät mit den Datensatzdateien. Stelle sicher, dass der Ordner zugänglich ist und das richtige Datenformat enthält.",
                "image_folder": "Bildordner",
                "image_folder_help": "Das Unterverzeichnis innerhalb des lokalen Verzeichnisses, das die Bilddateien für das Training enthält.",
                "annotation_file": "Annotationsdatei",
                "annotation_file_placeholder": "z.B. captions.txt",
                "annotation_file_help": "Die Datei mit Annotationen oder Beschriftungen für die Bilder. Wird für Modelle benötigt, die Texteingaben erfordern.",
                "resolution": "Auflösung",
                "resolution_help": "Die Zielauflösung für das Training. Höhere Auflösungen erfassen mehr Details, benötigen aber mehr VRAM. Stelle sicher, dass die Auflösung für die Modellarchitektur geeignet ist (z.B. 1024x1024 für SDXL, 512x512 für SD1.5).",
                "batch_size": "Batch-Größe",
                "batch_size_help": "Die Anzahl gleichzeitig verarbeiteter Bilder. Größere Batch-Größen können zu schnellerem Training und glatteren Gradienten führen, erhöhen aber den VRAM-Verbrauch deutlich.",
                "max_epochs": "Max. Epochen",
                "max_epochs_help": "Die Gesamtzahl vollständiger Durchläufe durch den Datensatz. Für LoRA-Training sind 10-20 Epochen typischerweise ausreichend. Zu viele Epochen können zu Overfitting und \"frittierten\" Bildern führen.",
                "max_steps": "Max. Schritte",
                "max_steps_help": "Ein optionales hartes Limit für die Gesamtzahl der Trainingsschritte.",
                "max_steps_placeholder": "Optional",
                "bucketing": "Seitenverhältnis-Bucketing aktivieren",
                "bucketing_help": "Gruppiert Bilder automatisch nach Seitenverhältnis, um unnötiges Zuschneiden zu vermeiden und die Originalkomposition deiner Trainingsdaten zu bewahren.",
                "bucket_steps": "Bucket-Auflösungsschritt",
                "bucket_steps_help": "Die Rastergröße für Bucket-Dimensionen. 64 ist der Standardwert für die meisten Architekturen.",
                "min_bucket": "Minimale Bucket-Auflösung",
                "min_bucket_help": "Die minimal erlaubte Auflösung für einen Bild-Bucket. Verhindert die Verwendung sehr kleiner oder unscharfer Bilder während des Trainings.",
                "max_bucket": "Maximale Bucket-Auflösung",
                "max_bucket_help": "Die maximal erlaubte Auflösung für einen Bild-Bucket, hilft Out-of-Memory (OOM)-Fehler bei außergewöhnlich großen Bildern zu vermeiden.",
                "center_crop": "Center Crop (Smart 1:1)",
                "center_crop_help": "Wenn aktiviert, werden Bilder, die fast quadratisch sind (z.B. 1210x1280), auf ein perfektes 1:1-Verhältnis zentriert zugeschnitten. Empfohlen für Charaktertraining, um konsistente Bildausschnitte sicherzustellen.",
                "no_upscale": "Keine Hochskalierung",
                "no_upscale_help": "Verhindert die Hochskalierung kleiner Bilder auf Bucket-Dimensionen und vermeidet potenzielle Artefakte.",
                "dreambooth": "DreamBooth & Prior Preservation",
                "prior_preservation": "Prior Preservation aktivieren (Reg Images)",
                "prior_preservation_help": "Verwendet Regularisierungsbilder, um zu verhindern, dass das Modell sein ursprüngliches Verständnis der Klasse (z.B. \"Person\") verliert, während es eine spezifische Instanz lernt. Essentiell für die Beibehaltung des allgemeinen Wissens des Modells, erhöht aber die Trainingszeit.",
                "num_class_images": "Anzahl Klassenbilder",
                "num_class_images_help": "Die Zielanzahl von Regularisierungsbildern. Eine gängige Empfehlung sind 100 Bilder pro Instanzbild.",
                "instance_prompt": "Instanz-Prompt",
                "instance_prompt_help": "Die Kombination aus einem eindeutigen Triggerwort und einem Klassenwort (z.B. \"sks person\"), das das spezifische trainierte Subjekt identifiziert.",
                "instance_prompt_placeholder": "e.g. a photo of sks person",
                "class_prompt": "Klassen-Prompt",
                "class_prompt_help": "Das generische Klassenwort (z.B. \"person\"), das zur Identifikation von Regularisierungsbildern verwendet wird.",
                "class_prompt_placeholder": "e.g. a photo of a person",
                "reg_dir": "Reg-Images-Verzeichnis",
                "reg_dir_help": "Der Ordner mit Regularisierungsbildern, wird nur verwendet, wenn Prior Preservation aktiviert ist.",
                "reg_dir_placeholder": "Optional",
                "auto_gen": "Reg-Images automatisch generieren",
                "auto_gen_help": "Generiert automatisch Regularisierungsbilder, wenn sie noch nicht im angegebenen Verzeichnis vorhanden sind.",
                "gen_settings": "Generierungseinstellungen",
                "neg_class_prompt": "Negativer Klassen-Prompt",
                "neg_class_prompt_help": "Negativer Prompt für die Generierung von Regularisierungsbildern.",
                "neg_class_prompt_placeholder": "Optional negative prompt",
                "guidance": "Guidance Scale",
                "guidance_help": "CFG-Skala für die Generierung von Regularisierungsbildern.",
                "steps": "Reg-Schritte",
                "steps_help": "Anzahl der Sampling-Schritte für die Generierung von Regularisierungsbildern.",
                "scheduler": "Reg-Scheduler",
                "scheduler_help": "Sampler für die Generierung von Regularisierungsbildern.",
                "seed": "Reg-Seed",
                "seed_help": "Seed für die Generierung von Regularisierungsbildern. -1 für zufällig.",
                "sample_warning": "Tipp: Wenn sowohl \"Sample Every N Steps\" als auch \"Sample Every N Epochs\" leer gelassen werden, generiert Onika keine Sample-Bilder (spart viel Zeit). Auf Low- und Mid-Range-GPUs wird das Generieren von Samples während des Trainings generell nicht empfohlen.",
                "train_val_split": "Train/Val-Split",
                "train_val_split_help": "Das Verhältnis von Trainings- zu Validierungsdaten. Ein üblicher Split ist 80% für Training und 20% für Validierung.",
                "shuffle": "DataLoader Shuffle",
                "shuffle_help": "Randomisiert die Reihenfolge der Bilder in jeder Epoche, um zu verhindern, dass das Modell die Sequenz des Datensatzes lernt.",
                "workers": "DataLoader Workers",
                "workers_help": "Die Anzahl der CPU-Threads für das Laden und Vorverarbeiten von Daten. Höhere Werte können die GPU schneller füttern, aber zu Systemverzögerungen führen, wenn zu hoch gesetzt.",
                "num_workers": "Anzahl Workers",
                "num_workers_help": "Die Anzahl der Subprozesse für das Laden von Daten. Mehr Workers können das Laden beschleunigen, benötigen aber mehr Systemressourcen.",
                "persistent_workers": "Persistente DataLoader-Workers",
                "persistent_workers_help": "Hält CPU-Datenlade-Workers zwischen Epochen aktiv, um die Trainingsgeschwindigkeit zu verbessern, auf Kosten erhöhter RAM-Nutzung.",
                "pin_memory": "Pin Memory",
                "pin_memory_help": "Wenn gesetzt, kopiert der Data-Loader Tensoren in CUDA-Pinned-Memory. Kann die Performance beim Transfer von Daten zur GPU verbessern."
            },
            "text_encoder": {
                "title": "Text-Encoder",
                "ti_title": "Textual Inversion (Pivotal Tuning)",
                "weighting_title": "Caption-Gewichtung",
                "train": "Text-Encoder trainieren",
                "train_help": "Aktiviert das Training des Text-Encoders zusammen mit dem UNet, was die Prompt-Treue verbessern kann, aber die Flexibilität oder Editierbarkeit des Modells reduzieren könnte.",
                "clip_skip": "Clip Skip",
                "clip_skip_help": "Überspringt die obersten N Layer des CLIP Text-Encoders. SD1.5 verwendet typischerweise einen Skip von 1, während SDXL normalerweise 0 verwendet. Falsche Werte können zu schlechtem Prompt-Verständnis führen.",
                "train_ti": "Textual Inversion trainieren",
                "train_ti_help": "Trainiert neue Tokens (Textual Inversion), um spezifische Wörter oder Konzepte zum Vokabular des Modells hinzuzufügen.",
                "ti_frac": "TI-Trainingsanteil",
                "ti_frac_help": "Der Prozentsatz der gesamten Trainingsepochen, während denen Textual Inversion Training aktiv ist.",
                "te_frac": "TE-Trainingsanteil",
                "te_frac_help": "Der Prozentsatz der gesamten Trainingsepochen, während denen Text-Encoder-Training aktiv ist.",
                "emphasis": "Standard-Emphasis",
                "emphasis_help": "Der Standard-Multiplikator für betonte Tags, wenn kein explizites Gewicht angegeben ist.",
                "de_emphasis": "Standard-De-Emphasis",
                "de_emphasis_help": "Der Standard-Multiplikator für entbetonte Tags, wenn kein explizites Gewicht angegeben ist.",
                "enable_weighted": "Gewichtete Captions aktivieren",
                "enable_weighted_help": "Aktiviert die Verwendung gewichteter Syntax (z.B. \"(word:1.1)\") in Captions für präzise Kontrolle über die Wichtigkeit spezifischer Begriffe.",
                "new_tokens": "Neue Tokens pro Abstraktion",
                "new_tokens_help": "Die Anzahl der Vektoren, die jedem neuen Token zugewiesen werden. Mehr Vektoren können mehr Details erfassen, sind aber schwieriger effektiv zu trainieren.",
                "token_abs": "Token-Abstraktion",
                "token_abs_help": "Der Platzhalter-String (z.B. \"TOK\"), der verwendet wird, um das neue Konzept in Captions darzustellen.",
                "train_text_encoder": "Text-Encoder trainieren",
                "train_text_encoder_help": "Ob der Text-Encoder zusammen mit dem UNet trainiert werden soll. Empfohlen für bessere Prompt-Treue.",
                "text_encoder_lr": "Text-Encoder-Lernrate",
                "text_encoder_lr_help": "Die Lernrate für den Text-Encoder. Normalerweise niedriger als die UNet-Lernrate."
            },
            "aug": {
                "aug_title": "Bildaugmentierung",
                "enable_augmentation": "Augmentierung aktivieren",
                "enable_augmentation_help": "Aktiviert Datenaugmentierungstechniken während des Trainings zur Verbesserung der Modellgeneralisierung.",
                "aug_mode": "Augmentierungsmodus",
                "aug_mode_help": "Entscheidet, wann Augmentierungen während des Trainings einsetzen. <strong>Immer:</strong> Jedes Bild wird bei jedem Schritt augmentiert — maximale Vielfalt, kann aber etwas verlangsamen. <strong>Nur pro Epoche:</strong> Neue Augmentierungen werden einmal pro Epoche gewürfelt und bleiben in diesem Durchgang konsistent. Hält die Dinge stabil, während es zwischen Epochen variiert. <strong>Zufällige Wahrscheinlichkeit:</strong> Jedes Bild kann bei jedem Schritt augmentiert werden oder nicht, basierend auf individuellen Einstellungen wie <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>. Meistens funktionieren \"Immer\" oder \"Nur pro Epoche\" gut.",
                "color_aug": "Farbaugmentierungsstärke",
                "color_aug_help": "Wie aggressiv Farben (Farbton, Sättigung, Helligkeit) während des Trainings variiert werden. <strong>0.0:</strong> Farben bleiben genau wie sie sind — keine Manipulation. <strong>0.3-0.5:</strong> Sanfte Variation, funktioniert super für die meisten Datensätze und hilft dem Modell, exakte Farben nicht auswendig zu lernen. <strong>0.7-1.0:</strong> Starke Farbverschiebungen. Kann seltsame Kombinationen erzeugen, pusht aber wirklich die Generalisierung. Besonders praktisch für Charakter- oder Stil-LoRAs, wo du willst, dass das Modell Formen und Konzepte beherrscht, ohne an bestimmten Paletten hängenzubleiben. Abschalten, wenn Farbtreue wichtig ist (Markenfarben, spezifische Outfits usw.).",
                "flip_aug": "Flip-Aug-Wahrscheinlichkeit",
                "flip_aug_help": "Chance, jedes Bild horizontal zu spiegeln. <strong>0.5</strong> = 50/50 Münzwurf, verdoppelt im Wesentlichen deinen Datensatz mit gespiegelten Versionen. <strong>0.0</strong> = überhaupt keine Spiegelung. <strong>Achtung:</strong> Setze dies auf 0, wenn deine Bilder haben: • <strong>Text oder Logos</strong> (wären rückwärts) • <strong>Asymmetrische Sachen</strong> (Autos, Gesichter mit einer markanten Seite) • <strong>Richtungsbezogene Inhalte</strong> (linke vs. rechte Hände, spezifische Posen). Funktioniert super für symmetrische Subjekte wie zentrierte Porträts, abstrakte Kunst oder viele Anime-Charaktere. Einer der besten Tricks gegen Overfitting bei kleinen Datensätzen.",
                "random_flip": "Zufälliges Spiegeln",
                "random_flip_help": "Spiegelt Bilder zufällig horizontal und/oder vertikal.",
                "crop_scale": "Zufällige Crop-Skala",
                "crop_scale_help": "Minimales Zoom-Level für zufälliges Zuschneiden. <strong>1.0:</strong> Kein Zuschneiden — Bilder bleiben in voller Größe. <strong>0.8:</strong> Schneidet irgendwo zwischen 80-100% zu, gibt subtile Zoom-Variationen. <strong>0.5:</strong> Dramatischer — kann bis zu 2× hineinzoomen und dem Modell völlig unterschiedliche Bildausschnitte zeigen. Zuschneiden lehrt das Modell, Subjekte in verschiedenen Maßstäben und Kompositionen zu handhaben. Super nützlich, wenn dein Datensatz nur \"perfekt zentrierte Kopfschüsse\" hat, du aber mehr Flexibilität in Ausgaben willst. Bei 1.0 lassen, wenn exakte Bildausschnitte wichtig sind.",
                "rotation_range": "Rotationsbereich",
                "rotation_range_help": "Rotiert Bilder zufällig innerhalb des angegebenen Bereichs (in Grad).",
                "width_shift_range": "Breitenverschiebungsbereich",
                "width_shift_range_help": "Verschiebt Bilder zufällig horizontal innerhalb des angegebenen Bereichs (als Bruchteil der Gesamtbreite).",
                "height_shift_range": "Höhenverschiebungsbereich",
                "height_shift_range_help": "Verschiebt Bilder zufällig vertikal innerhalb des angegebenen Bereichs (als Bruchteil der Gesamthöhe).",
                "shear_range": "Scherungsbereich",
                "shear_range_help": "Wendet zufällig Schertransformationen innerhalb des angegebenen Bereichs an (in Grad).",
                "zoom_range": "Zoombereich",
                "zoom_range_help": "Zoomt zufällig in Bilder hinein oder heraus innerhalb des angegebenen Bereichs.",
                "channel_shift_range": "Kanalverschiebungsbereich",
                "channel_shift_range_help": "Verschiebt zufällig die Farbkanäle der Bilder.",
                "brightness_range": "Helligkeitsbereich",
                "brightness_range_help": "Passt zufällig die Helligkeit der Bilder innerhalb des angegebenen Bereichs an.",
                "contrast_range": "Kontrastbereich",
                "contrast_range_help": "Passt zufällig den Kontrast der Bilder innerhalb des angegebenen Bereichs an.",
                "saturation_range": "Sättigungsbereich",
                "saturation_range_help": "Passt zufällig die Sättigung der Bilder innerhalb des angegebenen Bereichs an.",
                "hue_range": "Farbtonbereich",
                "hue_range_help": "Passt zufällig den Farbton der Bilder innerhalb des angegebenen Bereichs an.",
                "cutout_size": "Cutout-Größe",
                "cutout_size_help": "Die Größe der Cutout-Quadrate für CutOut-Augmentierung. Auf 0 setzen zum Deaktivieren.",
                "grid_mask_size": "Grid-Mask-Größe",
                "grid_mask_size_help": "Die Größe der Rasterzellen für Grid-Mask-Augmentierung. Auf 0 setzen zum Deaktivieren.",
                "random_eraser": "Random Eraser",
                "random_eraser_help": "Wendet Random-Erasing-Augmentierung auf die Bilder an.",
                "mixup_alpha": "Mixup Alpha",
                "mixup_alpha_help": "Der Alpha-Parameter für Mixup-Augmentierung. Auf 0 setzen zum Deaktivieren.",
                "cutmix_alpha": "CutMix Alpha",
                "cutmix_alpha_help": "Der Alpha-Parameter für CutMix-Augmentierung. Auf 0 setzen zum Deaktivieren.",
                "label_smoothing": "Label Smoothing",
                "label_smoothing_help": "Wendet Label Smoothing auf die Ziel-Labels während des Trainings an.",
                "random_crop": "Zufälliger Zuschnitt",
                "random_crop_help": "Schneidet Bilder zufällig auf die angegebene Zielgröße zu.",
                "resize": "Größe ändern",
                "resize_help": "Ändert die Größe der Bilder auf die angegebene Zielgröße.",
                "normalize": "Normalisieren",
                "normalize_help": "Normalisiert die Bilder auf Null-Mittelwert und Einheitsvarianz.",
                "denormalize": "Denormalisieren",
                "denormalize_help": "Denormalisiert die Bilder, um die ursprünglichen Pixelwerte wiederherzustellen.",
                "caption_aug_title": "Caption-Augmentierung",
                "shuffle": "Captions mischen",
                "shuffle_help": "Randomisiert die Reihenfolge von Tags/Wörtern in Captions jedes Mal, wenn sie verwendet werden. Hindert das Modell daran, feste Muster wie \"blaue Augen kommen immer vor langen Haaren\" auswendig zu lernen — stattdessen lernt es, dass diese Tags überall erscheinen können. Perfekt für Booru-Stil kommagetrennten Tags, wo die Reihenfolge bedeutungslos ist. Verwende <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a>, wenn du das Triggerwort festhalten musst. <strong>Nicht verwenden</strong> für natürlichsprachliche Captions, wo Wortreihenfolge Bedeutung trägt (\"Frau hält Katze\" ≠ \"Katze hält Frau\").",
                "keep_tokens": "Tokens behalten",
                "keep_tokens_help": "Wie viele Tokens am Anfang jeder Caption \"heilig\" sind und nicht gemischt werden. <strong>0:</strong> Alles ist zum Mischen freigegeben. <strong>1:</strong> Erstes Token (normalerweise dein Triggerwort) bleibt immer zuerst. <strong>2-3:</strong> Schützt \"Trigger, Charaktername\"-Stil-Muster. Wenn deine Captions aussehen wie \"mytrigger, blonde Haare, blaue Augen, ...\", hält das Setzen auf 1 \"mytrigger\" vorne verankert, während der Rest randomisiert wird. Hilft dem Modell, sich auf die Trigger-Konzept-Assoziation zu fixieren, ohne exakte Tag-Reihenfolgen auswendig zu lernen. Relevant nur, wenn <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a> aktiviert ist.",
                "dropout": "Dropout-Rate",
                "dropout_help": "Wahrscheinlichkeit, die Caption eines Bildes vollständig zu löschen (durch nichts ersetzen). Wenn Captions verschwinden, muss das Modell das Bild rein aus Visuellem herausfinden — dies trainiert bedingungslose Generierung. <strong>0.0:</strong> Captions immer vorhanden. <strong>0.05-0.1:</strong> Leichter Dropout, verbessert sanft, was passiert, wenn Benutzer leere oder schwache Prompts geben. <strong>0.15-0.2:</strong> Aggressiver — pusht wirklich die bedingungslose Qualität. Dropout hilft CFG (Classifier-Free Guidance) zur Inferenzzeit besser zu funktionieren, da das Modell tatsächlich weiß, wie \"kein Prompt\" aussieht. Aber für LoRA-Training, wo Prompt-Treue König ist, kann zu viel Dropout nach hinten losgehen. Ein kleines bisschen (0.05) hilft oft; komplett überspringen für sehr kurze Trainingsläufe.",
                "dropout_epochs": "Dropout alle N Epochen",
                "dropout_epochs_help": "Statt zufälligen Dropouts pro Schritt, löscht dies Captions bei spezifischen Epochen. <strong>0:</strong> Fällt zurück auf das normale zufällige <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a>-Verhalten. <strong>1:</strong> Jede einzelne Epoche ist ohne Captions (ziemlich extrem). <strong>5:</strong> Epochen 5, 10, 15... laufen ohne Captions. Das gibt dir ein strukturierteres Muster — normales Training die meiste Zeit, mit periodischen \"blinden\" Epochen. Nützlich für spezifische Curricula, aber zufälliger Dropout pro Schritt ist in der Praxis häufiger.",
                "noise_title": "Rausch-Einstellungen",
                "noise_type": "Noise-Offset-Typ",
                "noise_type_help": "Die mathematische Methode zur Berechnung des Noise-Offset. \"Original\" ist die Standardimplementierung.",
                "noise_offset": "Noise-Offset-Stärke",
                "noise_offset_help": "Fügt einen Offset zum Rauschen während des Trainings hinzu, was dem Modell hilft, Bilder mit besserem dynamischen Bereich (tiefere Schwarztöne und hellere Weißtöne) zu generieren. Ein Wert von 0.05-0.1 wird generell empfohlen.",
                "adaptive_noise": "Adaptive Noise-Skala",
                "adaptive_noise_help": "Skaliert das Rauschen basierend auf dem Vorhersagefehler des Modells während des Trainings.",
                "noise_random": "Noise-Offset zufällige Stärke",
                "noise_random_help": "Randomisiert die Stärke des Noise-Offset für jeden Trainingsschritt.",
                "multires_iter": "Multires-Noise-Iterationen",
                "multires_iter_help": "Wendet Multi-Resolution-Rauschen an, um das Lernen feiner Texturen und Details zu verbessern.",
                "multires_discount": "Multires-Noise-Discount",
                "multires_discount_help": "Der Diskontfaktor, der auf Multi-Resolution-Noise-Iterationen angewendet wird.",
                "edm": "EDM-Stil-Training (SDXL)",
                "edm_help": "Verwendet die EDM-Formulierung (Elucidating the Design Space of Diffusion-Based Generative Models), was zu höherer Qualität für SDXL-Modelle führen kann.",
                "caching_title": "Caching",
                "cache_latents": "Latents in RAM cachen",
                "cache_latents_help": "Vorberechnet VAE-Latents, um die Trainingsgeschwindigkeit signifikant zu erhöhen (oft 2x-3x). Wenn aktiviert, werden alle Latents gecacht und bleiben während des gesamten Trainings im RAM. Dies deaktiviert bestimmte Echtzeit-Augmentierungen.",
                "cache_te": "Text-Embeddings cachen",
                "cache_te_help": "Vorberechnet Text-Embeddings zur Verbesserung der Trainingsgeschwindigkeit. Diese Einstellung ist inkompatibel mit aktivem Text-Encoder-Training.",
                "cache_disk": "Latents auf Disk cachen",
                "cache_disk_help": "Speichert vorberechnete Latents auf der Festplatte, um RAM zu sparen. Beachte, dass langsame Disk-I/O die Trainingsleistung negativ beeinflussen kann. Diese Latents werden in Trainingssessions wiederverwendet. Wenn sich der Datensatz ändert, musst du die alten Latents manuell aus dem Cache löschen.",
                "vae_batch": "VAE-Batch-Größe",
                "vae_batch_help": "Die Batch-Größe für VAE-Encoding während des Caching-Prozesses. Höhere Werte erhöhen die Geschwindigkeit, benötigen aber mehr VRAM.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Optimizer",
                "optimizer_help": "The algorithm that actually updates your weights based on gradients. Different optimizers have different strengths. <strong>AdamW8bit:</strong> Memory-efficient 8-bit version of the classic. Uses ~50% less optimizer state memory. Go-to choice for most LoRA training when VRAM is tight. Needs the bitsandbytes library. <strong>AdamW:</strong> The original, full precision. Most stable and battle-tested. Pick this if you've got VRAM to spare or hit issues with the 8-bit version. <strong>Lion / Lion8bit:</strong> Newer hotness — often needs much lower LR (3-10× lower than AdamW). Can give good results with less memory, but still somewhat experimental. <strong>DAdaptAdam:</strong> Self-adjusting optimizer. Set your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> to 1.0 and let it find the sweet spot. <strong>Prodigy:</strong> Another smart adaptive optimizer. Also use LR=1.0. Often produces excellent results hands-free. <strong>CAME:</strong> Experimental, decent memory efficiency. <strong>Adafactor:</strong> Super memory-friendly, originally built for huge language models. Last resort for extreme VRAM constraints. <strong>SGD:</strong> Basic gradient descent. Rarely used for diffusion stuff but it's here if you want to experiment. Fine-tune behavior with <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer_args\" data-xref-label=\"Optimizer Args\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer Args</a>.",
                "optimizer_adamw": "AdamW (Stable)",
                "optimizer_adamw8bit": "AdamW8bit (Memory-efficient)",
                "optimizer_lion": "Lion (Experimental)",
                "optimizer_prodigy": "Prodigy (Advanced)",
                "optimizer_dadaptation": "DAdaptAdam (Adaptive LR)",
                "optimizer_args": "Optimizer Args",
                "optimizer_args_help": "Extra tweaks passed straight to your <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>. Format: <code>key=value</code>, comma-separated. Some examples: <strong>weight_decay=0.01</strong> — L2 regularization to fight overfitting (common with AdamW). <strong>betas=(0.9,0.999)</strong> — Adam momentum coefficients. <strong>d_coef=1.0</strong> — D-coefficient for Prodigy/DAdaptAdam. <strong>eps=1e-8</strong> — Tiny number for numerical stability. Defaults are solid for most training. Only mess with these if you're following a specific guide or you really know what you're doing.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning Rate",
                "lr_help": "The base learning rate for training.",
                "learning_rate": "Learning Rate",
                "learning_rate_help": "The single most impactful setting in your entire training run. This determines how aggressively the model adjusts its weights each step. <strong>Too high</strong> (like 1e-3 or more) and you'll fry your model — expect artifacts, noise, or complete collapse. <strong>Too low</strong> (like 1e-6 or less) and training will crawl or go nowhere at all. <strong>Where to start:</strong> - <strong>LoRA/LyCORIS:</strong> 1e-4 to 5e-4 is the sweet spot - <strong>Full finetune:</strong> go much lower, around 1e-6 to 1e-5 - <strong>Prodigy/DAdaptAdam:</strong> just use 1.0 and let the optimizer figure it out The 'perfect' value depends on batch size, dataset, and your setup. Bigger effective batches (via <a href=\"#\" class=\"xref-link\" data-xref=\"grad_acc\" data-xref-label=\"Gradient Accumulation\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Gradient Accumulation</a>) usually handle higher learning rates gracefully.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Override the learning rate just for the UNet — that's the core image generation engine. Leave blank to use the global <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a> setting. Splitting rates between UNet and Text Encoder gives you precise control over what learns faster. The UNet handles all your visuals (style, composition, structure), so this directly affects how quickly the \"look\" of your training subject gets baked in. Honestly, same rate for both usually works fine — only split if you know what you're doing.",
                "unet_lr_placeholder": "Optional",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Learning rate specifically for the Text Encoder (CLIP or T5, depending on your base model). This component translates your text prompts into something the image generator understands. Common wisdom: use about <strong>1/10th to 1/2</strong> of the UNet rate, or just turn it off entirely (set to 0). <strong>Training it:</strong> Helps the model recognize new concepts, names, and respond better to your style-specific prompts. <strong>Skipping it (0):</strong> Keeps the base model's language understanding intact — good when you only care about visual style. For LoRA work, half of <a href=\"#\" class=\"xref-link\" data-xref=\"unet_lr\" data-xref-label=\"UNet LR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet LR</a> is a solid starting point.",
                "te_lr_placeholder": "Optional",
                "lr_scheduler": "LR Scheduler",
                "lr_scheduler_help": "Dictates how your learning rate evolves over the course of training. This can make or break your results. <strong>constant:</strong> Stays flat the whole way. Simple, predictable — works great for short runs. <strong>cosine:</strong> Starts strong, then smoothly tapers off following a cosine curve. Popular choice because it pushes learning early and refines gently at the end. <strong>cosine_with_restarts:</strong> Same cosine idea, but periodically kicks the LR back up. Can help escape local minima. Set restart count with <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_num_cycles\" data-xref-label=\"LR Scheduler Cycles\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">LR Scheduler Cycles</a>. <strong>linear:</strong> Straight line from starting LR down to zero. Simple and effective. <strong>polynomial:</strong> Decays based on a power function. Tweak behavior with <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_power\" data-xref-label=\"LR Scheduler Power\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">LR Scheduler Power</a>. <strong>constant_with_warmup:</strong> Ramps up from zero, then holds steady. Pair with <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Steps</a> for smooth starts. For most LoRA jobs, <strong>constant</strong> or <strong>cosine</strong> will serve you well.",
                "lr_scheduler_constant": "constant",
                "lr_scheduler_linear": "linear",
                "lr_scheduler_cosine": "cosine",
                "lr_scheduler_cosine_with_restarts": "cosine_with_restarts",
                "lr_scheduler_polynomial": "polynomial",
                "lr_scheduler_constant_with_warmup": "constant_with_warmup",
                "scheduler": "LR Scheduler",
                "scheduler_help": "The strategy used to adjust the learning rate during training.",
                "warmup": "Warmup Steps",
                "warmup_help": "How many steps to spend gently ramping the learning rate from zero up to its target value. Think of it like warming up before exercise — prevents the model from getting shocked by sudden large gradients when weights are still way off from optimal. <strong>0</strong> = no warmup, jump straight to full LR. <strong>10-100 steps</strong> is usually plenty for LoRA training. <strong>100-500 steps</strong> might help with full finetuning or when using big batches. You can also use <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> to specify this as a percentage of total steps instead. Especially useful with high learning rates or adaptive optimizers like <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy or DAdaptAdam</a>.",
                "warmup_steps": "Warmup Steps",
                "warmup_steps_help": "How many steps to spend gently ramping the learning rate from zero up to its target value. Think of it like warming up before exercise — prevents the model from getting shocked by sudden large gradients when weights are still way off from optimal. <strong>0</strong> = no warmup, jump straight to full LR. <strong>10-100 steps</strong> is usually plenty for LoRA training. <strong>100-500 steps</strong> might help with full finetuning or when using big batches. You can also use <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> to specify this as a percentage of total steps instead. Especially useful with high learning rates or adaptive optimizers like <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy or DAdaptAdam</a>.",
                "warmup_ratio": "Warmup Ratio",
                "warmup_ratio_help": "Alternative to <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Steps</a> — specify warmup as a fraction of total training instead of an exact step count. <strong>0.0</strong> = no warmup. <strong>0.05</strong> = warmup over the first 5% of training. <strong>0.1</strong> = warmup over the first 10% of training. Way more convenient than counting steps manually since it scales automatically with your training length. If you set both this and warmup_steps, this one wins.",
                "cycles": "LR Cycles",
                "cycles_help": "How many complete up-and-down cycles to run when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a>. Each cycle drops the LR to minimum then kicks it back up (usually to a lower peak than before). <strong>1</strong> = one cycle across all training, basically the same as regular cosine. <strong>2-4</strong> cycles can shake the model out of local minima and explore the loss landscape better. More cycles = more 'restarts' = better exploration but potentially less stable near the end. Only matters if you're using cosine_with_restarts.",
                "scheduler_power": "Scheduler Power",
                "scheduler_power_help": "The exponent when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial scheduler</a>. Shapes how aggressively the LR drops off. <strong>1.0</strong> = linear decay, nothing fancy. <strong>> 1.0</strong> (like 2.0) = fast drop early, slows down later. <strong>< 1.0</strong> (like 0.5) = slow drop early, faster toward the end. Only does anything with polynomial scheduler selected. Default 1.0 is fine for most cases.",
                "total_steps": "Total Steps",
                "total_steps_help": "The total number of training steps.",
                "gradient_accumulation_steps": "Gradient Accumulation Steps",
                "gradient_accumulation_steps_help": "The number of steps to accumulate gradients over.",
                "ema_unet": "EMA for UNet",
                "ema_unet_help": "Keeps a running average of UNet weights during training. Instead of using the raw weights from the last step (which can be noisy), EMA gives you a \"smoothed\" version that typically looks better and generalizes more reliably. Think of it as noise reduction for your model weights — the EMA checkpoint often outperforms the raw one. The catch: it doubles VRAM usage for UNet weights (you're storing both sets). Smoothing rate controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a>. Strongly recommended for longer training runs if your VRAM can handle it.",
                "ema_te": "EMA for Text Encoder",
                "ema_te_help": "Same idea as <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, but for the text encoder instead. Less critical than UNet EMA since text encoder weights usually shift more gently during LoRA training. Still adds VRAM overhead for storing the extra weights. Turn this on if you're training the text encoder and want the absolute best quality.",
                "ema_decay": "EMA Decay",
                "ema_decay_help": "How much the EMA \"remembers\" old weights vs. new: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Higher (0.999-0.9999):</strong> More smoothing, slower to pick up changes. Good for long training runs. <strong>Lower (0.99-0.995):</strong> Less smoothing, reacts faster to new weights. Better for short runs. <strong>0.995</strong> works well for typical LoRA training (hundreds to a few thousand steps). Doing tens of thousands of steps? Try <strong>0.9999</strong>. Only matters if <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for UNet</a> or <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for Text Encoder</a> is on.",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Whether to use mixed precision training (FP16).",
                "mixed_precision_no": "no",
                "mixed_precision_fp16": "fp16",
                "mixed_precision_bf16": "bf16",
                "fp16_opt_level": "FP16 Opt Level",
                "fp16_opt_level_help": "The optimization level for FP16 training.",
                "loss_scale": "Loss Scale",
                "loss_scale_help": "The loss scaling factor for FP16 training.",
                "clip_grad": "Clip Grad",
                "clip_grad_help": "Whether to clip gradients during training.",
                "log_interval": "Log Interval",
                "log_interval_help": "The interval (in steps) at which to log training progress.",
                "save_interval": "Save Interval",
                "save_interval_help": "The interval (in steps) at which to save checkpoints.",
                "resume_from_checkpoint": "Resume from Checkpoint",
                "resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint.",
                "checkpoint_path": "Checkpoint Path",
                "checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "checkpoint_path_help": "The path to the checkpoint file to resume training from.",
                "use_8bit_adam": "Use 8-bit Adam",
                "use_8bit_adam_help": "Whether to use the 8-bit Adam optimizer.",
                "beta1": "Beta 1",
                "beta1_help": "The beta1 parameter for the Adam optimizer.",
                "beta2": "Beta 2",
                "beta2_help": "The beta2 parameter for the Adam optimizer.",
                "epsilon": "Epsilon",
                "epsilon_help": "The epsilon parameter for the Adam optimizer.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer.",
                "weight_decay": "Weight Decay",
                "weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer.",
                "max_grad_norm": "Max Grad Norm",
                "max_grad_norm_help": "The maximum norm for gradient clipping.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "The beta1 parameter for the Adam optimizer.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "The beta2 parameter for the Adam optimizer.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "The epsilon parameter for the Adam optimizer."
            },
            "advanced": {
                "attention": "Attention Backend",
                "attention_help": "Which implementation handles the attention mechanism — the most memory-hungry part of transformers. <strong>SDPA:</strong> PyTorch's built-in optimized attention (requires PyTorch 2.0+). Good balance of speed and memory, no extra installs. Recommended default. <strong>xFormers:</strong> Meta's attention library, often the fastest option on NVIDIA cards. Can slash VRAM by 20-40% compared to vanilla attention. Needs separate installation. <strong>None:</strong> Plain PyTorch attention — eats more memory but works everywhere. Only fall back to this if SDPA or xFormers are giving you trouble. Both SDPA and xFormers play nicely with <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Aggressive Memory Saving</a>.",
                "use_xformers": "Use Xformers",
                "use_xformers_help": "Whether to use Xformers for efficient attention computation.",
                "xformers_memory_efficient": "Memory Efficient Attention",
                "xformers_memory_efficient_help": "Whether to use memory-efficient attention with Xformers.",
                "gradient_checkpointing": "Gradient Checkpointing",
                "gradient_checkpointing_help": "Whether to use gradient checkpointing to save memory.",
                "grad_acc": "Gradient Accumulation",
                "grad_acc_help": "Stacks up gradients over multiple passes before updating weights — essentially fakes a bigger batch size. Your <strong>effective batch = batch_size × gradient_accumulation</strong>. So batch_size=2 with accumulation=4 acts like batch_size=8. This is a lifesaver for VRAM-starved systems: can't fit batch=8? Do batch=2 with accumulation=4 instead. Bigger effective batches mean more stable training, but you might need to tweak your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">learning rate</a>. Downside: slows things down proportionally (4× accumulation = 4× slower per effective step).",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Numerical precision for calculations — directly impacts VRAM and speed. <strong>fp16:</strong> Half-precision floats, cuts memory in half compared to full precision. Solid choice for GTX 10-series and RTX 20-series cards, though SDXL can sometimes throw NaN errors due to fp16's limited range. <strong>bf16:</strong> Also 16 bits but with better range for exponents — much more stable. Definitely use this on RTX 30-series and newer. Won't work on older hardware. <strong>fp8:</strong> 8-bit precision for extreme memory savings. Experimental and may hurt quality. Needs specific hardware. <strong>no:</strong> Full fp32 precision. Eats 2× the VRAM but rock-solid numerically. Only use for debugging NaN issues or if you've got VRAM to burn.",
                "mixed_precision_training": "Mixed Precision Training",
                "mixed_precision_training_help": "Whether to use mixed precision training (FP16).",
                "tf32": "Allow TF32",
                "tf32_help": "TensorFloat-32 — a compute mode exclusive to RTX 30-series and newer. Uses fp32 range but with less mantissa precision (10 bits instead of 23), giving you up to 8× speedup on some operations with barely any quality difference for ML workloads. <strong>Always leave this on for RTX 30-series and 40-series cards.</strong> Does nothing on older GPUs (GTX 10-series, RTX 20-series) since they don't have the hardware for it. Only turn off if you suspect TF32 is somehow tanking quality (super rare).",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "Separate batch size just for VAE encoding — when images get converted to latent space before the UNet sees them. Setting this lower than your main batch size can tame VRAM spikes during encoding. Leave blank to match training batch size. Set to <strong>1</strong> for minimum VRAM at the cost of slower encoding. Handy if you're OOMing specifically during the latent encoding phase.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max Token Length",
                "max_token_length_help": "Maximum tokens (roughly words/subwords) allowed in captions. CLIP's native context is <strong>77 tokens</strong> (75 usable + start/end markers). <strong>75:</strong> Standard, handles most captions without issue. <strong>150:</strong> Room for more detailed descriptions — good for complex scenes or thorough character breakdowns. <strong>225:</strong> Very long, super-detailed captions. Eats significantly more VRAM and slows down processing. Higher limits work through caption chunking techniques. Bump up to <strong>150</strong> or <strong>225</strong> if your captions regularly exceed ~60-70 words and truncation is cutting off important info.",
                "memory_saving": "Aggressive Memory Saving",
                "memory_saving_help": "Throws everything at VRAM savings: gradient checkpointing, CPU offloading, aggressive garbage collection. <strong>A must-have for 4GB-8GB cards</strong> — lets you train SDXL on hardware that would otherwise choke. Can cut VRAM usage by 30-50% depending on the model. The price? Training becomes 2-4× slower due to recomputation and shuffling data between CPU and GPU. Pairs well with a good <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention Backend</a> (SDPA or xFormers) for maximum savings. Got 16GB+ VRAM? Leave this off and enjoy faster training.",
                "seed": "Training Seed",
                "seed_help": "The random seed for all RNGs in training. Pick a specific seed and you get <strong>reproducibility</strong>: same config + same seed = identical results (assuming same hardware and software). Leave blank for random initialization — totally fine for everyday training. Useful when A/B testing settings, hunting bugs, or sharing reproducible configs with others. People often pick memorable numbers like <strong>42</strong>, <strong>1234</strong>, or <strong>0</strong>.",
                "seed_placeholder": "Random",
                "fp16_opt_level": "FP16 Opt Level",
                "fp16_opt_level_help": "The optimization level for FP16 training.",
                "loss_scale": "Loss Scale",
                "loss_scale_help": "The loss scaling factor for FP16 training.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "The beta1 parameter for the Adam optimizer.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "The beta2 parameter for the Adam optimizer.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "The epsilon parameter for the Adam optimizer.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer.",
                "weight_decay": "Weight Decay",
                "weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer.",
                "max_grad_norm": "Max Grad Norm",
                "max_grad_norm_help": "The maximum norm for gradient clipping.",
                "lr_scheduler": "LR Scheduler",
                "lr_scheduler_help": "The learning rate scheduler to use.",
                "warmup_steps": "Warmup Steps",
                "warmup_steps_help": "The number of steps to perform learning rate warmup.",
                "total_steps": "Total Steps",
                "total_steps_help": "The total number of training steps.",
                "gradient_accumulation_steps": "Gradient Accumulation Steps",
                "gradient_accumulation_steps_help": "The number of steps to accumulate gradients over.",
                "clip_grad": "Clip Grad",
                "clip_grad_help": "Whether to clip gradients during training.",
                "log_interval": "Log Interval",
                "log_interval_help": "The interval (in steps) at which to log training progress.",
                "save_interval": "Save Interval",
                "save_interval_help": "The interval (in steps) at which to save checkpoints.",
                "resume_from_checkpoint": "Resume from Checkpoint",
                "resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint.",
                "checkpoint_path": "Checkpoint Path",
                "checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "checkpoint_path_help": "The path to the checkpoint file to resume training from."
            },
            "flux": {
                "title": "Flux Specific Settings",
                "t5xxl": "T5XXL Model",
                "t5xxl_help": "Path to the T5XXL text encoder. Required for Flux training to process complex textual descriptions.",
                "ae": "Autoencoder (VAE)",
                "ae_help": "Path to the Flux Autoencoder. Handles the conversion between pixel space and latent space.",
                "clip_l": "CLIP-L Model",
                "clip_l_help": "Path to the CLIP-L text encoder, used alongside T5XXL for text processing in Flux.",
                "max_seq_len": "Max Sequence Length",
                "max_seq_len_help": "Max token length for T5 text encoder (Flux / SD3). Higher values allow longer prompts/captions, but increase VRAM use and slow down training. Common values: <strong>256</strong> (default) or <strong>512</strong>.",
                "guidance_scale": "Guidance Scale",
                "guidance_scale_help": "Classifier-free guidance for training Flux and SD3 models. Tells the model how hard to stick to the text prompt during training. <strong>3.5:</strong> Stability AI's recommended default for SD3, works great for Flux too. <strong>Lower (1-2):</strong> More creative/varied outputs but might wander from the prompt. <strong>Higher (5-7):</strong> Tighter prompt adherence but can look a bit stiff. For SD1.5/SDXL, this typically isn't used — guidance only kicks in at inference time for those.",
                "discrete_flow_shift": "Discrete Flow Shift",
                "discrete_flow_shift_help": "Adjusts the flow matching shift parameter. Higher values can improve detail but may require more sampling steps.",
                "model_prediction_type": "Prediction Type",
                "model_prediction_type_help": "Specifies what the model is predicting (e.g., raw noise or velocity). Usually set to 'raw' for Flux.",
                "split_mode": "Split Mode",
                "split_mode_help": "Advanced memory management for Flux. 'Symmetry' is standard; 'Asymmetry' can save VRAM at a slight performance cost.",
                "train_blocks": "Train Blocks",
                "train_blocks_help": "Specifies which blocks of the Flux model to train. 'all' is standard for full fine-tuning.",
                "weighting_scheme": "Weighting Scheme",
                "weighting_scheme_help": "How timesteps are sampled during training for flow-matching models (Flux, SD3). This decides which noise levels get more attention during training. <strong>None (Uniform):</strong> Equal love for all noise levels. Safe default when you don't have specific needs. <strong>Logit Normal:</strong> Bell-curve distribution centered on <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> with width set by <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Lets you dial in exactly which timesteps matter most. <strong>Mode:</strong> Focuses hard on a specific timestep, spread controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a>. These only affect Flux and SD3 — does nothing for SD1.5 or SDXL (epsilon-prediction models).",
                "logit_mean": "Logit Mean",
                "logit_mean_help": "Where the logit-normal distribution peaks when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>0.0:</strong> Centers on the middle timestep (t=0.5). <strong>Negative:</strong> Push focus toward noisier timesteps (coarse structure). <strong>Positive:</strong> Push focus toward cleaner timesteps (fine details). Tune this based on what part of the generation process matters most for your use case. Only kicks in when Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit Std",
                "logit_std_help": "How spread out the logit-normal distribution is when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>Lower (0.5-0.8):</strong> Tight focus around <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> — really zeros in on specific timesteps. <strong>Higher (1.5-2.0):</strong> Spreads out more, approaching uniform sampling. <strong>1.0:</strong> Balanced default. Only matters when Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode Scale",
                "mode_scale_help": "How tightly the mode distribution clusters when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode weighting</a>. <strong>Lower:</strong> Samples spread across more timesteps. <strong>Higher:</strong> Samples bunch up tighter around the mode. <strong>1.29:</strong> The value from the original SD3 paper. Stick with it unless experimenting. Only relevant when Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep Sampling",
                "type": "Sampling Type",
                "type_help": "The method used to sample timesteps during training. 'sigma_distribution' is recommended for Flux.",
                "min": "Min Timestep",
                "min_help": "Minimum timestep for sampling.",
                "max": "Max Timestep",
                "max_help": "Maximum timestep for sampling.",
                "sampling_type": "Sampling Type",
                "sampling_type_help": "The method used to distribute noise across timesteps. 'sigma' is modern and generally preferred for Flux/SD3.",
                "sampling_type_sigma": "sigma",
                "sampling_type_timestep": "timestep",
                "timestep_spacing": "Timestep Spacing",
                "timestep_spacing_help": "How timesteps are spaced during the diffusion process. 'linspace' is standard.",
                "timestep_spacing_linspace": "linspace",
                "timestep_spacing_leading": "leading",
                "timestep_spacing_trailing": "trailing",
                "min_timestep": "Min Timestep",
                "min_timestep_help": "Floor for timestep sampling — keeps training in higher-noise territory. Timesteps go from 0 (clean image) to ~1000 (pure noise). Setting something like <strong>100</strong> stops the model from training on near-clean images. Good for: - Focusing on rough structure when fine details don't matter - Testing specific noise regimes - Weird specialized training schedules Leave blank for normal full-range training. Pairs with <a href=\"#\" class=\"xref-link\" data-xref=\"max_timestep\" data-xref-label=\"Max Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Max Timestep</a> to carve out a custom range.",
                "max_timestep": "Max Timestep",
                "max_timestep_help": "Ceiling for timestep sampling — keeps training away from the noisiest levels. Setting something like <strong>800</strong> skips training on maximum-noise timesteps. Useful for: - Polishing fine details when coarse structure is already solid - Finetuning while preserving the base model's high-noise behavior - Experimental training approaches Leave blank for normal full-range training. Combined with <a href=\"#\" class=\"xref-link\" data-xref=\"min_timestep\" data-xref-label=\"Min Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min Timestep</a>, you can train on any subset of the noise schedule."
            },
            "caption": {
                "title": "Caption Settings",
                "extension": "Caption Extension",
                "extension_help": "File extension for your caption files. Default is <strong>.txt</strong> — so \"image001.png\" expects its caption in \"image001.txt\". Supports whatever extension you want (.caption, .captions, .tags, etc.). Caption file needs to live in the same folder as its image. Change this if your captioning tool outputs something different.",
                "prefix": "Caption Prefix",
                "prefix_help": "Text to prepend to every caption during training.",
                "suffix": "Caption Suffix",
                "suffix_help": "Text to append to every caption during training.",
                "dropout": "Caption Dropout",
                "dropout_help": "Probability of dropping the entire caption during training. Helps with classifier-free guidance.",
                "token_warmup": "Token Warmup",
                "token_warmup_help": "Gradually introduce tokens from the caption during the initial steps of training.",
                "weighted": "Weighted Captions",
                "weighted_help": "Parses A1111-style weights in captions like \"(important thing:1.3)\" or \"[less important:0.7]\". Lets you emphasize or de-emphasize specific words during training. <strong>Weight > 1.0:</strong> Model pays more attention to those tokens. <strong>Weight < 1.0:</strong> Model cares less about those tokens. Great for fine-tuning what aspects of your captions matter most. Turn this off if your captions use literal parentheses that aren't meant as weights.",
                "caption_dropout_rate": "Caption Dropout Rate",
                "caption_dropout_rate_help": "The probability of dropping a caption during training. This can help the model learn to generate images without relying solely on captions.",
                "caption_dropout_every_n_epochs": "Caption Dropout Every N Epochs",
                "caption_dropout_every_n_epochs_help": "The interval (in epochs) at which to apply caption dropout.",
                "caption_tag_dropout_rate": "Caption Tag Dropout Rate",
                "caption_tag_dropout_rate_help": "The probability of dropping individual tags within a caption during training."
            },
            "loss": {
                "title": "Loss & Optimization",
                "optimization_title": "Loss Optimization",
                "loss_type": "Loss Type",
                "loss_type_help": "The mathematical function used to calculate the error between predicted and actual noise. 'l2' is standard; 'huber' is more robust to outliers.",
                "type": "Loss Type",
                "type_help": "The mathematical function used to calculate the difference between predicted and target noise.",
                "loss_type_l2": "l2",
                "loss_type_l1": "l1",
                "loss_type_huber": "huber",
                "huber_delta": "Huber Delta",
                "huber_delta_help": "The threshold at which the Huber loss switches from quadratic to linear.",
                "huber_c": "Huber C",
                "huber_c_help": "The threshold parameter for Huber loss. Lower values make it more like L1.",
                "huber_schedule": "Huber Schedule",
                "huber_schedule_help": "How the Huber delta changes over time during training.",
                "huber_schedule_constant": "constant",
                "huber_schedule_exponential": "exponential",
                "huber_schedule_snr": "snr",
                "min_snr_gamma": "Min-SNR Gamma",
                "min_snr_gamma_help": "From the \"Efficient Diffusion Training via Min-SNR Weighting Strategy\" paper — fixes the problem where models overfit some timesteps while ignoring others. Without this, quality tends to be inconsistent across different denoising stages. The gamma value (<strong>5.0</strong> typically) sets the SNR clipping threshold. <strong>Lower (1-3):</strong> Emphasizes high-noise timesteps (early denoising, coarse structure). <strong>Higher (8-20):</strong> Emphasizes low-noise timesteps (final details, polish). <strong>0:</strong> Turns Min-SNR off entirely. Generally should be enabled — big quality boost with almost no performance cost.",
                "min_snr": "Min SNR Gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. Helps stabilize training by weighting loss based on noise levels.",
                "ip_noise_gamma": "IP Noise Gamma",
                "ip_noise_gamma_help": "Input Perturbation Noise adds additional random noise to the input during training as a regularization trick — kind of like dropout but for images. Helps prevent the model from memorizing specific details and improves generalization, especially on smaller datasets. <strong>0.05:</strong> Light regularization, won't mess with your training. <strong>Above 0.1:</strong> Might hurt quality by adding too much noise. Keep at <strong>0</strong> for normal training, or bump up slightly if your model is memorizing individual images instead of learning concepts. Independent from <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — you can stack them.",
                "ip_noise": "IP Noise Gamma",
                "ip_noise_help": "Strength of Input Perturbation noise. Helps with over-smoothing.",
                "ip_noise_gamma_random_range": "IP Noise Random Range",
                "ip_noise_gamma_random_range_help": "The range of random noise added when using IP Noise Gamma.",
                "noise_offset": "Noise Offset",
                "noise_offset_help": "Adds a tiny constant offset to noise during training so the model can actually generate true blacks and true whites instead of washed-out grays. Standard diffusion struggles with extreme dark/bright areas because the noise schedule doesn't cover them well. <strong>0.035-0.05:</strong> Safe defaults, improve dark/light handling without side effects. <strong>0.05-0.1:</strong> More aggressive push toward better extremes. <strong>Above 0.1:</strong> Might destabilize training or cause color shifts. Works best when paired with <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a> — they complement each other. Essential if your dataset has dark scenes, night shots, or high-contrast images.",
                "v_pred": "V-Pred Like Loss",
                "v_pred_help": "Blends velocity-prediction behavior into standard noise prediction training. Originally from v-prediction models (SD 2.x depth/inpainting) — can smooth out training and reduce artifacts. <strong>0:</strong> Pure noise prediction, the standard approach. <strong>1:</strong> Full velocity prediction mode. <strong>0.1-0.2:</strong> Subtle v-pred influence while staying compatible with noise-pred base models. Can help with smoother gradients and fewer artifacts in some cases. Leave at <strong>0</strong> if things are already working well or you're not sure.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Advanced Loss Options",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> Tweaks the noise scheduler so the final timestep is pure noise (SNR = 0), which unlocks true black generation. Essential companion to <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — auto-enabled when using the Auto toggle. Without it, noise offset alone won't fully fix dark image issues. From the paper \"Common Diffusion Noise Schedules and Sample Steps are Flawed\".<br><strong>Debiased Estimation:</strong> Fixes the bias in timestep sampling where some get oversampled. Reweights the loss so all timesteps train evenly, improving overall quality. Works alongside <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> but tackles a different problem.<br><strong>Scale V-Pred Loss:</strong> Normalizes v-prediction loss to match noise-prediction magnitude. Mainly useful for v-pred base models (SD 2.x variants) or when using <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a>. Doesn't do much for standard SD 1.5 or SDXL (epsilon-prediction).<br><strong>Masked Loss (Alpha):</strong> Uses PNG alpha channels as training masks — model only learns from non-transparent pixels. Perfect for character/object training where you want to ignore backgrounds. <strong>Needs PNGs with proper alpha channels!</strong>",
                "presets": "Loss Presets",
                "presets_help": "Quickly apply recommended loss settings for different training goals.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light"
            },
            "samples": {
                "title": "Sample Generation",
                "prompts_title": "Sample Prompts",
                "inference_title": "Inference Settings",
                "schedule_title": "Generation Schedule",
                "generated_title": "Generated Samples",
                "gallery_placeholder": "Generated samples will appear here during training...",
                "prompts": "Sample Prompts",
                "prompts_help": "Enter prompts to generate sample images during training. One prompt per line.",
                "neg_prompt": "Negative Prompt",
                "neg_prompt_help": "Negative prompt to use for sample generation.",
                "every_n_epochs": "Every N Epochs",
                "every_n_epochs_help": "Generate samples every N epochs.",
                "every_n_steps": "Every N Steps",
                "every_n_steps_help": "Generate samples every N steps.",
                "sampler": "Sampler",
                "sampler_help": "The sampler to use for generating sample images.",
                "steps": "Sample Steps",
                "steps_help": "Number of sampling steps for each image.",
                "inference_steps": "Inference Steps",
                "inference_steps_help": "Number of denoising steps for sample generation.",
                "cfg_scale": "CFG Scale",
                "cfg_scale_help": "Classifier Free Guidance scale for sample generation.",
                "width": "Width",
                "width_help": "Width of the sample image.",
                "height": "Height",
                "height_help": "Height of the sample image.",
                "num_images": "Images per Prompt",
                "num_images_help": "Number of images to generate for each prompt.",
                "seed": "Seed",
                "seed_help": "Seed for reproducible sample generation. -1 for random.",
                "format": "Sample Format",
                "format_help": "File format for saved samples.",
                "sample_every_n_steps": "Sample Every N Steps",
                "sample_every_n_steps_help": "Generates validation samples every N steps to provide visual feedback on training progress. Note that frequent sample generation can slightly increase total training time.",
                "sample_prompt": "Sample Prompt",
                "sample_prompt_help": "The prompts used for validation images to evaluate how well the model has learned the target concepts. Enter one prompt per line — each prompt will be rendered separately during sampling.",
                "sample_negative_prompt": "Sample Negative Prompt",
                "sample_negative_prompt_help": "The negative prompt used during sample generation to suppress unwanted artifacts or styles.",
                "sample_num_images": "Sample Num Images",
                "sample_num_images_help": "The number of sample images to generate at each sampling interval.",
                "sample_seed": "Sample Seed",
                "sample_seed_help": "The seed value for random number generation during sample image creation.",
                "sample_width": "Sample Width",
                "sample_width_help": "The width of the generated sample images.",
                "sample_height": "Sample Height",
                "sample_height_help": "The height of the generated sample images.",
                "sample_guidance_scale": "Sample Guidance Scale",
                "sample_guidance_scale_help": "The guidance scale to use for controlling the strength of the prompt during image generation.",
                "sample_steps": "Sample Steps",
                "sample_steps_help": "Number of denoising steps to use when generating validation samples. 20–30 steps are usually sufficient for evaluation; higher values can improve quality but increase generation time.",
                "sample_eta": "Sample Eta",
                "sample_eta_help": "The eta parameter for controlling the randomness of the diffusion process during image generation.",
                "sample_scheduler": "Sample Scheduler",
                "sample_scheduler_help": "The sampler used for validation image generation. `Euler a` is generally recommended for speed and visual quality, but choose whichever sampler fits your workflow.",
                "sample_lora": "Sample LoRA",
                "sample_lora_help": "The LoRA model to use for generating sample images.",
                "sample_lycoris": "Sample LyCORIS",
                "sample_lycoris_help": "The LyCORIS model to use for generating sample images.",
                "sample_oft": "Sample OFT",
                "sample_oft_help": "The OFT model to use for generating sample images.",
                "sample_use_8bit_adam": "Sample Use 8-bit Adam",
                "sample_use_8bit_adam_help": "Whether to use the 8-bit Adam optimizer for generating sample images.",
                "sample_fp16_opt_level": "Sample FP16 Opt Level",
                "sample_fp16_opt_level_help": "The optimization level for FP16 training when generating sample images.",
                "sample_loss_scale": "Sample Loss Scale",
                "sample_loss_scale_help": "The loss scaling factor for FP16 training when generating sample images.",
                "sample_adam_beta1": "Sample Adam Beta1",
                "sample_adam_beta1_help": "The beta1 parameter for the Adam optimizer when generating sample images.",
                "sample_adam_beta2": "Sample Adam Beta2",
                "sample_adam_beta2_help": "The beta2 parameter for the Adam optimizer when generating sample images.",
                "sample_adam_epsilon": "Sample Adam Epsilon",
                "sample_adam_epsilon_help": "The epsilon parameter for the Adam optimizer when generating sample images.",
                "sample_amsgrad": "Sample AMSGrad",
                "sample_amsgrad_help": "Whether to use the AMSGrad variant of the Adam optimizer when generating sample images.",
                "sample_weight_decay": "Sample Weight Decay",
                "sample_weight_decay_help": "The weight decay (L2 penalty) to apply to the optimizer when generating sample images.",
                "sample_max_grad_norm": "Sample Max Grad Norm",
                "sample_max_grad_norm_help": "The maximum norm for gradient clipping when generating sample images.",
                "sample_lr_scheduler": "Sample LR Scheduler",
                "sample_lr_scheduler_help": "The learning rate scheduler to use when generating sample images.",
                "sample_warmup_steps": "Sample Warmup Steps",
                "sample_warmup_steps_help": "The number of steps to perform learning rate warmup when generating sample images.",
                "sample_total_steps": "Sample Total Steps",
                "sample_total_steps_help": "The total number of training steps when generating sample images.",
                "sample_gradient_accumulation_steps": "Sample Gradient Accumulation Steps",
                "sample_gradient_accumulation_steps_help": "The number of steps to accumulate gradients over when generating sample images.",
                "sample_clip_grad": "Sample Clip Grad",
                "sample_clip_grad_help": "Whether to clip gradients during training when generating sample images.",
                "sample_log_interval": "Sample Log Interval",
                "sample_log_interval_help": "The interval (in steps) at which to log training progress when generating sample images.",
                "sample_save_interval": "Sample Save Interval",
                "sample_save_interval_help": "The interval (in steps) at which to save checkpoints when generating sample images.",
                "sample_resume_from_checkpoint": "Sample Resume from Checkpoint",
                "sample_resume_from_checkpoint_help": "Whether to resume training from the latest checkpoint when generating sample images.",
                "sample_checkpoint_path": "Sample Checkpoint Path",
                "sample_checkpoint_path_placeholder": "e.g. path/to/checkpoint",
                "sample_checkpoint_path_help": "The path to the checkpoint file to resume training from when generating sample images."
            },
            "metadata": {
                "title": "Model Metadata",
                "title_label": "Title",
                "title_help": "Public name of the LoRA.",
                "author": "Author",
                "author_help": "Your name.",
                "desc": "Description",
                "desc_help": "Public description.",
                "license": "License",
                "license_help": "Usage license.",
                "tags": "Tags",
                "tags_help": "Search tags.",
                "comment": "Training Comment",
                "comment_help": "Private notes saved in metadata."
            },
            "progress": {
                "title": "Training Progress",
                "status": "Status",
                "eta": "ETA",
                "vram": "VRAM",
                "steps": "Steps",
                "epochs": "Epochs",
                "epoch_prefix": "Epoch",
                "loss": "Loss",
                "loss_label": "LOSS",
                "start": "Start Training",
                "idle": "System Idle",
                "btn_stop": "Stop Training"
            },
            "conversion": {
                "title": "Model Conversion",
                "subtitle": "Convert LoRA models between formats (Diffusers &harr; Kohya/LDM)",
                "card_title": "Conversion Tool",
                "help_text": "Use this tool to fix compatibility issues with AUTOMATIC1111/Forge. It converts \"Diffusers\" style keys to \"Kohya/LDM\" style keys and vice versa.",
                "input_model": "Input Model (.safetensors)",
                "btn_refresh": "Refresh",
                "model_architecture": "Model Architecture",
                "architecture_help": "Specifies the architecture logic required for correct key mapping. Selecting the wrong architecture will result in a non-functional model.",
                "target_format": "Target Format",
                "target_format_kohya": "Kohya / LDM (for A1111, Forge, ComfyUI)",
                "output_filename": "Output Filename (Optional)",
                "output_filename_placeholder": "Leave empty to auto-name (e.g. model_converted.safetensors)",
                "btn_convert": "Convert Model",
                "success": "Conversion Successful!"
            }
        },
        "console": {
            "title": "Systemkonsole",
            "subtitle": "Echtzeit-Trainingslogs und Systemausgabe",
            "output_title": "Konsolenausgabe",
            "clear": "Konsole leeren",
            "copy": "Logs kopieren",
            "auto_scroll": "Auto-Scroll",
            "wrap_lines": "Zeilen umbrechen",
            "status": "Status",
            "step": "Schritt",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU-Last",
            "cpu_load": "CPU-Last",
            "ram": "RAM",
            "waiting": "Warte auf Trainingsstart..."
        },
        "metadata_editor": {
            "title": "Metadaten-Editor",
            "subtitle": "Verwalte eingebettete Metadaten für LoRA- und Checkpoint-Dateien",
            "select_file": "Datei auswählen",
            "select_placeholder": "Datei auswählen...",
            "select_help": "Wähle eine Modell- oder Metadatendatei, um ihre internen Metadaten anzuzeigen und zu bearbeiten.",
            "btn_load": "Metadaten laden",
            "load_help": "Lade Metadaten aus der ausgewählten Datei zum Bearbeiten.",
            "btn_save": "Metadaten speichern",
            "load_model": "Modell laden",
            "save_metadata": "Metadaten speichern",
            "clear_fields": "Felder leeren"
        },
        "modals": {
            "save_preset_title": "Voreinstellung speichern",
            "save_preset_placeholder": "Voreinstellungsname eingeben...",
            "btn_confirm": "Bestätigen",
            "btn_cancel": "Abbrechen",
            "new_preset_title": "Neue Voreinstellung",
            "new_preset_placeholder": "Neuen Voreinstellungsnamen eingeben...",
            "preset_title": "Voreinstellung",
            "preset_name": "Voreinstellungsname",
            "preset_placeholder": "Voreinstellungsnamen eingeben...",
            "btn_create": "Voreinstellung erstellen",
            "delete_preset_title": "Voreinstellung löschen",
            "delete_preset_confirm": "Bist du sicher, dass du diese Voreinstellung löschen möchtest?",
            "delete_title": "Löschung bestätigen",
            "delete_confirm": "Bist du sicher, dass du löschen möchtest",
            "delete_warning": "Diese Aktion kann nicht rückgängig gemacht werden. Die Datei wird permanent von der Festplatte entfernt.",
            "btn_delete": "Löschen",
            "stop_title": "Training stoppen",
            "stop_confirm": "Bist du sicher, dass du den Trainingsprozess stoppen möchtest?",
            "stop_warning": "Jeder nicht gespeicherte Fortschritt geht verloren. Der aktuelle Checkpoint wird nicht gespeichert.",
            "btn_stop": "Training stoppen"
        },
        "notifications": {
            "success": "Erfolg",
            "error": "Fehler",
            "warning": "Warnung",
            "info": "Information"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Option auswählen...",
            "loading": "Lade...",
            "searching": "Suche...",
            "no_results": "Keine Ergebnisse gefunden",
            "all": "Alle",
            "none": "Keine"
        }
    }
}