{
    "language_name": "Русский",
    "ui": {
        "sidebar": {
            "training": "Обучение",
            "console": "Консоль",
            "metadata": "Метаданные",
            "updates": "Обновления",
            "system": "Система",
            "connecting": "Подключение...",
            "language": "Язык"
        },
        "updates": {
            "title": "Обновления системы",
            "subtitle": "Держи установку Onika в актуальном состоянии",
            "check_status_default": "Проверь обновления, чтобы увидеть доступные изменения.",
            "btn_check": "Проверить обновления",
            "btn_apply": "Применить обновления",
            "details_title": "Детали обновления",
            "col_file": "Путь к файлу",
            "col_status": "Статус",
            "log_title": "Журнал обновлений",
            "status_uptodate": "Ваша система обновлена.",
            "status_failed": "Не удалось проверить обновления.",
            "status_error": "Ошибка подключения к серверу обновлений.",
            "status_applying": "Применение обновлений...",
            "status_checking": "Проверка обновлений...",
            "btn_checking": "Проверка...",
            "found_updates": "Найдено {count} доступных обновлений.",
            "checked_at": "Проверено: {date}",
            "updated_count": "Обновлено {count} файлов.",
            "failed_count": "Не удалось обновить {count} файлов.",
            "file_error": "Ошибка в {path}: {error}",
            "apply_success": "Обновления успешно применены!",
            "apply_partial": "Обновление завершено с некоторыми ошибками."
        },
        "training": {
            "title": "Обучение",
            "subtitle": "Настрой и запусти обучение LoRA/LyCORIS",
            "preset": {
                "label": "Пресет",
                "select_placeholder": "Выбери пресет...",
                "btn_load": "Загрузить",
                "btn_save": "Сохранить пресет",
                "btn_new": "Новый пресет",
                "btn_delete": "Удалить",
                "help": "Загружает пресет из <span class=\"mono\">presets/</span> и применяет его к форме. Пресеты — быстрый способ переключать профили. «Сохранить пресет» перезаписывает выбранный пресет. «Новый пресет» создаёт новый с твоим названием. «Удалить» удаляет выбранный пресет."
            },
            "tabs": {
                "model": "Модель",
                "network": "Сеть",
                "dataset": "Датасет",
                "text_encoder": "Текстовый энкодер",
                "aug": "Аугментация & кэширование",
                "learning": "Параметры обучения",
                "advanced": "Дополнительно",
                "samples": "Сэмплы",
                "metadata": "Метаданные"
            },
            "model": {
                "base_model": "Базовая модель",
                "base_model_help": "Базовая модель, на которой будет идти обучение. Если выбрать модель, максимально близкую к твоему целевому стилю, сходимость обычно будет быстрее. Убедись, что архитектура совпадает с настройками обучения (например, SDXL base для обучения SDXL). Обучение на базовой модели вроде SDXL 1.0 чаще стабильнее, чем на сильно дообученной «baked» модели.",
                "architecture": "Архитектура модели",
                "architecture_help": "Задаёт логику архитектуры (SDXL, SD1.5, Flux), нужную для загрузки весов и работы с латентами. Если архитектура не совпадает с базовой моделью, ты получишь ошибки несовпадения размеров (shape mismatch).",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Авто-оптимизация под моё железо",
                "optimize_help": "Автоматически подбирает precision, quantization и настройки памяти на основе обнаруженного объёма VRAM на твоём GPU.",
                "btn_adjust": "Автонастройка (с учётом датасета)",
                "adjust_help": "Анализирует твой датасет (количество/размеры изображений + captions) и рекомендует стабильные настройки обучения для выбранной архитектуры. Применяет изменения как пресет.",
                "quantization": "Quantization модели (Q-LoRA)",
                "quantization_none": "None (обычный fp16/bf16)",
                "quantization_8bit": "8-bit (Low VRAM)",
                "quantization_4bit": "4-bit (Extreme Low VRAM)",
                "quantization_help": "Снижает точность модели до 8-bit или 4-bit, чтобы заметно уменьшить требования к VRAM и дать возможность обучаться даже на железе с ~8GB VRAM. Учти: более низкая точность может слегка ударить по точности и требует библиотеку `bitsandbytes`.",
                "output_name": "Имя вывода",
                "output_name_placeholder": "e.g. character_lora_v1",
                "output_name_help": "Префикс имени файла для сохранённых адаптеров LoRA. Используй уникальные имена, чтобы удобно хранить версии и не перезаписать прошлые результаты.",
                "output_dir": "Папка вывода",
                "output_dir_help": "Папка, куда будут сохраняться результаты обучения. Проверь, что на диске достаточно места — иначе обучение может оборваться.",
                "save_precision": "Точность сохранения",
                "save_precision_help": "Разрядность для сохраняемых файлов модели. `float16` — стандартный баланс размера и точности, а `float32` даёт максимальную точность ценой существенно большего размера файлов.",
                "save_format": "Формат сохранения",
                "save_format_help": "Формат выходных файлов. `safetensors` рекомендуется из-за безопасности и скорости загрузки. `ckpt` — устаревший формат, а `diffusers` сохраняет модель в виде структуры папок.",
                "save_epochs": "Сохранять каждые N эпох",
                "save_epochs_help": "Частота сохранения checkpoint'ов во время обучения. Регулярные сохранения помогают восстановиться после краша и дают несколько версий, чтобы оценить overfitting.",
                "save_steps": "Сохранять каждые N шагов",
                "save_steps_placeholder": "Необязательно",
                "save_steps_help": "Альтернативная частота сохранения, привязанная к шагам, а не к эпохам.",
                "resume": "Продолжить с checkpoint",
                "resume_placeholder": "e.g. latest or path/to/checkpoint-1000",
                "resume_help": "Продолжает обучение из ранее сохранённого состояния. Учти: если при продолжении менять базовые параметры вроде learning rate или rank, обучение может стать нестабильным.",
                "save_best": "Сохранять только лучшие модели (минимальный loss)",
                "save_best_help": "Оставляет только checkpoint с самым низким зафиксированным loss, чтобы экономить место на диске. Учти: самый низкий loss не всегда означает лучшее визуальное качество.",
                "checkpoints_limit": "Лимит checkpoint'ов",
                "checkpoints_limit_placeholder": "Необязательно",
                "checkpoints_limit_help": "Максимальное число checkpoint'ов, которые нужно хранить. Старые checkpoint'ы автоматически удаляются, чтобы контролировать расход места."
            },
            "network": {
                "type": "Тип сети",
                "type_help": "Архитектура адаптера. LoRA — индустриальный стандарт. LoHa и LoKr дают больше выразительности, но могут требовать более тонкой настройки. OFT спроектирован так, чтобы сохранять энергию гиперсферы, и особенно хорошо подходит для моделей Flux.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonal Finetuning)",
                "module": "Модуль сети (Network Module)",
                "module_help": "Внутренний Python-модуль, который используется адаптером. Это продвинутая настройка — обычно её лучше не трогать.",
                "dim": "Размерность сети (Network Dim / Rank)",
                "dim_help": "Ёмкость адаптера. Более высокие значения (например, 128) позволяют выучить больше деталей, но повышают риск overfitting и дают более крупные файлы. Низкие значения (например, 16) обычно лучше обобщают и дают меньшие файлы.",
                "alpha": "Alpha сети (Network Alpha)",
                "alpha_help": "Коэффициент масштабирования, который не даёт обновлениям весов быть слишком агрессивными. Частое правило: ставь Alpha примерно в половину от Network Dim для стабильности, или равным Dim для более сильного эффекта.",
                "algo": "Алгоритм LyCORIS",
                "algo_help": "Вариант алгоритма LyCORIS (актуально только для режимов LyCORIS/LoHa). Разные алгоритмы балансируют выразительность и стабильность. Если не уверен(а), начни с <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span> и переключайся только когда можешь явно измерить пользу.",
                "conv_dim": "Размер conv (Conv Rank / Dim)",
                "conv_dim_help": "Необязательный conv-rank для LoCon/LyCORIS. Низкие значения дают небольшой прирост свёрточной ёмкости; высокие увеличивают VRAM и могут быстро переобучить текстуры/детали. Оставь пустым, если ты явно не используешь conv-метод.",
                "conv_alpha": "Alpha conv (Conv Alpha)",
                "conv_alpha_help": "Необязательное масштабирование (alpha) для conv-части. Низкая alpha делает conv мягче; высокая — «кусает» сильнее и может раскачивать обучение/перестреливать. Обычно <= conv dim; если conv слишком сильный — сначала снизь alpha, а потом уже dim.",
                "dora_wd": "Weight Decay для DoRA (DoRA Weight Decay)",
                "dora_wd_help": "Weight decay, который применяется именно к magnitude-векторам DoRA.",
                "network_dropout": "Dropout сети (Network Dropout)",
                "network_dropout_help": "Случайно «выбрасывает» выходы нейронов внутри адаптера, чтобы улучшить устойчивость.",
                "rank_dropout": "Dropout rank (Rank Dropout)",
                "rank_dropout_help": "Случайно отключает отдельные rank-измерения как форму регуляризации.",
                "module_dropout": "Dropout модуля (Module Dropout)",
                "module_dropout_help": "Случайно отключает целые модули во время обучения, чтобы снижать overfitting и улучшать обобщение.",
                "lora_blocks": "Блоки LoRA (LoRA Blocks)",
                "lora_blocks_help": "Позволяет нацеливаться на конкретные блоки (например, mid blocks) внутри архитектуры модели.",
                "lora_layers": "Слои LoRA (LoRA Layers)",
                "lora_layers_help": "Позволяет нацеливаться на конкретные слои (например, attention-слои) для «хирургического» fine-tuning.",
                "advanced_lora": "Доп. опции LoRA",
                "lycoris_settings": "Настройки LyCORIS",
                "args": "Аргументы сети (args)",
                "args_help": "Дополнительные аргументы для network module в виде пар key=value, разделённых запятыми. Используй это для специальных опций вроде dropout или decomposition. Неверные аргументы могут сделать обучение нестабильным.",
                "args_placeholder": "key=value, key2=value2",
                "conv_alpha_placeholder": "Optional",
                "conv_dim_placeholder": "Optional",
                "dora_wd_placeholder": "Optional",
                "lora_blocks_placeholder": "Optional",
                "lora_layers_placeholder": "Optional",
                "module_placeholder": "Optional"
            },
            "dataset": {
                "path": "Путь к датасету",
                "path_help": "Папка с тренировочными изображениями и соответствующими им caption-файлами (например, .txt). Качество датасета — самый критичный фактор успеха: убедись, что captions точно описывают изображения.",
                "resolution": "Разрешение",
                "resolution_help": "Целевое разрешение обучения. Более высокое разрешение сохраняет больше деталей, но требует больше VRAM. Убедись, что разрешение подходит архитектуре (например, 1024x1024 для SDXL, 512x512 для SD1.5).",
                "batch_size": "Размер batch",
                "batch_size_help": "Сколько изображений обрабатывается одновременно. Больший batch_size может ускорить обучение и сделать градиенты более плавными, но заметно увеличивает расход VRAM.",
                "max_epochs": "Макс. эпох",
                "max_epochs_help": "Общее число полных проходов по датасету. Для обучения LoRA обычно достаточно 10–20 эпох. Слишком много эпох легко приводит к overfitting и «fried» изображениям.",
                "max_steps": "Макс. шагов",
                "max_steps_help": "Необязательный жёсткий лимит на общее число шагов обучения.",
                "max_steps_placeholder": "Необязательно",
                "bucketing": "Включить bucketing по aspect ratio",
                "bucketing_help": "Автоматически группирует изображения по соотношению сторон, чтобы избежать лишнего кропа и сохранить исходную композицию данных.",
                "bucket_steps": "Шаг bucket-разрешения",
                "bucket_steps_help": "Размер сетки для размеров bucket'ов. 64 — стандарт для большинства архитектур.",
                "min_bucket": "Мин. bucket-разрешение",
                "min_bucket_help": "Минимально допустимое разрешение bucket'а. Это не даёт обучению использовать слишком маленькие или мыльные изображения.",
                "max_bucket": "Макс. bucket-разрешение",
                "max_bucket_help": "Максимально допустимое разрешение bucket'а — помогает избежать OOM на особенно больших изображениях.",
                "center_crop": "Center crop (умный 1:1)",
                "center_crop_help": "Если включено, изображения, близкие к квадрату (например, 1210x1280), будут центр-кропнуты до идеального 1:1. Рекомендуется для обучения персонажей, чтобы кадрирование было стабильным.",
                "no_upscale": "Без upscale",
                "no_upscale_help": "Запрещает апскейл маленьких изображений до размеров bucket'а, чтобы избежать лишних артефактов.",
                "dreambooth": "DreamBooth и Prior Preservation",
                "prior_preservation": "Включить Prior Preservation (Reg Images)",
                "prior_preservation_help": "Использует regularization images, чтобы модель, обучаясь на конкретном экземпляре, не «забыла» своё базовое понимание класса (например, «person»). Это важно для сохранения общего знания модели, но увеличивает время обучения.",
                "num_class_images": "Количество class images",
                "num_class_images_help": "Целевое число regularization images. Частая рекомендация — 100 изображений класса на одно instance-изображение.",
                "instance_prompt": "Instance prompt",
                "instance_prompt_help": "Комбинация уникального trigger-слова и class-слова (например, «sks person»), которая идентифицирует конкретный объект обучения.",
                "instance_prompt_placeholder": "e.g. a photo of sks person",
                "class_prompt": "Class prompt",
                "class_prompt_help": "Общее class-слово (например, «person»), по которому определяются regularization images.",
                "class_prompt_placeholder": "e.g. a photo of a person",
                "reg_dir": "Папка Reg Images",
                "reg_dir_help": "Папка с regularization images. Используется только если включён Prior Preservation.",
                "reg_dir_placeholder": "Необязательно",
                "auto_gen": "Автогенерация Reg Images",
                "auto_gen_help": "Автоматически генерирует regularization images, если их ещё нет в указанной папке.",
                "gen_settings": "Настройки генерации",
                "neg_class_prompt": "Negative class prompt",
                "neg_class_prompt_help": "Negative prompt, который используется при генерации regularization images.",
                "neg_class_prompt_placeholder": "Необязательный negative prompt",
                "guidance": "Шкала guidance (Guidance Scale)",
                "guidance_help": "CFG scale для генерации regularization images.",
                "steps": "Шаги Reg (Reg Steps)",
                "steps_help": "Количество sampling steps для генерации regularization images.",
                "scheduler": "Reg scheduler",
                "scheduler_help": "Sampler, который используется при генерации regularization images.",
                "seed": "Reg seed",
                "seed_help": "Seed для генерации regularization images. -1 = случайный.",
                "sample_warning": "Совет: если «Sample Every N Steps» и «Sample Every N Epochs» оба оставлены пустыми, Onika не будет генерировать sample images (экономит кучу времени). На слабых и средних GPU генерировать samples во время обучения обычно не рекомендуется.",
                "shuffle": "Перемешивание DataLoader",
                "shuffle_help": "Перемешивает порядок изображений в каждой эпохе, чтобы модель не выучивала последовательность датасета.",
                "workers": "Workers DataLoader",
                "workers_help": "Количество CPU-потоков для загрузки и препроцессинга данных. Большие значения быстрее кормят GPU, но если переборщить — система может начать лагать.",
                "persistent_workers": "Persistent workers DataLoader",
                "persistent_workers_help": "Оставляет workers загрузчика данных активными между эпохами, ускоряя обучение ценой повышенного расхода RAM."
            },
            "text_encoder": {
                "ti_title": "Textual Inversion (Pivotal Tuning)",
                "weighting_title": "Взвешивание captions",
                "train": "Обучать Text Encoder",
                "train_help": "Включает обучение text encoder вместе с UNet. Это может улучшить prompt adherence, но иногда снижает гибкость/редактируемость модели.",
                "clip_skip": "Clip Skip",
                "clip_skip_help": "Пропускает верхние N слоёв CLIP text encoder. Для SD1.5 обычно используют 1, а для SDXL чаще 0. Неверные значения могут ухудшить понимание промптов.",
                "train_ti": "Обучать Textual Inversion",
                "train_ti_help": "Обучает новые tokens (Textual Inversion), добавляя в словарь модели конкретные слова/понятия.",
                "ti_frac": "Доля обучения TI",
                "ti_frac_help": "Процент от общего числа эпох, в течение которых активно обучение Textual Inversion.",
                "te_frac": "Доля обучения TE",
                "te_frac_help": "Процент от общего числа эпох, в течение которых активно обучение Text Encoder.",
                "emphasis": "Emphasis по умолчанию",
                "emphasis_help": "Множитель по умолчанию для emphasized-тегов, если вес явно не указан.",
                "de_emphasis": "De-Emphasis по умолчанию",
                "de_emphasis_help": "Множитель по умолчанию для de-emphasized-тегов, если вес явно не указан.",
                "enable_weighted": "Включить weighted captions",
                "enable_weighted_help": "Разрешает weighted-синтаксис (например, \"(word:1.1)\") в captions, чтобы точно управлять важностью отдельных терминов.",
                "new_tokens": "Новых tokens на abstraction",
                "new_tokens_help": "Сколько векторов назначается каждому новому token. Больше векторов может захватить больше деталей, но такие tokens сложнее качественно обучить.",
                "token_abs": "Token abstraction",
                "token_abs_help": "Строка-заглушка (например, \"TOK\"), которая используется в captions, чтобы обозначать новый концепт."
            },
            "aug": {
                "aug_title": "Аугментация изображений",
                "aug_mode": "Режим аугментации",
                "aug_mode_help": "Определяет, когда аугментации включаются во время обучения. <strong>Always:</strong> каждое изображение аугментируется на каждом шаге — максимум разнообразия, но может немного замедлить обучение. <strong>Per Epoch Only:</strong> новые аугментации «бросаются» один раз на эпоху и остаются одинаковыми внутри этого прохода. Стабильнее, но всё равно меняется между эпохами. <strong>Random Probability:</strong> на каждом шаге аугментация для изображения может сработать или нет — в зависимости от отдельных настроек вроде <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>. В большинстве случаев хватает «Always» или «Per Epoch Only».",
                "color_aug": "Сила Color Aug",
                "color_aug_help": "Насколько агрессивно «дёргать» цвета (hue, saturation, brightness) во время обучения. <strong>0.0:</strong> цвета остаются как есть — без вмешательства. <strong>0.3-0.5:</strong> мягкая вариативность, отлично подходит большинству датасетов и помогает модели не запоминать точные цвета. <strong>0.7-1.0:</strong> сильные сдвиги цвета. Может давать странные сочетания, но реально толкает обобщение. Особенно полезно для LoRA персонажей/стилей, когда тебе важно, чтобы модель «поймала» формы и концепты и не зациклилась на конкретной палитре. Выключи, если важна точная цветопередача (брендовые цвета, конкретные наряды и т. п.).",
                "flip_aug": "Вероятность Flip Aug",
                "flip_aug_help": "Шанс отзеркалить каждое изображение по горизонтали. <strong>0.5</strong> = условный бросок монетки 50/50, по сути удваивает датасет зеркальными версиями. <strong>0.0</strong> = без флипа вообще. <strong>Важно:</strong> поставь 0, если на изображениях есть: • <strong>текст или логотипы</strong> (станут наоборот) • <strong>асимметричные вещи</strong> (машины, лица с выраженной «стороной») • <strong>направленный контент</strong> (левая/правая рука, конкретные позы). Отлично работает для симметричных сюжетов: центрированные портреты, абстракция, многие аниме-персонажи. Один из лучших приёмов против overfitting на маленьких датасетах.",
                "crop_scale": "Scale для Random Crop",
                "crop_scale_help": "Минимальный уровень зума для случайного кропа. <strong>1.0:</strong> без кропа — изображения остаются в полном размере. <strong>0.8:</strong> кроп от 80% до 100%, даёт лёгкие вариции масштаба. <strong>0.5:</strong> заметнее — может приблизить до 2×, показывая модели очень разные кадрирования. Кроп учит модель работать с объектами в разных масштабах и композициях. Суперполезно, если датасет — это одни «идеально центрированные хедшоты», а ты хочешь больше гибкости в результатах. Держи 1.0, если точное кадрирование принципиально.",
                "caption_aug_title": "Аугментация captions",
                "shuffle": "Перемешивание captions (Shuffle Captions)",
                "shuffle_help": "Каждый раз при использовании captions перемешивает порядок тегов/слов. Это не даёт модели заучивать фиксированные шаблоны вроде «blue eyes всегда перед long hair» — вместо этого она учится, что теги могут встречаться где угодно. Идеально для booru-стиля с тегами через запятую, где порядок не важен. Используй <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a>, если нужно, чтобы trigger-слово оставалось на месте. <strong>Не используй это</strong> для captions естественным языком, где порядок слов меняет смысл («woman holding cat» ≠ «cat holding woman»).",
                "keep_tokens": "Зафиксировать tokens (Keep Tokens)",
                "keep_tokens_help": "Сколько tokens в начале каждого caption считаются «священными» и не будут перемешиваться. <strong>0:</strong> перемешивается вообще всё. <strong>1:</strong> первый token (обычно trigger) всегда остаётся первым. <strong>2-3:</strong> защищает паттерны вида «trigger, character_name». Если твои captions выглядят как «mytrigger, blonde hair, blue eyes, ...», то значение 1 закрепляет «mytrigger» в начале, а остальное рандомится. Это помогает модели связать trigger ↔ концепт, не заучивая конкретный порядок тегов. Имеет смысл только когда включён <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a>.",
                "dropout": "Вероятность dropout (Dropout Rate)",
                "dropout_help": "Вероятность полностью выкинуть caption у изображения (заменив его на пустоту). Когда captions пропадают, модели приходится объяснять изображение чисто по визуалу — это тренирует unconditional generation. <strong>0.0:</strong> captions всегда присутствуют. <strong>0.05-0.1:</strong> лёгкий dropout — мягко улучшает поведение при пустых/слабых промптах. <strong>0.15-0.2:</strong> агрессивнее — сильнее толкает unconditional качество. Dropout помогает CFG (classifier-free guidance) работать лучше на inference, потому что модель реально знает, как выглядит «no prompt». Но в обучении LoRA, где prompt adherence — король, слишком большой dropout может навредить. Небольшое значение (0.05) часто полезно; для очень коротких прогонов можно вообще отключить.",
                "dropout_epochs": "Dropout каждые N эпох",
                "dropout_epochs_help": "Вместо случайного dropout на каждом шаге, это отключает captions на конкретных эпохах. <strong>0:</strong> возвращается к обычному поведению <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a>. <strong>1:</strong> каждая эпоха идёт без captions (довольно экстремально). <strong>5:</strong> эпохи 5, 10, 15... проходят без captions. Это даёт более структурированный режим: обычно обучение нормальное, но периодически — «слепые» эпохи. Полезно для некоторых учебных программ, но на практике чаще используют случайный dropout по шагам.",
                "noise_title": "Настройки noise",
                "noise_type": "Тип Noise Offset",
                "noise_type_help": "Математический метод, который используется для расчёта noise offset. «Original» — стандартная реализация.",
                "noise_offset": "Сила Noise Offset",
                "noise_offset_help": "Добавляет смещение к шуму во время обучения — это помогает модели давать лучший динамический диапазон (глубже чёрные и ярче белые). Обычно рекомендуют 0.05–0.1.",
                "adaptive_noise": "Adaptive Noise Scale",
                "adaptive_noise_help": "Масштабирует шум на основе ошибки предсказания модели во время обучения.",
                "noise_random": "Noise Offset (random strength)",
                "noise_random_help": "Рандомит силу noise offset для каждого шага обучения.",
                "multires_iter": "Multires Noise Iter",
                "multires_iter_help": "Применяет multi-resolution noise, чтобы улучшить обучение тонких текстур и деталей.",
                "multires_discount": "Multires Noise Discount",
                "multires_discount_help": "Коэффициент скидки (discount factor), который применяется к итерациям multi-resolution noise.",
                "edm": "EDM-style обучение (SDXL)",
                "edm_help": "Использует формулировку EDM (Elucidating the Design Space of Diffusion-Based Generative Models), которая может дать более качественные результаты для моделей SDXL.",
                "caching_title": "Кэширование",
                "cache_latents": "Кэшировать latents в RAM",
                "cache_latents_help": "Предварительно вычисляет VAE latents, чтобы заметно ускорить обучение (часто 2x–3x). Если включено, все latents кэшируются и остаются в RAM на всё обучение. Это отключает часть аугментаций «в реальном времени».",
                "cache_te": "Кэшировать text embeddings",
                "cache_te_help": "Предварительно вычисляет text embeddings для ускорения обучения. Эта настройка несовместима с активным обучением Text Encoder.",
                "cache_disk": "Кэшировать latents на диск",
                "cache_disk_help": "Сохраняет заранее вычисленные latents на диск, чтобы экономить RAM. Учти: медленный диск/IO может ухудшить производительность. Эти latents переиспользуются между сессиями обучения. Если датасет изменился, старые latents нужно удалить из cache вручную.",
                "vae_batch": "Размер batch для VAE (VAE Batch Size)",
                "vae_batch_help": "Batch size, который используется для VAE-энкодинга во время кэширования. Большие значения ускоряют процесс, но требуют больше VRAM.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Оптимизатор",
                "optimizer_help": "Алгоритм, обновляющий веса модели. <strong>AdamW8bit:</strong> Эффективен по памяти. <strong>Prodigy:</strong> Адаптивный (авто-LR). <strong>DAdaptation:</strong> Адаптивный.",
                "optimizer_args": "Optimizer Args",
                "optimizer_args_help": "Extra tweaks passed straight to your <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>. Format: <code>key=value</code>, comma-separated. Some examples: <strong>weight_decay=0.01</strong> — L2 regularization to fight overfitting (common with AdamW). <strong>betas=(0.9,0.999)</strong> — Adam momentum coefficients. <strong>d_coef=1.0</strong> — D-coefficient for Prodigy/DAdaptAdam. <strong>eps=1e-8</strong> — Tiny number for numerical stability. Defaults are solid for most training. Only mess with these if you're following a specific guide or you really know what you're doing.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning Rate",
                "lr_help": "The base learning rate for training.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Скорость обучения для UNet (основная часть модели). Обычно совпадает с общим LR.",
                "unet_lr_placeholder": "Optional",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Скорость обучения для Text Encoder (понимание промптов). Обычно ниже, чем UNet LR (напр., 5e-5).",
                "te_lr_placeholder": "Optional",
                "scheduler": "Планировщик (LR Scheduler)",
                "scheduler_help": "Как LR меняется со временем. <strong>Constant:</strong> Не меняется. <strong>Cosine:</strong> Плавно снижается. <strong>Cosine with Restarts:</strong> Циклическое снижение.",
                "warmup": "Warmup Steps",
                "warmup_help": "Количество шагов в начале, когда LR постепенно растет от 0 до максимума. Помогает стабилизировать начало обучения.",
                "warmup_ratio": "Warmup Ratio",
                "warmup_ratio_help": "Процент от общего количества шагов для warmup (напр., 0.1 = 10%).",
                "cycles": "Циклы (Restarts)",
                "cycles_help": "Для планировщика 'Cosine with Restarts' — сколько раз LR будет сбрасываться к максимуму.",
                "scheduler_power": "Scheduler Power",
                "scheduler_power_help": "The exponent when using <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial scheduler</a>. Shapes how aggressively the LR drops off. <strong>1.0</strong> = linear decay, nothing fancy. <strong>> 1.0</strong> (like 2.0) = fast drop early, slows down later. <strong>< 1.0</strong> (like 0.5) = slow drop early, faster toward the end. Only does anything with polynomial scheduler selected. Default 1.0 is fine for most cases.",
                "ema_unet": "EMA for UNet",
                "ema_unet_help": "Keeps a running average of UNet weights during training. Instead of using the raw weights from the last step (which can be noisy), EMA gives you a \"smoothed\" version that typically looks better and generalizes more reliably. Think of it as noise reduction for your model weights — the EMA checkpoint often outperforms the raw one. The catch: it doubles VRAM usage for UNet weights (you're storing both sets). Smoothing rate controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a>. Strongly recommended for longer training runs if your VRAM can handle it.",
                "ema_te": "EMA for Text Encoder",
                "ema_te_help": "Same idea as <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, but for the text encoder instead. Less critical than UNet EMA since text encoder weights usually shift more gently during LoRA training. Still adds VRAM overhead for storing the extra weights. Turn this on if you're training the text encoder and want the absolute best quality.",
                "ema_decay": "EMA Decay",
                "ema_decay_help": "How much the EMA \"remembers\" old weights vs. new: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Higher (0.999-0.9999):</strong> More smoothing, slower to pick up changes. Good for long training runs. <strong>Lower (0.99-0.995):</strong> Less smoothing, reacts faster to new weights. Better for short runs. <strong>0.995</strong> works well for typical LoRA training (hundreds to a few thousand steps). Doing tens of thousands of steps? Try <strong>0.9999</strong>. Only matters if <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for UNet</a> or <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA for Text Encoder</a> is on."
            },
            "advanced": {
                "attention": "Attention Backend",
                "attention_help": "Which implementation handles the attention mechanism — the most memory-hungry part of transformers. <strong>SDPA:</strong> PyTorch's built-in optimized attention (requires PyTorch 2.0+). Good balance of speed and memory, no extra installs. Recommended default. <strong>xFormers:</strong> Meta's attention library, often the fastest option on NVIDIA cards. Can slash VRAM by 20-40% compared to vanilla attention. Needs separate installation. <strong>None:</strong> Plain PyTorch attention — eats more memory but works everywhere. Only fall back to this if SDPA or xFormers are giving you trouble. Both SDPA and xFormers play nicely with <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Aggressive Memory Saving</a>.",
                "grad_acc": "Gradient Accumulation",
                "grad_acc_help": "Stacks up gradients over multiple passes before updating weights — essentially fakes a bigger batch size. Your <strong>effective batch = batch_size × gradient_accumulation</strong>. So batch_size=2 with accumulation=4 acts like batch_size=8. This is a lifesaver for VRAM-starved systems: can't fit batch=8? Do batch=2 with accumulation=4 instead. Bigger effective batches mean more stable training, but you might need to tweak your <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">learning rate</a>. Downside: slows things down proportionally (4× accumulation = 4× slower per effective step).",
                "mixed_precision": "Mixed Precision",
                "mixed_precision_help": "Numerical precision for calculations — directly impacts VRAM and speed. <strong>fp16:</strong> Half-precision floats, cuts memory in half compared to full precision. Solid choice for GTX 10-series and RTX 20-series cards, though SDXL can sometimes throw NaN errors due to fp16's limited range. <strong>bf16:</strong> Also 16 bits but with better range for exponents — much more stable. Definitely use this on RTX 30-series and newer. Won't work on older hardware. <strong>fp8:</strong> 8-bit precision for extreme memory savings. Experimental and may hurt quality. Needs specific hardware. <strong>no:</strong> Full fp32 precision. Eats 2× the VRAM but rock-solid numerically. Only use for debugging NaN issues or if you've got VRAM to burn.",
                "tf32": "Allow TF32",
                "tf32_help": "TensorFloat-32 — a compute mode exclusive to RTX 30-series and newer. Uses fp32 range but with less mantissa precision (10 bits instead of 23), giving you up to 8× speedup on some operations with barely any quality difference for ML workloads. <strong>Always leave this on for RTX 30-series and 40-series cards.</strong> Does nothing on older GPUs (GTX 10-series, RTX 20-series) since they don't have the hardware for it. Only turn off if you suspect TF32 is somehow tanking quality (super rare).",
                "vae_batch": "VAE Batch Size",
                "vae_batch_help": "Separate batch size just for VAE encoding — when images get converted to latent space before the UNet sees them. Setting this lower than your main batch size can tame VRAM spikes during encoding. Leave blank to match training batch size. Set to <strong>1</strong> for minimum VRAM at the cost of slower encoding. Handy if you're OOMing specifically during the latent encoding phase.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max Token Length",
                "max_token_length_help": "Максимальная длина промпта. 75 — стандарт, можно увеличить до 150 или 225.",
                "memory_saving": "Aggressive Memory Saving",
                "memory_saving_help": "Throws everything at VRAM savings: gradient checkpointing, CPU offloading, aggressive garbage collection. <strong>A must-have for 4GB-8GB cards</strong> — lets you train SDXL on hardware that would otherwise choke. Can cut VRAM usage by 30-50% depending on the model. The price? Training becomes 2-4× slower due to recomputation and shuffling data between CPU and GPU. Pairs well with a good <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention Backend</a> (SDPA or xFormers) for maximum savings. Got 16GB+ VRAM? Leave this off and enjoy faster training.",
                "seed": "Seed (Зерно)",
                "seed_help": "Число для воспроизводимости. Оставьте пустым для случайного значения.",
                "seed_placeholder": "Random"
            },
            "flux": {
                "title": "Flux Specific Settings",
                "max_seq_len": "Max Sequence Length",
                "max_seq_len_help": "Max token length for T5 text encoder (Flux / SD3). Higher values allow longer prompts/captions, but increase VRAM use and slow down training. Common values: <strong>256</strong> (default) or <strong>512</strong>.",
                "guidance_scale": "Guidance Scale",
                "guidance_scale_help": "Classifier-free guidance for training Flux and SD3 models. Tells the model how hard to stick to the text prompt during training. <strong>3.5:</strong> Stability AI's recommended default for SD3, works great for Flux too. <strong>Lower (1-2):</strong> More creative/varied outputs but might wander from the prompt. <strong>Higher (5-7):</strong> Tighter prompt adherence but can look a bit stiff. For SD1.5/SDXL, this typically isn't used — guidance only kicks in at inference time for those.",
                "weighting_scheme": "Weighting Scheme",
                "weighting_scheme_help": "How timesteps are sampled during training for flow-matching models (Flux, SD3). This decides which noise levels get more attention during training. <strong>None (Uniform):</strong> Equal love for all noise levels. Safe default when you don't have specific needs. <strong>Logit Normal:</strong> Bell-curve distribution centered on <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> with width set by <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Lets you dial in exactly which timesteps matter most. <strong>Mode:</strong> Focuses hard on a specific timestep, spread controlled by <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a>. These only affect Flux and SD3 — does nothing for SD1.5 or SDXL (epsilon-prediction models).",
                "logit_mean": "Logit Mean",
                "logit_mean_help": "Where the logit-normal distribution peaks when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>0.0:</strong> Centers on the middle timestep (t=0.5). <strong>Negative:</strong> Push focus toward noisier timesteps (coarse structure). <strong>Positive:</strong> Push focus toward cleaner timesteps (fine details). Tune this based on what part of the generation process matters most for your use case. Only kicks in when Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit Std",
                "logit_std_help": "How spread out the logit-normal distribution is when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal weighting</a>. <strong>Lower (0.5-0.8):</strong> Tight focus around <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> — really zeros in on specific timesteps. <strong>Higher (1.5-2.0):</strong> Spreads out more, approaching uniform sampling. <strong>1.0:</strong> Balanced default. Only matters when Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode Scale",
                "mode_scale_help": "How tightly the mode distribution clusters when using <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode weighting</a>. <strong>Lower:</strong> Samples spread across more timesteps. <strong>Higher:</strong> Samples bunch up tighter around the mode. <strong>1.29:</strong> The value from the original SD3 paper. Stick with it unless experimenting. Only relevant when Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep Sampling",
                "min": "Min Timestep",
                "min_help": "Minimum timestep for sampling.",
                "max": "Max Timestep",
                "max_help": "Maximum timestep for sampling.",
                "max_placeholder": "Optional",
                "min_placeholder": "Optional"
            },
            "caption": {
                "title": "Caption Settings",
                "extension": "Caption Extension",
                "extension_help": "File extension for your caption files. Default is <strong>.txt</strong> — so \"image001.png\" expects its caption in \"image001.txt\". Supports whatever extension you want (.caption, .captions, .tags, etc.). Caption file needs to live in the same folder as its image. Change this if your captioning tool outputs something different.",
                "weighted": "Weighted Captions",
                "weighted_help": "Parses A1111-style weights in captions like \"(important thing:1.3)\" or \"[less important:0.7]\". Lets you emphasize or de-emphasize specific words during training. <strong>Weight > 1.0:</strong> Model pays more attention to those tokens. <strong>Weight < 1.0:</strong> Model cares less about those tokens. Great for fine-tuning what aspects of your captions matter most. Turn this off if your captions use literal parentheses that aren't meant as weights."
            },
            "loss": {
                "title": "Loss & Optimization",
                "optimization_title": "Loss Optimization",
                "type": "Loss Type",
                "type_help": "The mathematical function used to calculate the difference between predicted and target noise.",
                "huber_c": "Huber C",
                "huber_c_help": "The threshold parameter for Huber loss. Lower values make it more like L1.",
                "min_snr": "Min SNR Gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. Helps stabilize training by weighting loss based on noise levels.",
                "ip_noise": "IP Noise Gamma",
                "ip_noise_help": "Strength of Input Perturbation noise. Helps with over-smoothing.",
                "noise_offset": "Noise Offset",
                "noise_offset_help": "Adds a tiny constant offset to noise during training so the model can actually generate true blacks and true whites instead of washed-out grays. Standard diffusion struggles with extreme dark/bright areas because the noise schedule doesn't cover them well. <strong>0.035-0.05:</strong> Safe defaults, improve dark/light handling without side effects. <strong>0.05-0.1:</strong> More aggressive push toward better extremes. <strong>Above 0.1:</strong> Might destabilize training or cause color shifts. Works best when paired with <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a> — they complement each other. Essential if your dataset has dark scenes, night shots, or high-contrast images.",
                "v_pred": "V-Pred Like Loss",
                "v_pred_help": "Blends velocity-prediction behavior into standard noise prediction training. Originally from v-prediction models (SD 2.x depth/inpainting) — can smooth out training and reduce artifacts. <strong>0:</strong> Pure noise prediction, the standard approach. <strong>1:</strong> Full velocity prediction mode. <strong>0.1-0.2:</strong> Subtle v-pred influence while staying compatible with noise-pred base models. Can help with smoother gradients and fewer artifacts in some cases. Leave at <strong>0</strong> if things are already working well or you're not sure.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Advanced Loss Options",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> Tweaks the noise scheduler so the final timestep is pure noise (SNR = 0), which unlocks true black generation. Essential companion to <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a> — auto-enabled when using the Auto toggle. Without it, noise offset alone won't fully fix dark image issues. From the paper \"Common Diffusion Noise Schedules and Sample Steps are Flawed\".<br><strong>Debiased Estimation:</strong> Fixes the bias in timestep sampling where some get oversampled. Reweights the loss so all timesteps train evenly, improving overall quality. Works alongside <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> but tackles a different problem.<br><strong>Scale V-Pred Loss:</strong> Normalizes v-prediction loss to match noise-prediction magnitude. Mainly useful for v-pred base models (SD 2.x variants) or when using <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a>. Doesn't do much for standard SD 1.5 or SDXL (epsilon-prediction).<br><strong>Masked Loss (Alpha):</strong> Uses PNG alpha channels as training masks — model only learns from non-transparent pixels. Perfect for character/object training where you want to ignore backgrounds. <strong>Needs PNGs with proper alpha channels!</strong>",
                "presets": "Loss Presets",
                "presets_help": "Quickly apply recommended loss settings for different training goals.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light"
            },
            "samples": {
                "prompts_title": "Промпты для сэмплов",
                "inference_title": "Настройки инференса",
                "schedule_title": "График генерации",
                "generated_title": "Сгенерированные сэмплы",
                "gallery_placeholder": "Сгенерированные сэмплы будут появляться здесь во время обучения...",
                "prompts": "Промпты для сэмплов",
                "prompts_help": "Введите промпты для генерации сэмплов во время обучения. Один промпт на строку.",
                "neg_prompt": "Отрицательный промпт",
                "neg_prompt_help": "Отрицательный промпт для генерации сэмплов.",
                "every_n_epochs": "Каждые N эпох",
                "every_n_epochs_help": "Генерировать сэмплы каждые N эпох.",
                "every_n_steps": "Каждые N шагов",
                "every_n_steps_help": "Генерировать сэмплы каждые N шагов.",
                "sampler": "Сэмплер",
                "sampler_help": "Сэмплер для генерации изображений.",
                "inference_steps": "Шаги инференса",
                "inference_steps_help": "Количество шагов денойзинга для генерации сэмплов.",
                "cfg_scale": "Шкала CFG",
                "cfg_scale_help": "Шкала Classifier Free Guidance для генерации сэмплов.",
                "num_images": "Изображений на промпт",
                "num_images_help": "Количество изображений для каждого промпта.",
                "seed": "Сид",
                "seed_help": "Сид для воспроизводимой генерации. -1 для случайного.",
                "every_n_epochs_placeholder": "Необязательно",
                "every_n_steps_placeholder": "Необязательно",
                "neg_prompt_placeholder": "Необязательный отрицательный промпт",
                "prompts_placeholder": "Один промпт на строку..."
            },
            "metadata": {
                "title": "Метаданные модели",
                "title_help": "Публичное имя LoRA.",
                "author": "Автор",
                "author_help": "Ваше имя.",
                "desc": "Описание",
                "desc_help": "Публичное описание.",
                "license": "Лицензия",
                "license_help": "Лицензия на использование.",
                "tags": "Теги",
                "tags_help": "Теги для поиска.",
                "comment": "Комментарий к обучению",
                "comment_help": "Приватные заметки, сохраненные в метаданных.",
                "comment_placeholder": "например, Обучено на 50 изображениях персонажа X",
                "desc_placeholder": "Опишите вашу LoRA...",
                "license_placeholder": "например, CreativeML Open RAIL-M",
                "tags_placeholder": "например, персонаж, стиль, аниме"
            },
            "progress": {
                "epoch_prefix": "Эпоха",
                "loss_label": "LOSS",
                "start": "Начать обучение",
                "stop": "Остановить обучение",
                "idle": "Система ожидает",
                "caching_latents": "Кэширование латентов",
                "latents_cache": "Кэш латентов"
            },
            "conversion": {
                "title": "Конвертация модели",
                "subtitle": "Конвертируйте модели LoRA между форматами (Diffusers &harr; Kohya/LDM)",
                "card_title": "Инструмент конвертации",
                "help_text": "Используйте этот инструмент для исправления проблем совместимости с AUTOMATIC1111/Forge. Он конвертирует ключи стиля \"Diffusers\" в стиль \"Kohya/LDM\" и наоборот.",
                "input_model": "Входная модель (.safetensors)",
                "btn_refresh": "Обновить",
                "model_architecture": "Архитектура модели",
                "architecture_help": "Задает логику архитектуры, необходимую для правильного сопоставления ключей. Выбор неправильной архитектуры приведет к неработоспособности модели.",
                "target_format": "Целевой формат",
                "target_format_kohya": "Kohya / LDM (для A1111, Forge, ComfyUI)",
                "output_filename": "Имя выходного файла (необязательно)",
                "output_filename_placeholder": "Оставьте пустым для автоименования (например, model_converted.safetensors)",
                "btn_convert": "Конвертировать модель",
                "success": "Конвертация успешна!"
            }
        },
        "console": {
            "title": "Системная консоль",
            "subtitle": "Логи обучения и системный вывод в реальном времени",
            "output_title": "Вывод консоли",
            "clear": "Очистить консоль",
            "status": "Статус",
            "step": "Шаг",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "Загрузка GPU",
            "cpu_load": "Загрузка CPU",
            "ram": "RAM",
            "waiting": "Ожидание начала обучения..."
        },
        "metadata_editor": {
            "title": "Редактор метаданных",
            "subtitle": "Управляй встроенными метаданными для файлов LoRA и чекпоинтов",
            "select_file": "Выбери файл",
            "select_placeholder": "Выбери файл...",
            "select_help": "Выбери модель или файл метаданных для просмотра и редактирования.",
            "btn_load": "Загрузить метаданные",
            "load_help": "Загрузить метаданные из выбранного файла для редактирования.",
            "btn_save": "Сохранить метаданные"
        },
        "modals": {
            "btn_cancel": "Отмена",
            "preset_title": "Пресет",
            "preset_name": "Название пресета",
            "preset_placeholder": "Введи название пресета...",
            "btn_create": "Создать пресет",
            "delete_title": "Подтверждение удаления",
            "delete_confirm": "Ты точно хочешь удалить",
            "delete_warning": "Это действие нельзя отменить. Файл будет навсегда удалён с диска.",
            "btn_delete": "Удалить",
            "stop_title": "Остановить обучение",
            "stop_confirm": "Ты точно хочешь остановить процесс обучения?",
            "stop_warning": "Весь несохранённый прогресс будет потерян. Текущий чекпоинт не будет сохранён.",
            "btn_stop": "Остановить обучение",
            "update_confirm": "Are you sure you want to apply {count} update(s)?"
        },
        "notifications": {
            "updates_available": "Доступны обновления!",
            "no_updates": "Обновлений не найдено.",
            "update_check_failed": "Ошибка проверки обновлений",
            "connection_error": "Ошибка подключения",
            "update_complete": "Обновление завершено!",
            "update_partial": "Частичное обновление",
            "update_failed": "Обновление не удалось",
            "select_preset_load": "Пожалуйста, выберите пресет для загрузки.",
            "preset_loaded": "Пресет загружен: {name}",
            "preset_load_failed": "Не удалось загрузить пресет",
            "config_saved": "Конфигурация сохранена!",
            "config_save_failed": "Не удалось сохранить конфигурацию.",
            "config_save_error": "Ошибка при сохранении конфигурации: {error}",
            "hardware_optimized": "Оптимизации оборудования применены!",
            "hardware_optimize_failed": "Не удалось получить оптимизации оборудования",
            "auto_adjust_failed": "Автонастройка не удалась",
            "auto_adjust_no_changes": "Автонастройка: изменений не предложено (отсутствует датасет?)",
            "auto_adjust_success": "Автонастройка применена.",
            "auto_adjust_success_count": "Автонастройка применена (датасет: {count} изображений).",
            "select_preset_save": "Пожалуйста, выберите пресет для сохранения.",
            "preset_saved": "Пресет сохранен: {name}",
            "preset_created": "Пресет создан: {name}",
            "preset_save_failed": "Не удалось сохранить пресет.",
            "preset_save_error": "Ошибка при сохранении пресета: {error}",
            "preset_name_empty": "Имя пресета не может быть пустым.",
            "preset_name_invalid": "Имя пресета может содержать только буквы, цифры, подчеркивания и дефисы.",
            "preset_create_failed": "Не удалось создать пресет.",
            "preset_create_error": "Ошибка при создании пресета: {error}",
            "select_preset_delete": "Пожалуйста, выберите пресет для удаления.",
            "preset_delete_failed": "Не удалось удалить пресет.",
            "preset_deleted": "Пресет удален: {name}",
            "preset_delete_error": "Ошибка при удалении пресета: {error}",
            "training_started": "Обучение начато! Job ID: {job_id}",
            "training_start_error": "Ошибка: {error}",
            "training_start_failed": "Ошибка при запуске обучения: {error}",
            "metadata_saved": "Метаданные успешно сохранены!",
            "metadata_load_failed": "Не удалось загрузить метаданные",
            "metadata_save_failed": "Не удалось сохранить метаданные",
            "metadata_save_error": "Ошибка при сохранении метаданных: {error}",
            "select_input_file": "Пожалуйста, выберите входной файл",
            "conversion_success": "Конвертация успешна!",
            "conversion_failed": "Конвертация не удалась: {error}",
            "conversion_error": "Ошибка конвертации: {error}",
            "output_load_failed": "Не удалось загрузить выходные файлы: {error}"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Авто",
            "select_placeholder": "Выберите опцию...",
            "unknown_error": "Неизвестная ошибка",
            "loading": "Загрузка...",
            "starting": "Запуск...",
            "stopped": "Остановлено",
            "converting": "Конвертация...",
            "no_files_found": "Файлы не найдены",
            "select_file": "-- Выберите файл --",
            "select_output_file": "Выберите файл из результатов..."
        }
    }
}