{
    "language_name": "Magyar",
    "ui": {
        "sidebar": {
            "training": "Tréning",
            "console": "Konzol",
            "metadata": "Metaadatok",
            "updates": "Frissítések",
            "system": "Rendszer",
            "connecting": "Kapcsolódás...",
            "language": "Nyelv"
        },
        "updates": {
            "title": "Rendszerfrissítések",
            "subtitle": "Tartsd naprakészen az Onika telepítésed",
            "check_status_default": "Ellenőrizd a frissítéseket, hogy lásd az elérhető változásokat.",
            "btn_check": "Frissítések keresése",
            "btn_apply": "Frissítések alkalmazása",
            "details_title": "Frissítés részletei",
            "col_file": "Fájl útvonala",
            "col_status": "Állapot",
            "log_title": "Frissítési napló",
            "status_uptodate": "A rendszer naprakész.",
            "status_failed": "Nem sikerült ellenőrizni a frissítéseket.",
            "status_error": "Hiba a frissítési szerverhez való kapcsolódáskor.",
            "status_applying": "Frissítések alkalmazása...",
            "status_checking": "Frissítések keresése...",
            "btn_checking": "Ellenőrzés...",
            "found_updates": "{count} elérhető frissítés található.",
            "checked_at": "Ellenőrizve: {date}",
            "updated_count": "{count} fájl frissítve.",
            "failed_count": "{count} fájl frissítése sikertelen.",
            "file_error": "Hiba a(z) {path} fájlnál: {error}",
            "apply_success": "Frissítések sikeresen alkalmazva!",
            "apply_partial": "A frissítések befejeződtek, de néhány hiba történt."
        },
        "training": {
            "title": "Tréning",
            "subtitle": "Állítsd be és indítsd el a LoRA/LyCORIS tréninget",
            "preset": {
                "label": "Előbeállítás",
                "select_placeholder": "Előbeállítás kiválasztása...",
                "btn_load": "Betöltés",
                "btn_save": "Előbeállítás mentése",
                "btn_new": "Új előbeállítás",
                "btn_delete": "Törlés",
                "help": "Betölt egy előbeállítást a <span class=\"mono\">presets/</span> mappából, és alkalmazza az űrlapra. Az előbeállítások gyors módot adnak a profilváltásra. Az Előbeállítás mentése felülírja a kiválasztott előbeállítást. Az Új előbeállítás létrehoz egy újat egyedi névvel. A Törlés eltávolítja a kiválasztott előbeállítást."
            },
            "tabs": {
                "model": "Modell",
                "network": "Hálózat",
                "dataset": "Adatkészlet",
                "text_encoder": "Szövegkódoló",
                "aug": "Augmentáció & gyorsítótárazás",
                "learning": "Tanulás",
                "advanced": "Haladó",
                "samples": "Minták",
                "metadata": "Metaadatok"
            },
            "model": {
                "base_model": "Alapmodell",
                "base_model_help": "A tréninghez használt alapmodell. Ha olyan modellt választasz, ami közel áll a célstílusodhoz, gyorsabban konvergál a tanulás. Ügyelj rá, hogy az architektúra passzoljon a tréning beállításaidhoz (pl. SDXL alap SDXL tréninghez). Egy alapmodell (pl. SDXL 1.0) használata általában stabilabb, mint egy erősen finomhangolt, \"baked\" modell.",
                "architecture": "Modellarchitektúra",
                "architecture_help": "Megadja az architektúra logikát (SDXL, SD1.5, Flux), ami kell a súlyok betöltéséhez és a latensek kezeléséhez. Ha nem egyezik az architektúra az alapmodellel, shape mismatch hibákat fogsz kapni.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-optimalizálás a hardveremhez",
                "optimize_help": "A felismert GPU VRAM alapján automatikusan állítja a pontosságot, a kvantálást és a memória beállításokat.",
                "btn_adjust": "Automatikus beállítás (adatkészlet-függő)",
                "adjust_help": "Elemzi az adatkészletedet (képszám/méretek + captionök), és stabil tréningbeállításokat javasol a kiválasztott architektúrához. Úgy alkalmazza a változtatásokat, mint egy előbeállítás.",
                "quantization": "Modellkvantálás (Q-LoRA)",
                "quantization_none": "Nincs (normál fp16/bf16)",
                "quantization_8bit": "8-bit (alacsony VRAM)",
                "quantization_4bit": "4-bit (extrém alacsony VRAM)",
                "quantization_help": "A modell pontosságát 8-bitre vagy 4-bitre csökkenti, ezzel jelentősen leviszi a VRAM igényt, és lehetővé teszi a tréninget akár 8GB VRAM környékén is. A kisebb pontosság kicsit ronthat a pontosságon, és kell hozzá a `bitsandbytes` könyvtár.",
                "output_name": "Kimenet neve",
                "output_name_placeholder": "pl. karakter_lora_v1",
                "output_name_help": "A mentett LoRA adapterek fájlnevének előtagja. Használj egyedi neveket, hogy rendszerezd a verziókat és elkerüld a felülírást.",
                "output_dir": "Kimeneti mappa",
                "output_dir_help": "A tréning eredményeinek célmappája. Ügyelj rá, hogy legyen elég szabad lemezhely, különben a tréning megszakadhat.",
                "save_precision": "Mentési pontosság",
                "save_precision_help": "A mentett modellfájlok bitmélysége. A `float16` a standard jó kompromisszum méret és pontosság között, a `float32` pedig maximális hűséget ad, cserébe sokkal nagyobb fájlmérettel.",
                "save_format": "Mentési formátum",
                "save_format_help": "A kimenet fájlformátuma. A `safetensors` ajánlott biztonság és gyors betöltés miatt. A `ckpt` egy régi formátum, a `diffusers` pedig mappastruktúrába menti a modellt.",
                "save_epochs": "Mentés minden N. epoch után",
                "save_epochs_help": "Milyen gyakran mentsen checkpointot tréning közben. A rendszeres mentés segít összeomlás után folytatni, és több verziót ad az overfitting ellenőrzéséhez.",
                "save_steps": "Mentés minden N. lépés után",
                "save_steps_placeholder": "Opcionális",
                "save_steps_help": "Alternatív gyakoriság beállítás lépésszám alapján (epochok helyett).",
                "resume": "Folytatás checkpointból",
                "resume_placeholder": "pl. latest vagy path/to/checkpoint-1000",
                "resume_help": "A tréninget egy korábban mentett állapotból folytatja. Figyelj rá: ha folytatáskor alap paramétereket (pl. learning rate vagy rank) megváltoztatsz, az instabil tréninghez vezethet.",
                "save_best": "Csak a legjobb modellek mentése (legalacsonyabb loss)",
                "save_best_help": "Csak a legalacsonyabb rögzített lossú checkpointot tartja meg, hogy spóroljon a lemezhellyel. A legalacsonyabb loss nem mindig jelenti a legjobb vizuális minőséget.",
                "checkpoints_limit": "Checkpointok maximális száma",
                "checkpoints_limit_placeholder": "Opcionális",
                "checkpoints_limit_help": "A megtartandó checkpointok maximális száma. A régebbieket automatikusan törli, hogy kezelje a lemezhasználatot."
            },
            "network": {
                "type": "Hálózat típusa",
                "type_help": "Az adapter architektúrája. A LoRA az ipari standard. A LoHa és LoKr nagyobb kifejezőképességet ad, de gyakran óvatosabb hangolást igényel. Az OFT a hypersphere energiájának megőrzésére készült, és különösen hatékony Flux modelleknél.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonal Finetuning)",
                "module": "Hálózati modul",
                "module_help": "Az adapterhez használt belső Python modul. Ez egy haladó beállítás, általában nem érdemes piszkálni.",
                "dim": "Hálózat dimenzió (rank)",
                "dim_help": "Az adapter kapacitása. A magasabb értékek (pl. 128) részletesebb tanulást engednek, de növelik az overfitting esélyét és nagyobb fájlokat eredményeznek. Az alacsonyabb értékek (pl. 16) jobban általánosítanak és kisebb fájlokat adnak.",
                "alpha": "Hálózat alfa",
                "alpha_help": "Skálázó tényező, ami megakadályozza, hogy a súlyfrissítések túl agresszívek legyenek. Gyakori ökölszabály: stabilitáshoz az Alpha legyen a dim fele, erősebb hatáshoz pedig legyen egyenlő a dimmel.",
                "algo": "LyCORIS algoritmus",
                "algo_help": "LyCORIS algoritmusváltozat (csak LyCORIS/LoHa módoknál releváns). Az algoritmusok az expresszivitás és stabilitás között cserélgetnek. Ha nem vagy biztos benne, kezdd a <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span>-nal, és csak akkor válts, ha mérhető előnyt látsz.",
                "conv_dim": "Conv rank (dim)",
                "conv_dim_help": "Opcionális conv adapter rank LoCon/LyCORIS-hoz. Kis értékek kis konvolúciós kapacitás pluszt adnak; nagy értékek növelik a VRAM igényt és könnyen túlillesztenek textúrára/részletekre. Hagyd üresen, ha nem használsz kifejezetten conv-os módszert.",
                "conv_alpha": "Conv alfa",
                "conv_alpha_help": "Opcionális conv skálázás (alpha). Alacsonyabb alpha finomabb conv hatást ad; magasabb alpha agresszívebb, és instabilitást/overshootot okozhat. Tipikusan <= conv dim; ha túl erősnek érzed a conv hatást, előbb az alphát csökkentsd, ne a dimet.",
                "dora_wd": "DoRA súlycsökkenés",
                "dora_wd_help": "Súlycsökkenés, ami kifejezetten a DoRA magnitude vektorokra vonatkozik.",
                "network_dropout": "Hálózat dropout",
                "network_dropout_help": "Véletlenszerűen eldobja az adapteren belüli neuron kimeneteket, hogy robusztusabb legyen.",
                "rank_dropout": "Rank dropout",
                "rank_dropout_help": "Véletlenszerűen eldob egyes rank dimenziókat regularizációként.",
                "module_dropout": "Modul dropout",
                "module_dropout_help": "Tréning közben véletlenszerűen letilt egész modulokat az overfitting csökkentéséhez és a jobb általánosításhoz.",
                "lora_blocks": "LoRA blokkok",
                "lora_blocks_help": "Lehetővé teszi konkrét blokkok célzását az architektúrán belül (pl. mid blokkok).",
                "lora_layers": "LoRA rétegek",
                "lora_layers_help": "Lehetővé teszi konkrét rétegek célzását (pl. attention rétegek) finomhangoláshoz.",
                "advanced_lora": "Haladó LoRA opciók",
                "lycoris_settings": "LyCORIS beállítások",
                "args": "Hálózat argumentumok",
                "args_help": "Extra argumentumok a hálózati modulhoz, vesszővel elválasztott key=value párok formájában. Használd speciális opciókhoz (pl. dropout, decomposition). A hibás argumentumok instabil tréninghez vezethetnek.",
                "args_placeholder": "key=value, key2=value2",
                "conv_alpha_placeholder": "Opcionális",
                "conv_dim_placeholder": "Opcionális",
                "dora_wd_placeholder": "Opcionális",
                "lora_blocks_placeholder": "Opcionális",
                "lora_layers_placeholder": "Opcionális",
                "module_placeholder": "Opcionális"
            },
            "dataset": {
                "path": "Adatkészlet útvonala",
                "path_help": "Az a mappa, ami a tréningképeket és a hozzájuk tartozó caption fájlokat tartalmazza (pl. .txt). Az adatkészlet minősége a tréning sikerének legfontosabb tényezője; ügyelj rá, hogy a captionök pontosan írják le a képeket.",
                "resolution": "Felbontás",
                "resolution_help": "A tréning célfelbontása. A nagyobb felbontás több részletet ad, de több VRAM kell hozzá. Ügyelj rá, hogy passzoljon a modellarchitektúrához (pl. 1024x1024 SDXL-hez, 512x512 SD1.5-höz).",
                "batch_size": "Batch méret",
                "batch_size_help": "Egyszerre feldolgozott képek száma. A nagyobb batch gyorsíthat és simább gradienseket adhat, viszont jelentősen növeli a VRAM használatot.",
                "max_epochs": "Max epoch",
                "max_epochs_help": "Hány teljes kör menjen végig az adatkészleten. LoRA tréninghez általában 10–20 epoch elég. Túl sok epoch overfittinget és \"szétégett\" képeket okozhat.",
                "max_steps": "Max lépésszám",
                "max_steps_help": "Opcionális kemény limit a teljes tréning lépésszámára.",
                "max_steps_placeholder": "Opcionális",
                "bucketing": "Képarány bucketing engedélyezése",
                "bucketing_help": "Automatikusan képarány szerint csoportosítja a képeket, hogy elkerülje a felesleges vágást és megőrizze az eredeti kompozíciót.",
                "bucket_steps": "Bucket felbontás lépésköze",
                "bucket_steps_help": "A bucket dimenziók rácslépése. A 64 a standard érték a legtöbb architektúrához.",
                "min_bucket": "Minimális bucket felbontás",
                "min_bucket_help": "A képbucket minimálisan engedélyezett felbontása. Megakadályozza, hogy túl kicsi vagy elmosódott képek kerüljenek tréningbe.",
                "max_bucket": "Maximális bucket felbontás",
                "max_bucket_help": "A képbucket maximálisan engedélyezett felbontása, ami segít elkerülni az out-of-memory (OOM) hibákat extrém nagy képeknél.",
                "center_crop": "Középre vágás (okos 1:1)",
                "center_crop_help": "Ha be van kapcsolva, a közel négyzetes képeket (pl. 1210x1280) középről 1:1 arányra vágja. Karaktertréninghez ajánlott a konzisztens framing miatt.",
                "no_upscale": "Nincs felskálázás",
                "no_upscale_help": "Megakadályozza a kis képek felskálázását a bucket dimenziókhoz, így elkerülhetők bizonyos artefaktok.",
                "dreambooth": "DreamBooth & prior megőrzés",
                "prior_preservation": "Prior megőrzés engedélyezése (reg képek)",
                "prior_preservation_help": "Regularizációs képeket használ, hogy a modell ne felejtse el az osztály (pl. \"person\") általános fogalmát, miközben egy konkrét példányt tanul. Ez fontos az általános tudás megőrzéséhez, viszont növeli a tréningidőt.",
                "num_class_images": "Reg képek száma",
                "num_class_images_help": "A cél regularizációs képszám. Gyakori ajánlás: 100 reg kép minden instance képre.",
                "instance_prompt": "Instance prompt",
                "instance_prompt_help": "Egyedi trigger szó + osztályszó kombinációja (pl. \"sks person\"), ami azonosítja a tanított alanyt.",
                "instance_prompt_placeholder": "pl. a photo of sks person",
                "class_prompt": "Class prompt",
                "class_prompt_help": "Az általános osztályszó (pl. \"person\"), amit a regularizációs képekhez használsz.",
                "class_prompt_placeholder": "pl. a photo of a person",
                "reg_dir": "Reg képek mappája",
                "reg_dir_help": "A regularizációs képeket tartalmazó mappa. Csak akkor használatos, ha a prior megőrzés be van kapcsolva.",
                "reg_dir_placeholder": "Opcionális",
                "auto_gen": "Reg képek automatikus generálása",
                "auto_gen_help": "Automatikusan legenerálja a regularizációs képeket, ha még nincsenek meg a megadott mappában.",
                "gen_settings": "Generálási beállítások",
                "neg_class_prompt": "Negatív class prompt",
                "neg_class_prompt_help": "Negatív prompt a regularizációs képek generálásához.",
                "neg_class_prompt_placeholder": "Opcionális negatív prompt",
                "guidance": "Guidance skála",
                "guidance_help": "CFG skála a regularizációs képek generálásához.",
                "steps": "Reg lépések",
                "steps_help": "Mintavételi lépések száma a regularizációs képek generálásához.",
                "scheduler": "Reg scheduler",
                "scheduler_help": "Sampler a regularizációs képek generálásához.",
                "seed": "Reg seed",
                "seed_help": "Seed a regularizációs képek generálásához. -1 = véletlen.",
                "sample_warning": "Tipp: ha a \"Minták generálása N lépésenként\" és a \"Minták generálása N epochonként\" is üresen marad, az Onika nem fog mintaképeket generálni (rengeteg időt spórol). Alacsony és középkategóriás GPU-kon tréning közben általában nem ajánlott gyakran mintákat generálni.",
                "shuffle": "DataLoader keverés",
                "shuffle_help": "Minden epochban összekeveri a képek sorrendjét, hogy a modell ne tanulja meg az adatkészlet sorrendjét.",
                "workers": "DataLoader workerek",
                "workers_help": "A betöltéshez és előfeldolgozáshoz dedikált CPU szálak száma. A magasabb érték gyorsabban etetheti a GPU-t, de ha túl magasra teszed, belassíthatja a rendszert.",
                "persistent_workers": "Perzisztens DataLoader workerek",
                "persistent_workers_help": "Az adatbetöltő CPU workereket aktívan tartja epochok között, így gyorsíthat, viszont több RAM-ot használ."
            },
            "text_encoder": {
                "ti_title": "Textual Inversion (pivotal tuning)",
                "weighting_title": "Caption súlyozás",
                "train": "Szövegkódoló tanítása",
                "train_help": "Engedélyezi a szövegkódoló tanítását a UNet mellett, ami javíthatja a prompt követést, viszont csökkentheti a modell rugalmasságát vagy \"editálhatóságát\".",
                "clip_skip": "Clip skip",
                "clip_skip_help": "A CLIP szövegkódoló legfelső N rétegét kihagyja. SD1.5-höz tipikusan 1-et használnak, SDXL-hez általában 0-t. Rossz érték gyenge prompt értelmezéshez vezethet.",
                "train_ti": "Textual Inversion tanítása",
                "train_ti_help": "Új tokeneket tanít (Textual Inversion), hogy konkrét szavakat vagy fogalmakat adjon a modell szókincséhez.",
                "ti_frac": "TI tanítási arány",
                "ti_frac_help": "Az összes tréning epoch hány százalékában aktív a Textual Inversion tréning.",
                "te_frac": "TE tanítási arány",
                "te_frac_help": "Az összes tréning epoch hány százalékában aktív a szövegkódoló (Text Encoder) tréning.",
                "emphasis": "Alapértelmezett kiemelés",
                "emphasis_help": "Az alapértelmezett szorzó a kiemelt tagekre, amikor nincs megadva külön súly.",
                "de_emphasis": "Alapértelmezett gyengítés",
                "de_emphasis_help": "Az alapértelmezett szorzó a gyengített tagekre, amikor nincs megadva külön súly.",
                "enable_weighted": "Súlyozott captionök engedélyezése",
                "enable_weighted_help": "Engedélyezi a súlyozott szintaxist a captionökben (pl. \"(word:1.1)\"), így pontosan szabályozhatod bizonyos kifejezések fontosságát.",
                "new_tokens": "Új tokenek absztrakciónként",
                "new_tokens_help": "Hány vektort kapjon egy új token. Több vektor több részletet tud elkapni, de nehezebb hatékonyan betanítani.",
                "token_abs": "Token absztrakció",
                "token_abs_help": "A helyettesítő szöveg (pl. \"TOK\"), ami az új fogalmat jelöli a captionökben."
            },
            "aug": {
                "aug_title": "Képaugmentáció",
                "aug_mode": "Augmentáció mód",
                "aug_mode_help": "Eldönti, mikor lépnek életbe az augmentációk tréning közben. <strong>Mindig:</strong> minden képet minden lépésnél augmentál — maximális változatosság, de kicsit lassíthat. <strong>Csak epochonként:</strong> az augmentációk epochonként egyszer sorsolódnak, és azon a körön belül fixek maradnak. Stabilabb, de még mindig keveri a dolgokat epochok között. <strong>Véletlen valószínűség:</strong> minden lépésnél képenként eldől, hogy legyen-e augmentáció, az egyedi beállítások alapján (pl. <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>). A legtöbbször a \"Mindig\" vagy a \"Csak epochonként\" tökéletes.",
                "color_aug": "Szín augmentáció erőssége",
                "color_aug_help": "Mennyire rángassa meg a színeket (árnyalat, telítettség, fényerő) tréning közben. <strong>0.0:</strong> a színek pontosan ugyanazok maradnak — nincs belenyúlás. <strong>0.3-0.5:</strong> finom variáció, a legtöbb adatkészlethez szuper, és segít, hogy a modell ne a konkrét színeket magolja be. <strong>0.7-1.0:</strong> erős színeltolások. Furcsa kombók is kijöhetnek, viszont keményen tolja az általánosítást. Különösen hasznos karakter/stílus LoRÁ-knál, ha azt akarod, hogy a forma és a koncepció üljön, ne ragadjon le egy adott palettán. Kapcsold ki, ha fontos a színhűség (brand színek, konkrét ruhák, stb.).",
                "flip_aug": "Tükrözés esélye",
                "flip_aug_help": "Az esélye annak, hogy a képet vízszintesen tükrözze. <strong>0.5</strong> = 50/50, gyakorlatilag megduplázza az adatkészletedet tükrözött verziókkal. <strong>0.0</strong> = nincs tükrözés. <strong>Figyi:</strong> tedd 0-ra, ha a képeiden van: • <strong>szöveg vagy logó</strong> (visszafelé lenne) • <strong>aszimmetrikus dolog</strong> (autók, arcok jellegzetes oldallal) • <strong>irányfüggő tartalom</strong> (bal vs jobb kéz, specifikus póz). Szimmetrikus témákhoz (középre komponált portré, absztrakt, sok anime karakter) viszont remek. Kicsi adatkészleteknél az egyik legjobb trükk overfitting ellen.",
                "crop_scale": "Véletlen vágás skála",
                "crop_scale_help": "A véletlen vágás minimális zoom szintje. <strong>1.0:</strong> nincs vágás — a képek teljes méretben maradnak. <strong>0.8:</strong> 80–100% között vág, finom zoom variációkat adva. <strong>0.5:</strong> drasztikusabb — akár 2×-es belenagyítást is jelenthet, teljesen eltérő framinggel. A vágás megtanítja a modellt különböző skálákra és kompozíciókra. Szuper, ha a dataseted csupa \"tökéletesen középre komponált headshot\", de rugalmasabb kimenetet szeretnél. Hagyd 1.0-n, ha a pontos framing számít.",
                "caption_aug_title": "Caption augmentáció",
                "shuffle": "Captionök keverése",
                "shuffle_help": "Minden használatkor összekeveri a tagek/szavak sorrendjét a captionökben. Megakadályozza, hogy a modell bemagoljon fix mintákat (pl. \"a kék szem mindig a hosszú haj előtt\"), inkább azt tanulja meg, hogy ezek a tagek bárhol előfordulhatnak. Booru-stílusú, vesszővel tagolt tagekhez tökéletes, ahol a sorrend nem számít. Használd a <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a>-t, ha azt akarod, hogy a trigger szó fixen maradjon. <strong>Ne használd</strong> természetes nyelvű captionökhöz, ahol a szórend jelentést hordoz (\"woman holding cat\" ≠ \"cat holding woman\").",
                "keep_tokens": "Megtartott tokenek",
                "keep_tokens_help": "Hány token legyen \"szent\" a caption elején, amit nem kever. <strong>0:</strong> mindent keverhet. <strong>1:</strong> az első token (általában a trigger szó) mindig első marad. <strong>2-3:</strong> védi a \"trigger, karakter_nev\" mintát. Ha a captionjeid így néznek ki: \"mytrigger, blonde hair, blue eyes, ...\", akkor 1-re állítva a \"mytrigger\" elöl rögzül, a többi pedig keveredik. Segít, hogy a modell a trigger–koncepció kapcsolatot tanulja, ne a konkrét sorrendet. Csak akkor számít, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a> be van kapcsolva.",
                "dropout": "Dropout arány",
                "dropout_help": "Annak valószínűsége, hogy egy kép captionje teljesen eldobásra kerül (mintha üres lenne). Ha a caption eltűnik, a modellnek kizárólag a vizuálisból kell kitalálnia a képet — ez az unconditional generálást tanítja. <strong>0.0:</strong> a caption mindig jelen van. <strong>0.05-0.1:</strong> enyhe dropout, finoman javíthatja az üres/gyenge promptokra adott viselkedést. <strong>0.15-0.2:</strong> agresszívebb — erősen tolja az unconditional minőséget. A dropout segíti a CFG-t inference-ben, mert a modell tényleg tudja, milyen a \"nincs prompt\" állapot. LoRA tréningnél, ahol a prompt követés a király, túl sok dropout visszaüthet. Egy pici (0.05) gyakran jó; nagyon rövid tréningnél akár hagyd is ki.",
                "dropout_epochs": "Dropout minden N. epochban",
                "dropout_epochs_help": "Nem lépésenként véletlenszerűen dob el, hanem konkrét epochokban. <strong>0:</strong> visszaáll a normál, véletlen <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a> működésre. <strong>1:</strong> minden epoch caption nélküli (elég durva). <strong>5:</strong> az 5., 10., 15... epochok mennek caption nélkül. Strukturáltabb minta: többnyire normál tréning, időnként \"vak\" epochokkal. Speciális tanítási menetrendekhez hasznos, de a véletlen lépésenkénti dropout a gyakoribb.",
                "noise_title": "Zaj beállítások",
                "noise_type": "Zaj offset típusa",
                "noise_type_help": "A zaj offset számításának matematikai módszere. Az \"Original\" a standard implementáció.",
                "noise_offset": "Zaj offset erőssége",
                "noise_offset_help": "Offsetet ad a zajhoz tréning közben, ami segít jobb dinamikaátfogást elérni (mélyebb feketék, világosabb fehérek). Általában 0.05–0.1 ajánlott.",
                "adaptive_noise": "Adaptív zaj skála",
                "adaptive_noise_help": "A zajt a modell predikciós hibája alapján skálázza tréning közben.",
                "noise_random": "Zaj offset véletlen erőssége",
                "noise_random_help": "Minden tréning lépésnél véletlenszerűvé teszi a zaj offset erősségét.",
                "multires_iter": "Multires zaj iteráció",
                "multires_iter_help": "Multi-resolution zajt alkalmaz, hogy javítsa a finom textúrák és részletek tanulását.",
                "multires_discount": "Multires zaj diszkont",
                "multires_discount_help": "A multi-resolution zaj iterációkra alkalmazott diszkont tényező.",
                "edm": "EDM-stílusú tréning (SDXL)",
                "edm_help": "Az EDM (Elucidating the Design Space of Diffusion-Based Generative Models) formulációt használja, ami SDXL-nél jobb minőségű kimenetet adhat.",
                "caching_title": "Gyorsítótárazás",
                "cache_latents": "Latensek cache-elése RAM-ba",
                "cache_latents_help": "Előre kiszámolja a VAE latenseket, amivel jelentősen gyorsíthatja a tréninget (gyakran 2×–3×). Bekapcsolva minden latent RAM-ban marad tréning alatt. Ez letilt bizonyos valós idejű augmentációkat.",
                "cache_te": "Szöveg embeddingek cache-elése",
                "cache_te_help": "Előre kiszámolja a szöveg embeddingeket a gyorsabb tréninghez. Ez a beállítás nem kompatibilis az aktív szövegkódoló tréninggel.",
                "cache_disk": "Latensek cache-elése lemezre",
                "cache_disk_help": "Előre kiszámolt latenseket ment lemezre RAM spórolásért. Lassú lemez I/O ronthatja a teljesítményt. Ezeket a latenseket későbbi tréningeknél újra felhasználja. Ha változik az adatkészlet, manuálisan törölnöd kell a régi latenseket a cache-ből.",
                "vae_batch": "VAE batch méret",
                "vae_batch_help": "A VAE enkódolás batch mérete cache-eléskor. A magasabb érték gyorsít, de több VRAM kell hozzá.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Optimalizáló",
                "optimizer_help": "Az algoritmus, ami a gradiensek alapján ténylegesen frissíti a súlyokat. Különböző optimalizálók különböző erősségekkel bírnak. <strong>AdamW8bit:</strong> a klasszikus 8-bites, memóriatakarékos verziója, kb. ~50%-kal kevesebb optimizer state memóriát használ. Szűk VRAM mellett a legtöbb LoRA tréningnél ez a go-to. Kell hozzá a bitsandbytes. <strong>AdamW:</strong> az eredeti, full precision változat. A legstabilabb és leginkább kipróbált. Válaszd ezt, ha van VRAM tartalékod, vagy gond van a 8-bites verzióval. <strong>Lion / Lion8bit:</strong> újabb, sokszor jó, de gyakran jóval kisebb LR kell (3–10× kisebb, mint AdamW-nál). <strong>DAdaptAdam:</strong> önbeállító optimizer. Állítsd a <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a>-et 1.0-ra és hagyd, hogy megtalálja az optimális értéket. <strong>Prodigy:</strong> egy másik okos, adaptív optimizer — itt is LR=1.0-t használj. Gyakran kiváló eredményt ad beállítás nélkül. <strong>CAME:</strong> kísérleti, korrekt memóriahatékonyság. <strong>Adafactor:</strong> brutál memóriabarát, eredetileg óriás nyelvi modellekhez készült — végső megoldás extrém VRAM szűkében. <strong>SGD:</strong> alap gradienscsökkenés; ritkán használják diffúziós tréningnél, de ha kísérleteznél, itt van. Finomhangold a viselkedést az <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer_args\" data-xref-label=\"Optimizer Args\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer Args</a>-szal.",
                "optimizer_args": "Optimizer argumentumok",
                "optimizer_args_help": "Extra finomhangolások, amik közvetlenül az <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>-nek mennek. Formátum: <code>key=value</code>, vesszővel elválasztva. Példák: <strong>weight_decay=0.01</strong> — L2 regularizáció overfitting ellen (gyakori AdamW-nál). <strong>betas=(0.9,0.999)</strong> — Adam momentum együtthatók. <strong>d_coef=1.0</strong> — D-koefficiens Prodigy/DAdaptAdam-hoz. <strong>eps=1e-8</strong> — numerikus stabilitáshoz. Az alapértékek a legtöbb tréninghez jók. Csak akkor nyúlj hozzá, ha követsz egy konkrét guide-ot vagy tényleg tudod, mit csinálsz.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning rate",
                "lr_help": "A tréning alap learning rate-je.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Felülírhatod a tanulási rátát csak az UNet-re — ez a kép-generálás fő motorja. Hagyd üresen, ha a globális <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">tanulási ráta</a> beállítást akarod használni. Ha külön rátát adsz az UNet-nek és a Text Encodernek, pontosan kézben tarthatod, mi tanuljon gyorsabban. Az UNet felel a vizuálért (stílus, kompozíció, szerkezet), szóval ez közvetlenül befolyásolja, milyen gyorsan „sül be” a tréningelt témád kinézete. Őszintén: a két résznek ugyanaz a ráta általában teljesen oké — csak akkor bontsd szét, ha tudod, mit csinálsz.",
                "unet_lr_placeholder": "Opcionális",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Tanulási ráta kifejezetten a Text Encoderhez (CLIP vagy T5, az alapmodelltől függően). Ez a komponens fordítja le a szöveges promptjaidat arra, amit a kép-generátor ért. Általános ökölszabály: az UNet rátájának kb. <strong>1/10–1/2</strong>-e, vagy akár teljesen kapcsold ki (0). <strong>Ha tréningeled:</strong> segít új fogalmakat/neveket felismerni, és jobban reagálni a stílus-specifikus promptjaidra. <strong>Ha kihagyod (0):</strong> érintetlenül hagyja az alapmodell nyelvi tudását — jó, ha csak a vizuális stílus érdekel. LoRA-nál fél <a href=\"#\" class=\"xref-link\" data-xref=\"unet_lr\" data-xref-label=\"UNet LR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet LR</a> gyakran stabil kiindulópont.",
                "te_lr_placeholder": "Opcionális",
                "scheduler": "LR ütemező",
                "scheduler_help": "Az a stratégia, ami tréning közben állítja a tanulási rátát.",
                "warmup": "Warmup lépések",
                "warmup_help": "Hány lépésen keresztül emelkedjen finoman a tanulási ráta nulláról a célértékig. Olyan, mint edzés előtt a bemelegítés — nem sokkolja a modellt hirtelen nagy gradiensekkel, amikor a súlyok még messze vannak az optimálistól. <strong>0</strong> = nincs warmup, azonnal teljes LR. <strong>10–100 lépés</strong> LoRA-hoz általában bőven elég. <strong>100–500 lépés</strong> segíthet teljes finetune-nál vagy nagy batch-eknél. Megadhatod arányként is a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> mezővel. Különösen hasznos magas LR-nél vagy adaptív optimalizereknél, pl. <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy vagy DAdaptAdam</a>.",
                "warmup_ratio": "Warmup arány",
                "warmup_ratio_help": "Alternatíva a <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup lépések</a> helyett — a warmupot a teljes tréning arányában adod meg, nem konkrét lépésszámmal. <strong>0.0</strong> = nincs warmup. <strong>0.05</strong> = az első 5%-ban warmup. <strong>0.1</strong> = az első 10%-ban warmup. Sokkal kényelmesebb, mert automatikusan skálázódik a tréning hosszával. Ha mindkettőt beállítod, ez nyer.",
                "cycles": "LR ciklusok",
                "cycles_help": "Hány teljes fel-le ciklust fusson, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a> ütemezőt használod. Minden ciklus leviszi az LR-t minimumra, majd visszarúgja (általában kisebb csúcsra, mint előtte). <strong>1</strong> = egy ciklus az egész tréningen, kb. ugyanaz, mint a sima cosine. <strong>2–4</strong> ciklus segíthet kimozdítani a modellt lokális minimumokból és jobban feltérképezni a loss tájat. Több ciklus = több „restart” = jobb felfedezés, de a végén kevésbé stabil lehet. Csak akkor számít, ha cosine_with_restarts van.",
                "scheduler_power": "Ütemező hatvány",
                "scheduler_power_help": "A kitevő, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial ütemezőt</a> használod. Meghatározza, mennyire agresszíven essen az LR. <strong>1.0</strong> = lineáris lecsengés. <strong>> 1.0</strong> (pl. 2.0) = gyors esés az elején, lassul később. <strong>< 1.0</strong> (pl. 0.5) = lassú esés az elején, gyorsabb a végén. Csak polynomialnál számít. Az 1.0 alapból a legtöbb esetben jó.",
                "ema_unet": "EMA az UNet-hez",
                "ema_unet_help": "Futó átlagot tart az UNet súlyaiból tréning közben. Ahelyett, hogy az utolsó lépés „nyers” súlyait használnád (ami zajos lehet), az EMA egy kisimított verziót ad, ami általában szebb és megbízhatóbban generalizál. Gondolj rá úgy, mint zajszűrés a súlyokon — az EMA checkpoint sokszor jobb a nyersnél. A bökkenő: kb. duplázza az UNet súlyok VRAM-igényét (mindkettőt tárolod). A simítás mértékét a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a> vezérli. Hosszabb tréningeknél erősen ajánlott, ha a VRAM-od bírja.",
                "ema_te": "EMA a Text Encoderhez",
                "ema_te_help": "Ugyanaz az ötlet, mint a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, csak a text encoderre. Kevésbé kritikus, mert a text encoder súlyai LoRA tréningnél általában finomabban mozdulnak. Ettől még extra VRAM kell az extra súlykészlethez. Kapcsold be, ha tényleg tréningelsz text encodert és a lehető legjobb minőséget akarod.",
                "ema_decay": "EMA lecsengés",
                "ema_decay_help": "Mennyire „emlékezzen” az EMA a régi súlyokra az újakkal szemben: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Magas (0.999–0.9999):</strong> több simítás, lassabban reagál a változásokra. Jó hosszú tréningekhez. <strong>Alacsony (0.99–0.995):</strong> kevesebb simítás, gyorsabban követi az új súlyokat. Rövid futásokhoz jobb. A <strong>0.995</strong> tipikus LoRA tréninghez (száz–pár ezer lépés) gyakran jól működik. Tízezres lépésszám? Próbáld a <strong>0.9999</strong>-et. Csak akkor számít, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA az UNet-hez</a> vagy az <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA a Text Encoderhez</a> be van kapcsolva."
            },
            "advanced": {
                "attention": "Attention backend",
                "attention_help": "Melyik implementáció kezelje az attention mechanizmust — ez a transformerek leginkább memóriaéhes része. <strong>SDPA:</strong> PyTorch beépített, optimalizált attentionje (PyTorch 2.0+ kell). Jó balansz sebesség és memória között, extra telepítés nélkül. Ajánlott alap. <strong>xFormers:</strong> a Meta attention könyvtára, NVIDIA kártyákon gyakran a leggyorsabb. A „sima” attentionhöz képest 20–40%-kal is vághat a VRAM-on. Külön telepítés kell hozzá. <strong>None:</strong> sima PyTorch attention — többet eszik, de mindenhol működik. Csak akkor válts erre, ha az SDPA vagy xFormers problémázik. Az SDPA és az xFormers is jól együttműködik az <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Agresszív memóriatakarékossággal</a>.",
                "grad_acc": "Gradiens felhalmozás",
                "grad_acc_help": "Több passzon keresztül halmozza a gradienseket, mielőtt frissítené a súlyokat — gyakorlatilag nagyobb batch-et „szimulál”. A <strong>hatékony batch = batch_size × gradient_accumulation</strong>. Tehát batch_size=2 és accumulation=4 olyan, mintha batch_size=8 lenne. Ez életmentő VRAM-szűk gépeken: nem fér be a batch=8? Csináld batch=2-vel és accumulation=4-gyel. A nagyobb effektív batch stabilabb tréninget ad, de lehet, hogy állítanod kell a <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">tanulási rátán</a>. Hátrány: arányosan lassít (4× accumulation = kb. 4× lassabb effektív lépésenként).",
                "mixed_precision": "Vegyes precizitás",
                "mixed_precision_help": "Számítási precizitás — közvetlenül hat a VRAM-ra és a sebességre. <strong>fp16:</strong> félprecíziós float, kb. fele memória a teljes precízióhoz képest. Jó választás GTX 10-es és RTX 20-as szérián, de SDXL-nél néha NaN-okat dobhat az fp16 korlátozott tartománya miatt. <strong>bf16:</strong> szintén 16 bit, de jobb kitevő-tartománnyal — sokkal stabilabb. RTX 30-as és újabb kártyán ezt érdemes használni. Régebbi hardveren nem megy. <strong>fp8:</strong> 8-bit, brutál VRAM spórolás. Kísérleti, ronthat minőséget, speciális hardver kell. <strong>no:</strong> teljes fp32. 2× VRAM, de numerikusan betonstabil. Csak NaN hibák debugolásához vagy ha van VRAM-od bőven.",
                "tf32": "TF32 engedélyezése",
                "tf32_help": "TensorFloat-32 — számítási mód, ami csak RTX 30-as és újabb kártyákon van. fp32 tartományt ad, de kevesebb mantissza pontossággal (10 bit 23 helyett), így egyes műveleteknél akár 8× gyorsulást hozhat minimális minőségkülönbséggel. <strong>RTX 30/40 szérián hagyd bekapcsolva.</strong> Régebbi GPU-n (GTX 10, RTX 20) nem csinál semmit, mert nincs hozzá hardver. Csak akkor kapcsold ki, ha valamiért tényleg gyanús, hogy ront (nagyon ritka).",
                "vae_batch": "VAE batch méret",
                "vae_batch_help": "Külön batch méret csak a VAE kódoláshoz — amikor a képek latent térbe kerülnek, mielőtt az UNet látná őket. Ha ezt kisebbre állítod, mint a fő batch-ed, le tudja simítani a VRAM tüskéket kódolás közben. Hagyd üresen, ha egyezzen a tréning batch-csel. <strong>1</strong> = minimális VRAM, cserébe lassabb kódolás. Hasznos, ha kifejezetten a latent kódolási fázisban OOM-olsz.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max tokenhossz",
                "max_token_length_help": "A captionökben engedett maximum token (kb. szavak/al-szavak). A CLIP natív kontextusa <strong>77 token</strong> (75 használható + start/end). <strong>75:</strong> standard, a legtöbb captionnel oké. <strong>150:</strong> több hely részletes leírásra — jó komplex jelenetekhez vagy részletes karakterleíráshoz. <strong>225:</strong> nagyon hosszú, extra részletes captionök. Jelentősen több VRAM-ot eszik és lassít. A magasabb limitek caption chunking trükkökkel működnek. Emeld <strong>150</strong>-re vagy <strong>225</strong>-re, ha a captionjeid gyakran meghaladják a ~60–70 szót és a levágás fontos infót dob ki.",
                "memory_saving": "Agresszív memóriatakarékosság",
                "memory_saving_help": "Mindent bevet VRAM spórolásra: gradiens checkpointing, CPU offload, agresszív garbage collection. <strong>4–8GB-os kártyákon kötelező</strong> — olyan hardveren is lehetővé teheti az SDXL tréninget, ami amúgy megfulladna. Modelltől függően 30–50% VRAM-ot is spórolhat. Az ára: a tréning 2–4× lassabb lehet az újraszámolás és a CPU↔GPU pakolgatás miatt. Érdemes egy jó <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention backenddel</a> (SDPA vagy xFormers) párosítani. 16GB+ VRAM? Hagyd kikapcsolva és élvezd a gyorsabb tréninget.",
                "seed": "Tréning seed",
                "seed_help": "A tréningben használt véletlen generátorok seedje. Ha fix seedet adsz meg, kapsz <strong>reprodukálhatóságot</strong>: ugyanaz a config + ugyanaz a seed = azonos eredmény (azonos hardver/szoftver mellett). Hagyd üresen a random induláshoz — mindennapi tréninghez teljesen jó. Hasznos A/B tesztekhez, bugvadászathoz, vagy ha megosztható, ismételhető configot akarsz. Sokan emlékezetes számokat választanak, pl. <strong>42</strong>, <strong>1234</strong> vagy <strong>0</strong>.",
                "seed_placeholder": "Véletlen"
            },
            "flux": {
                "title": "Flux-specifikus beállítások",
                "max_seq_len": "Max szekvenciahossz",
                "max_seq_len_help": "Max tokenhossz a T5 text encoderhez (Flux / SD3). Magasabb értéknél hosszabb prompt/caption fér bele, de nő a VRAM igény és lassul a tréning. Gyakori értékek: <strong>256</strong> (alap) vagy <strong>512</strong>.",
                "guidance_scale": "Guidance skála",
                "guidance_scale_help": "Classifier-free guidance Flux és SD3 tréninghez. Azt szabályozza, mennyire ragaszkodjon a modell a szöveghez tréning közben. <strong>3.5:</strong> a Stability AI ajánlott alapja SD3-hoz, Fluxhoz is nagyon jó. <strong>Alacsony (1–2):</strong> kreatívabb/változatosabb, de könnyebben elkalandozik a prompttól. <strong>Magas (5–7):</strong> szorosabb prompt-követés, de kicsit merev lehet. SD1.5/SDXL-nél ezt általában nem használják — ott a guidance jellemzően csak inference-nél játszik.",
                "weighting_scheme": "Súlyozási séma",
                "weighting_scheme_help": "Hogyan mintavételezze a timestep-eket flow-matching modelleknél (Flux, SD3). Ez dönti el, mely zajszintek kapjanak több figyelmet tréning közben. <strong>None (Uniform):</strong> minden zajszint egyenlő. Biztonságos alap, ha nincs külön igényed. <strong>Logit Normal:</strong> haranggörbe eloszlás, középpontja <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a>, szélessége <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Pontosan belőheted, mely timestep-ek számítsanak. <strong>Mode:</strong> erősen egy konkrét timestep köré fókuszál, a szórást a <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a> vezérli. Ezek csak Flux/SD3 esetén számítanak — SD1.5/SDXL-nél (epsilon-pred modellek) nem csinálnak semmit.",
                "logit_mean": "Logit mean",
                "logit_mean_help": "Hol tetőzzön a logit-normal eloszlás, ha <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> súlyozást használsz. <strong>0.0:</strong> a középső timestep köré (t=0.5) kerül a fókusz. <strong>Negatív:</strong> zajosabb timestep-ek felé tol (durva szerkezet). <strong>Pozitív:</strong> tisztább timestep-ek felé tol (finom részletek). Az alapján hangold, a generálás mely szakasza fontos neked. Csak akkor számít, ha Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit std",
                "logit_std_help": "Mennyire legyen szétterülve a logit-normal eloszlás <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> súlyozásnál. <strong>Alacsony (0.5–0.8):</strong> szűk fókusz a <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> körül — tényleg rákattan pár timestep-re. <strong>Magas (1.5–2.0):</strong> jobban szór, közelít az egyenletes mintavételezéshez. <strong>1.0:</strong> kiegyensúlyozott alap. Csak akkor számít, ha Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode scale",
                "mode_scale_help": "Mennyire szorosan csoportosuljon a mode eloszlás, ha <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode</a> súlyozást használsz. <strong>Alacsony:</strong> több timestep között szór. <strong>Magas:</strong> jobban összecsomósodik a módusz körül. <strong>1.29:</strong> az eredeti SD3 paper értéke. Maradhat, hacsak nem kísérletezel. Csak akkor releváns, ha Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep mintavételezés",
                "min": "Min timestep",
                "min_help": "A mintavételezés minimális timestepje.",
                "max": "Max timestep",
                "max_help": "A mintavételezés maximális timestepje.",
                "max_placeholder": "Opcionális",
                "min_placeholder": "Opcionális"
            },
            "caption": {
                "title": "Caption beállítások",
                "extension": "Caption kiterjesztés",
                "extension_help": "A caption fájlok kiterjesztése. Alapból <strong>.txt</strong> — tehát az \"image001.png\" a captionjét az \"image001.txt\" fájlban keresi. Bármilyen kiterjesztést használhatsz (.caption, .captions, .tags, stb.). A caption fájlnak ugyanabban a mappában kell lennie, mint a képnek. Állítsd át, ha a captionelő eszközöd mást ír ki.",
                "weighted": "Súlyozott captionök",
                "weighted_help": "A1111-stílusú súlyokat olvas a captionökből, pl. \"(fontos dolog:1.3)\" vagy \"[kevésbé fontos:0.7]\". Így tréning közben kiemelhetsz vagy visszavehetsz bizonyos szavakat. <strong>Súly > 1.0:</strong> a modell jobban figyel azokra a tokenekre. <strong>Súly < 1.0:</strong> kevésbé foglalkozik velük. Remek arra, hogy finomhangold, mi számítson a captionjeidben. Kapcsold ki, ha a captionjeidben a zárójelek szó szerintiek és nem súlyozást jelentenek."
            },
            "loss": {
                "title": "Loss & optimalizáció",
                "optimization_title": "Loss optimalizáció",
                "type": "Loss típusa",
                "type_help": "Az a matematikai függvény, amivel kiszámolja a különbséget a prediktált és a cél zaj között.",
                "huber_c": "Huber C",
                "huber_c_help": "Küszöb paraméter a Huber lossnál. Alacsonyabb értéknél jobban hasonlít L1-re.",
                "min_snr": "Min SNR gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. A zajszintek alapján súlyozza a losst, ezzel stabilizálhatja a tréninget.",
                "ip_noise": "IP noise gamma",
                "ip_noise_help": "Az Input Perturbation noise erőssége. Segíthet a túlsimítás ellen.",
                "noise_offset": "Noise offset",
                "noise_offset_help": "Tréning közben egy apró, konstans eltolást ad a zajhoz, hogy a modell ténylegesen tudjon igazi feketét és igazi fehéret generálni a kimosott szürkék helyett. A standard diffúzió szenved a nagyon sötét/világos tartományokkal, mert a zajütemezés nem fedi jól. <strong>0.035–0.05:</strong> biztonságos alapok, javítják a sötét/világos kezelést mellékhatás nélkül. <strong>0.05–0.1:</strong> agresszívebb tolás a szélsőségek felé. <strong>0.1 felett:</strong> instabilizálhat, vagy színeltolódást okozhat. A legjobban a <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a>-rel együtt működik — kiegészítik egymást. Kifejezetten fontos, ha a datasetedben sok a sötét jelenet, éjszakai kép vagy nagy kontraszt.",
                "v_pred": "V-Pred jellegű loss",
                "v_pred_help": "Sebesség-predikciós (velocity) viselkedést kever a standard zaj-predikciós tréningbe. Eredetileg v-pred modellekből jön (SD 2.x depth/inpaint) — kisimíthatja a tréninget és csökkentheti az artifactokat. <strong>0:</strong> tiszta zaj-pred, a standard. <strong>1:</strong> teljes velocity pred mód. <strong>0.1–0.2:</strong> finom v-pred hatás úgy, hogy kompatibilis marad zaj-pred alapmodellekkel. Néha segít simább gradienssel és kevesebb artifacttal. Hagyd <strong>0</strong>-n, ha amúgy jól működik, vagy ha nem vagy benne biztos.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Haladó loss opciók",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> úgy állítja a noise schedulert, hogy az utolsó timestep tiszta zaj legyen (SNR = 0), ami segít a valódi feketéknél. Fontos társa a <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a>-nek — Auto módban automatikusan bekapcsol. Nélküle a noise offset önmagában nem feltétlen javítja teljesen a sötét képes problémákat. A \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" paperből.<br><strong>Debiased Estimation:</strong> javítja a timestep mintavételezés torzítását, amikor egyesek túl vannak mintavételezve. Újrasúlyozza a losst, hogy minden timestep egyenletesen tanuljon, így összességében jobb minőséget ad. A <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> mellett is működik, de más problémát céloz.<br><strong>Scale V-Pred Loss:</strong> normalizálja a v-prediction losst, hogy hasonló nagyságrendű legyen, mint a zaj-pred loss. Főleg v-pred alapmodelleknél (SD 2.x variánsok) vagy <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a> használatakor hasznos. Standard SD 1.5/SDXL-nél (epsilon-pred) keveset számít.<br><strong>Masked Loss (Alpha):</strong> PNG alpha csatornát használ tréning maszknak — a modell csak a nem átlátszó pixelekből tanul. Tökéletes karakter/tárgy tréninghez, ha ignorálni akarod a hátteret. <strong>PNG kell rendes alpha csatornával!</strong>",
                "presets": "Loss presetek",
                "presets_help": "Gyorsan alkalmazhatsz ajánlott loss beállításokat különböző tréning célokhoz.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light"
            },
            "samples": {
                "prompts_title": "Minta promptok",
                "inference_title": "Inference beállítások",
                "schedule_title": "Generálási ütemezés",
                "generated_title": "Generált minták",
                "gallery_placeholder": "A generált minták tréning közben itt fognak megjelenni...",
                "prompts": "Minta promptok",
                "prompts_help": "Adj meg promptokat mintaképek generálásához tréning közben. Soronként egy prompt.",
                "neg_prompt": "Negatív prompt",
                "neg_prompt_help": "A mintageneráláshoz használt negatív prompt.",
                "every_n_epochs": "Minden N epoch",
                "every_n_epochs_help": "Generáljon mintákat minden N. epoch után.",
                "every_n_steps": "Minden N lépés",
                "every_n_steps_help": "Generáljon mintákat minden N. lépés után.",
                "sampler": "Sampler",
                "sampler_help": "A mintaképek generálásához használt sampler.",
                "inference_steps": "Inference lépések",
                "inference_steps_help": "Denoise lépések száma mintagenerálásnál.",
                "cfg_scale": "CFG skála",
                "cfg_scale_help": "Classifier Free Guidance skála mintageneráláshoz.",
                "num_images": "Képek promptonként",
                "num_images_help": "Hány képet generáljon promptonként.",
                "seed": "Seed",
                "seed_help": "Seed reprodukálható mintageneráláshoz. -1 = véletlen.",
                "every_n_epochs_placeholder": "Opcionális",
                "every_n_steps_placeholder": "Opcionális",
                "neg_prompt_placeholder": "Opcionális negatív prompt",
                "prompts_placeholder": "Soronként egy prompt..."
            },
            "metadata": {
                "title": "Modell metaadatok (metadata)",
                "title_help": "A LoRA nyilvános neve (title).",
                "author": "Szerző (author)",
                "author_help": "A neved.",
                "desc": "Leírás (description)",
                "desc_help": "Nyilvános leírás.",
                "license": "Licenc (license)",
                "license_help": "Felhasználási licenc.",
                "tags": "Címkék (tags)",
                "tags_help": "Keresési címkék / tag-ek.",
                "comment": "Tréning megjegyzés (comment)",
                "comment_help": "Privát megjegyzések, metaadatba mentve.",
                "comment_placeholder": "pl. 50 kép alapján tanítva X karakterről",
                "desc_placeholder": "Írd le a LoRA-dat...",
                "license_placeholder": "pl. CreativeML Open RAIL-M",
                "tags_placeholder": "pl. karakter, stílus, anime"
            },
            "progress": {
                "epoch_prefix": "Epoch",
                "loss_label": "LOSS",
                "start": "Tréning indítása",
                "stop": "Tréning leállítása",
                "idle": "Rendszer tétlen",
                "caching_latents": "Latensek gyorsítótárazása",
                "latents_cache": "Latens gyorsítótár"
            },
            "conversion": {
                "title": "Modell konvertálás (conversion)",
                "subtitle": "LoRA modellek konvertálása formátumok között (Diffusers &harr; Kohya/LDM)",
                "card_title": "Konvertáló eszköz",
                "help_text": "Ezzel az eszközzel kompatibilitási gondokat javíthatsz AUTOMATIC1111/Forge alatt. \"Diffusers\" stílusú kulcsokat alakít \"Kohya/LDM\" stílusú kulcsokra és vissza.",
                "input_model": "Bemeneti modell (.safetensors)",
                "btn_refresh": "Frissítés",
                "model_architecture": "Modell architektúra",
                "architecture_help": "Megadja azt az architektúrát (architecture), ami a helyes key mappinghez kell. Ha rosszat választasz, a kimeneti modell nem fog működni.",
                "target_format": "Cél formátum",
                "target_format_kohya": "Kohya / LDM (A1111, Forge, ComfyUI)",
                "output_filename": "Kimeneti fájlnév (opcionális)",
                "output_filename_placeholder": "Hagyd üresen az automatikus névhez (pl. model_converted.safetensors)",
                "btn_convert": "Modell konvertálása",
                "success": "Konvertálás sikeres!"
            }
        },
        "console": {
            "title": "Rendszerkonzol (console)",
            "subtitle": "Valós idejű tréning logok és rendszerkimenet",
            "output_title": "Konzol kimenet",
            "clear": "Konzol törlése",
            "status": "Állapot",
            "step": "Lépés",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU terhelés (GPU load)",
            "cpu_load": "CPU terhelés (CPU load)",
            "ram": "RAM",
            "waiting": "Várakozás a tréning indítására..."
        },
        "metadata_editor": {
            "title": "Metaadat szerkesztő (metadata editor)",
            "subtitle": "Beágyazott metadata kezelése LoRA és Checkpoint fájlokhoz",
            "select_file": "Fájl kiválasztása",
            "select_placeholder": "Válassz egy fájlt...",
            "select_help": "Válassz egy modellt vagy metadata fájlt, hogy megnézd és szerkeszd a belső metaadatait.",
            "btn_load": "Metaadat betöltése",
            "load_help": "Metaadat betöltése a kiválasztott fájlból szerkesztéshez.",
            "btn_save": "Metaadat mentése"
        },
        "modals": {
            "btn_cancel": "Mégse",
            "preset_title": "Preset",
            "preset_name": "Preset neve",
            "preset_placeholder": "Add meg a preset nevét...",
            "btn_create": "Preset létrehozása",
            "delete_title": "Törlés megerősítése",
            "delete_confirm": "Biztosan törlöd:",
            "delete_warning": "Ez a művelet nem vonható vissza. A fájl véglegesen törlődik a lemezről.",
            "btn_delete": "Törlés",
            "stop_title": "Tréning leállítása",
            "stop_confirm": "Biztosan leállítod a tréninget?",
            "stop_warning": "Minden el nem mentett haladás elveszik. Az aktuális checkpoint nem lesz elmentve.",
            "btn_stop": "Leállítás",
            "update_confirm": "Biztosan alkalmazni szeretnél {count} frissítést?"
        },
        "notifications": {
            "updates_available": "Frissítések érhetők el!",
            "no_updates": "Nem találhatók frissítések.",
            "update_check_failed": "Frissítés ellenőrzése sikertelen",
            "connection_error": "Kapcsolódási hiba",
            "update_complete": "Frissítés kész!",
            "update_partial": "Részleges frissítés",
            "update_failed": "Frissítés sikertelen",
            "select_preset_load": "Kérlek, válassz ki egy előbeállítást a betöltéshez.",
            "preset_loaded": "Előbeállítás betöltve: {name}",
            "preset_load_failed": "Nem sikerült betölteni az előbeállítást",
            "config_saved": "Konfiguráció mentve!",
            "config_save_failed": "Nem sikerült menteni a konfigurációt.",
            "config_save_error": "Hiba a konfiguráció mentésekor: {error}",
            "hardware_optimized": "Hardver optimalizációk alkalmazva!",
            "hardware_optimize_failed": "Nem sikerült lekérni a hardver optimalizációkat",
            "auto_adjust_failed": "Automatikus beállítás sikertelen",
            "auto_adjust_no_changes": "Automatikus beállítás: nincs javasolt változtatás (hiányzó adatkészlet?)",
            "auto_adjust_success": "Automatikus beállítás alkalmazva.",
            "auto_adjust_success_count": "Automatikus beállítás alkalmazva (adatkészlet: {count} kép).",
            "select_preset_save": "Kérlek, válassz ki egy előbeállítást a mentéshez.",
            "preset_saved": "Előbeállítás mentve: {name}",
            "preset_created": "Előbeállítás létrehozva: {name}",
            "preset_save_failed": "Nem sikerült menteni az előbeállítást.",
            "preset_save_error": "Hiba az előbeállítás mentésekor: {error}",
            "preset_name_empty": "Az előbeállítás neve nem lehet üres.",
            "preset_name_invalid": "Az előbeállítás neve csak betűket, számokat, aláhúzást és kötőjelet tartalmazhat.",
            "preset_create_failed": "Nem sikerült létrehozni az előbeállítást.",
            "preset_create_error": "Hiba az előbeállítás létrehozásakor: {error}",
            "select_preset_delete": "Kérlek, válassz ki egy előbeállítást a törléshez.",
            "preset_delete_failed": "Nem sikerült törölni az előbeállítást.",
            "preset_deleted": "Előbeállítás törölve: {name}",
            "preset_delete_error": "Hiba az előbeállítás törlésekor: {error}",
            "training_started": "Tréning elindítva! Job ID: {job_id}",
            "training_start_error": "Hiba: {error}",
            "training_start_failed": "Hiba a tréning indításakor: {error}",
            "metadata_saved": "Metaadatok sikeresen mentve!",
            "metadata_load_failed": "Nem sikerült betölteni a metaadatokat",
            "metadata_save_failed": "Nem sikerült menteni a metaadatokat",
            "metadata_save_error": "Hiba a metaadatok mentésekor: {error}",
            "select_input_file": "Kérlek, válassz ki egy bemeneti fájlt",
            "conversion_success": "Konvertálás sikeres!",
            "conversion_failed": "Konvertálás sikertelen: {error}",
            "conversion_error": "Konvertálási hiba: {error}",
            "output_load_failed": "Nem sikerült betölteni a kimeneti fájlokat: {error}"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Válassz egy opciót...",
            "unknown_error": "Ismeretlen hiba",
            "loading": "Betöltés...",
            "starting": "Indítás...",
            "stopped": "Leállítva",
            "converting": "Konvertálás...",
            "no_files_found": "Nem találhatók fájlok",
            "select_file": "-- Válassz fájlt --",
            "select_output_file": "Válassz egy fájlt a kimenetek közül..."
        }
    }
}