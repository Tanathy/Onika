{
    "language_name": "Magyar",
    "ui": {
        "sidebar": {
            "training": "Tréning",
            "console": "Konzol",
            "metadata": "Metaadatok",
            "updates": "Frissítések",
            "system": "Rendszer",
            "connecting": "Kapcsolódás...",
            "language": "Nyelv"
        },
        "updates": {
            "title": "Rendszerfrissítések",
            "subtitle": "Tartsd naprakészen az Onika telepítésed",
            "check_status_default": "Ellenőrizd a frissítéseket, hogy lásd az elérhető változásokat.",
            "btn_check": "Frissítések keresése",
            "btn_checking": "Ellenőrzés...",
            "btn_apply": "Frissítések alkalmazása",
            "btn_applying": "Alkalmazás...",
            "details_title": "Frissítés részletei",
            "col_file": "Fájl útvonala",
            "col_status": "Állapot",
            "log_title": "Frissítési napló",
            "status_uptodate": "A rendszered naprakész.",
            "status_failed": "Nem sikerült ellenőrizni a frissítéseket.",
            "status_error": "Hiba a frissítőszerverhez csatlakozáskor.",
            "status_found": "{count} frissítés érhető el.",
            "checked_at": "Ellenőrizve: {time}",
            "confirm_apply": "Biztosan alkalmazod a(z) {count} frissítést?",
            "apply_success": "A frissítések sikeresen alkalmazva!",
            "apply_partial": "A frissítés lefutott, de voltak hibák.",
            "apply_failed": "Nem sikerült alkalmazni a frissítéseket.",
            "apply_error": "Hiba a frissítések alkalmazása közben."
        },
        "training": {
            "title": "Tréning",
            "subtitle": "Állítsd be és indítsd el a LoRA/LyCORIS tréninget",
            "preset": {
                "label": "Előbeállítás",
                "select_placeholder": "Előbeállítás kiválasztása...",
                "btn_load": "Betöltés",
                "btn_save": "Előbeállítás mentése",
                "btn_new": "Új előbeállítás",
                "btn_delete": "Törlés",
                "help": "Betölt egy előbeállítást a <span class=\"mono\">presets/</span> mappából, és alkalmazza az űrlapra. Az előbeállítások gyors módot adnak a profilváltásra. Az Előbeállítás mentése felülírja a kiválasztott előbeállítást. Az Új előbeállítás létrehoz egy újat egyedi névvel. A Törlés eltávolítja a kiválasztott előbeállítást."
            },
            "tabs": {
                "model": "Modell",
                "network": "Hálózat",
                "dataset": "Adatkészlet",
                "text_encoder": "Szövegkódoló",
                "aug": "Augmentáció \u0026 gyorsítótárazás",
                "learning": "Tanulás",
                "advanced": "Haladó",
                "samples": "Minták",
                "metadata": "Metaadatok"
            },
            "model": {
                "base_model": "Alapmodell",
                "base_model_help": "A tréninghez használt alapmodell. Ha olyan modellt választasz, ami közel áll a célstílusodhoz, gyorsabban konvergál a tanulás. Ügyelj rá, hogy az architektúra passzoljon a tréning beállításaidhoz (pl. SDXL alap SDXL tréninghez). Egy alapmodell (pl. SDXL 1.0) használata általában stabilabb, mint egy erősen finomhangolt, \"baked\" modell.",
                "architecture": "Modellarchitektúra",
                "architecture_help": "Megadja az architektúra logikát (SDXL, SD1.5, Flux), ami kell a súlyok betöltéséhez és a latensek kezeléséhez. Ha nem egyezik az architektúra az alapmodellel, shape mismatch hibákat fogsz kapni.",
                "type_sdxl": "SDXL / Pony",
                "type_sd_legacy": "SD 1.5 / 2.0",
                "type_sd3": "SD 3.0",
                "type_sd3_5": "SD 3.5",
                "type_flux1": "Flux.1",
                "type_flux2": "Flux.2",
                "btn_optimize": "Auto-optimalizálás a hardveremhez",
                "optimize_help": "A felismert GPU VRAM alapján automatikusan állítja a pontosságot, a kvantálást és a memória beállításokat.",
                "btn_adjust": "Automatikus beállítás (adatkészlet-függő)",
                "adjust_help": "Elemzi az adatkészletedet (képszám/méretek + captionök), és stabil tréningbeállításokat javasol a kiválasztott architektúrához. Úgy alkalmazza a változtatásokat, mint egy előbeállítás.",
                "quantization": "Modellkvantálás (Q-LoRA)",
                "quantization_none": "Nincs (normál fp16/bf16)",
                "quantization_8bit": "8-bit (alacsony VRAM)",
                "quantization_4bit": "4-bit (extrém alacsony VRAM)",
                "quantization_help": "A modell pontosságát 8-bitre vagy 4-bitre csökkenti, ezzel jelentősen leviszi a VRAM igényt, és lehetővé teszi a tréninget akár 8GB VRAM környékén is. A kisebb pontosság kicsit ronthat a pontosságon, és kell hozzá a `bitsandbytes` könyvtár.",
                "output_name": "Kimenet neve",
                "output_name_placeholder": "pl. karakter_lora_v1",
                "output_name_help": "A mentett LoRA adapterek fájlnevének előtagja. Használj egyedi neveket, hogy rendszerezd a verziókat és elkerüld a felülírást.",
                "output_dir": "Kimeneti mappa",
                "output_dir_help": "A tréning eredményeinek célmappája. Ügyelj rá, hogy legyen elég szabad lemezhely, különben a tréning megszakadhat.",
                "save_precision": "Mentési pontosság",
                "save_precision_help": "A mentett modellfájlok bitmélysége. A `float16` a standard jó kompromisszum méret és pontosság között, a `float32` pedig maximális hűséget ad, cserébe sokkal nagyobb fájlmérettel.",
                "save_precision_auto": "AUTO",
                "save_precision_fp16": "float16",
                "save_precision_bf16": "bf16",
                "save_precision_fp32": "float32",
                "save_format": "Mentési formátum",
                "save_format_help": "A kimenet fájlformátuma. A `safetensors` ajánlott biztonság és gyors betöltés miatt. A `ckpt` egy régi formátum, a `diffusers` pedig mappastruktúrába menti a modellt.",
                "save_format_safetensors": "safetensors",
                "save_format_ckpt": "ckpt",
                "save_format_diffusers": "diffusers",
                "save_epochs": "Mentés minden N. epoch után",
                "save_epochs_help": "Milyen gyakran mentsen checkpointot tréning közben. A rendszeres mentés segít összeomlás után folytatni, és több verziót ad az overfitting ellenőrzéséhez.",
                "save_steps": "Mentés minden N. lépés után",
                "save_steps_placeholder": "Opcionális",
                "save_steps_help": "Alternatív gyakoriság beállítás lépésszám alapján (epochok helyett).",
                "resume": "Folytatás checkpointból",
                "resume_placeholder": "pl. latest vagy path/to/checkpoint-1000",
                "resume_help": "A tréninget egy korábban mentett állapotból folytatja. Figyelj rá: ha folytatáskor alap paramétereket (pl. learning rate vagy rank) megváltoztatsz, az instabil tréninghez vezethet.",
                "save_best": "Csak a legjobb modellek mentése (legalacsonyabb loss)",
                "save_best_help": "Csak a legalacsonyabb rögzített lossú checkpointot tartja meg, hogy spóroljon a lemezhellyel. A legalacsonyabb loss nem mindig jelenti a legjobb vizuális minőséget.",
                "checkpoints_limit": "Checkpointok maximális száma",
                "checkpoints_limit_placeholder": "Opcionális",
                "checkpoints_limit_help": "A megtartandó checkpointok maximális száma. A régebbieket automatikusan törli, hogy kezelje a lemezhasználatot."
            },
            "network": {
                "title": "Hálózati beállítások",
                "type": "Hálózat típusa",
                "type_help": "Az adapter architektúrája. A LoRA az ipari standard. A LoHa és LoKr nagyobb kifejezőképességet ad, de gyakran óvatosabb hangolást igényel. Az OFT a hypersphere energiájának megőrzésére készült, és különösen hatékony Flux modelleknél.",
                "type_lora": "LoRA",
                "type_lycoris": "LyCORIS",
                "type_loha": "LoHA",
                "type_lokr": "LoKr",
                "type_oft": "OFT (Orthogonal Finetuning)",
                "module": "Hálózati modul",
                "module_help": "Az adapterhez használt belső Python modul. Ez egy haladó beállítás, általában nem érdemes piszkálni.",
                "dim": "Hálózat dimenzió (rank)",
                "dim_help": "Az adapter kapacitása. A magasabb értékek (pl. 128) részletesebb tanulást engednek, de növelik az overfitting esélyét és nagyobb fájlokat eredményeznek. Az alacsonyabb értékek (pl. 16) jobban általánosítanak és kisebb fájlokat adnak.",
                "alpha": "Hálózat alfa",
                "alpha_help": "Skálázó tényező, ami megakadályozza, hogy a súlyfrissítések túl agresszívek legyenek. Gyakori ökölszabály: stabilitáshoz az Alpha legyen a dim fele, erősebb hatáshoz pedig legyen egyenlő a dimmel.",
                "algo": "LyCORIS algoritmus",
                "algo_help": "LyCORIS algoritmusváltozat (csak LyCORIS/LoHa módoknál releváns). Az algoritmusok az expresszivitás és stabilitás között cserélgetnek. Ha nem vagy biztos benne, kezdd a <span class=\"mono\">lora</span>/<span class=\"mono\">locon</span>-nal, és csak akkor válts, ha mérhető előnyt látsz.",
                "conv_dim": "Conv rank (dim)",
                "conv_dim_help": "Opcionális conv adapter rank LoCon/LyCORIS-hoz. Kis értékek kis konvolúciós kapacitás pluszt adnak; nagy értékek növelik a VRAM igényt és könnyen túlillesztenek textúrára/részletekre. Hagyd üresen, ha nem használsz kifejezetten conv-os módszert.",
                "conv_alpha": "Conv alfa",
                "conv_alpha_help": "Opcionális conv skálázás (alpha). Alacsonyabb alpha finomabb conv hatást ad; magasabb alpha agresszívebb, és instabilitást/overshootot okozhat. Tipikusan <= conv dim; ha túl erősnek érzed a conv hatást, előbb az alphát csökkentsd, ne a dimet.",
                "dora_wd": "DoRA súlycsökkenés",
                "dora_wd_help": "Súlycsökkenés, ami kifejezetten a DoRA magnitude vektorokra vonatkozik.",
                "network_dropout": "Hálózat dropout",
                "network_dropout_help": "Véletlenszerűen eldobja az adapteren belüli neuron kimeneteket, hogy robusztusabb legyen.",
                "rank_dropout": "Rank dropout",
                "rank_dropout_help": "Véletlenszerűen eldob egyes rank dimenziókat regularizációként.",
                "module_dropout": "Modul dropout",
                "module_dropout_help": "Tréning közben véletlenszerűen letilt egész modulokat az overfitting csökkentéséhez és a jobb általánosításhoz.",
                "lora_blocks": "LoRA blokkok",
                "lora_blocks_help": "Lehetővé teszi konkrét blokkok célzását az architektúrán belül (pl. mid blokkok).",
                "lora_layers": "LoRA rétegek",
                "lora_layers_help": "Lehetővé teszi konkrét rétegek célzását (pl. attention rétegek) finomhangoláshoz.",
                "advanced_lora": "Haladó LoRA opciók",
                "lycoris_settings": "LyCORIS beállítások",
                "args": "Hálózat argumentumok",
                "args_help": "Extra argumentumok a hálózati modulhoz, vesszővel elválasztott key=value párok formájában. Használd speciális opciókhoz (pl. dropout, decomposition). A hibás argumentumok instabil tréninghez vezethetnek."
            },
            "dataset": {
                "dataset_type": "Adatkészlet típusa",
                "dataset_type_hub": "Hugging Face Hub",
                "dataset_type_local": "Helyi fájlok",
                "dataset_type_help": "Válaszd ki az adatkészleted forrását. A Hugging Face Hub megkönnyíti a megosztást és a letöltést. A Helyi fájlokkal a gépeden lévő fájlokat használhatod.",
                "path": "Adatkészlet útvonala",
                "path_help": "Az a mappa, ami a tréningképeket és a hozzájuk tartozó caption fájlokat tartalmazza (pl. .txt). Az adatkészlet minősége a tréning sikerének legfontosabb tényezője; ügyelj rá, hogy a captionök pontosan írják le a képeket.",
                "repo_id": "Repo azonosító",
                "repo_id_placeholder": "pl. felhasznalo/repo",
                "repo_id_help": "A Hugging Face repo azonosítója az adatkészlet letöltéséhez. Hagyd üresen, ha helyi fájlokat használsz.",
                "local_dir": "Helyi mappa",
                "local_dir_help": "A mappa a gépeden, ami az adatkészlet fájljait tartalmazza. Győződj meg róla, hogy elérhető és a megfelelő formátumot tartalmazza.",
                "image_folder": "Képmappa",
                "image_folder_help": "A helyi mappán belüli almappa, ami a tréninghez használt képfájlokat tartalmazza.",
                "annotation_file": "Annotációs fájl",
                "annotation_file_placeholder": "pl. captions.txt",
                "annotation_file_help": "A képek annotációit vagy captionjeit tartalmazó fájl. Olyan tréningnél hasznos, ahol szöveg bemenet is kell.",
                "resolution": "Felbontás",
                "resolution_help": "A tréning célfelbontása. A nagyobb felbontás több részletet ad, de több VRAM kell hozzá. Ügyelj rá, hogy passzoljon a modellarchitektúrához (pl. 1024x1024 SDXL-hez, 512x512 SD1.5-höz).",
                "batch_size": "Batch méret",
                "batch_size_help": "Egyszerre feldolgozott képek száma. A nagyobb batch gyorsíthat és simább gradienseket adhat, viszont jelentősen növeli a VRAM használatot.",
                "max_epochs": "Max epoch",
                "max_epochs_help": "Hány teljes kör menjen végig az adatkészleten. LoRA tréninghez általában 10–20 epoch elég. Túl sok epoch overfittinget és \"szétégett\" képeket okozhat.",
                "max_steps": "Max lépésszám",
                "max_steps_help": "Opcionális kemény limit a teljes tréning lépésszámára.",
                "max_steps_placeholder": "Opcionális",
                "bucketing": "Képarány bucketing engedélyezése",
                "bucketing_help": "Automatikusan képarány szerint csoportosítja a képeket, hogy elkerülje a felesleges vágást és megőrizze az eredeti kompozíciót.",
                "bucket_steps": "Bucket felbontás lépésköze",
                "bucket_steps_help": "A bucket dimenziók rácslépése. A 64 a standard érték a legtöbb architektúrához.",
                "min_bucket": "Minimális bucket felbontás",
                "min_bucket_help": "A képbucket minimálisan engedélyezett felbontása. Megakadályozza, hogy túl kicsi vagy elmosódott képek kerüljenek tréningbe.",
                "max_bucket": "Maximális bucket felbontás",
                "max_bucket_help": "A képbucket maximálisan engedélyezett felbontása, ami segít elkerülni az out-of-memory (OOM) hibákat extrém nagy képeknél.",
                "center_crop": "Középre vágás (okos 1:1)",
                "center_crop_help": "Ha be van kapcsolva, a közel négyzetes képeket (pl. 1210x1280) középről 1:1 arányra vágja. Karaktertréninghez ajánlott a konzisztens framing miatt.",
                "no_upscale": "Nincs felskálázás",
                "no_upscale_help": "Megakadályozza a kis képek felskálázását a bucket dimenziókhoz, így elkerülhetők bizonyos artefaktok.",
                "dreambooth": "DreamBooth \u0026 prior megőrzés",
                "prior_preservation": "Prior megőrzés engedélyezése (reg képek)",
                "prior_preservation_help": "Regularizációs képeket használ, hogy a modell ne felejtse el az osztály (pl. \"person\") általános fogalmát, miközben egy konkrét példányt tanul. Ez fontos az általános tudás megőrzéséhez, viszont növeli a tréningidőt.",
                "num_class_images": "Reg képek száma",
                "num_class_images_help": "A cél regularizációs képszám. Gyakori ajánlás: 100 reg kép minden instance képre.",
                "instance_prompt": "Instance prompt",
                "instance_prompt_help": "Egyedi trigger szó + osztályszó kombinációja (pl. \"sks person\"), ami azonosítja a tanított alanyt.",
                "instance_prompt_placeholder": "pl. a photo of sks person",
                "class_prompt": "Class prompt",
                "class_prompt_help": "Az általános osztályszó (pl. \"person\"), amit a regularizációs képekhez használsz.",
                "class_prompt_placeholder": "pl. a photo of a person",
                "reg_dir": "Reg képek mappája",
                "reg_dir_help": "A regularizációs képeket tartalmazó mappa. Csak akkor használatos, ha a prior megőrzés be van kapcsolva.",
                "reg_dir_placeholder": "Opcionális",
                "auto_gen": "Reg képek automatikus generálása",
                "auto_gen_help": "Automatikusan legenerálja a regularizációs képeket, ha még nincsenek meg a megadott mappában.",
                "gen_settings": "Generálási beállítások",
                "neg_class_prompt": "Negatív class prompt",
                "neg_class_prompt_help": "Negatív prompt a regularizációs képek generálásához.",
                "neg_class_prompt_placeholder": "Opcionális negatív prompt",
                "guidance": "Guidance skála",
                "guidance_help": "CFG skála a regularizációs képek generálásához.",
                "steps": "Reg lépések",
                "steps_help": "Mintavételi lépések száma a regularizációs képek generálásához.",
                "scheduler": "Reg scheduler",
                "scheduler_help": "Sampler a regularizációs képek generálásához.",
                "seed": "Reg seed",
                "seed_help": "Seed a regularizációs képek generálásához. -1 = véletlen.",
                "sample_warning": "Tipp: ha a \"Minták generálása N lépésenként\" és a \"Minták generálása N epochonként\" is üresen marad, az Onika nem fog mintaképeket generálni (rengeteg időt spórol). Alacsony és középkategóriás GPU-kon tréning közben általában nem ajánlott gyakran mintákat generálni.",
                "train_val_split": "Train/Val arány",
                "train_val_split_help": "A tréning és validáció aránya. Gyakori felosztás: 80% tréning, 20% validáció.",
                "shuffle": "DataLoader keverés",
                "shuffle_help": "Minden epochban összekeveri a képek sorrendjét, hogy a modell ne tanulja meg az adatkészlet sorrendjét.",
                "workers": "DataLoader workerek",
                "workers_help": "A betöltéshez és előfeldolgozáshoz dedikált CPU szálak száma. A magasabb érték gyorsabban etetheti a GPU-t, de ha túl magasra teszed, belassíthatja a rendszert.",
                "num_workers": "Workerek száma",
                "num_workers_help": "Az adatbetöltéshez használt subprocesszek száma. Több worker gyorsíthat, de több erőforrást igényel.",
                "persistent_workers": "Perzisztens DataLoader workerek",
                "persistent_workers_help": "Az adatbetöltő CPU workereket aktívan tartja epochok között, így gyorsíthat, viszont több RAM-ot használ.",
                "pin_memory": "Pin memória",
                "pin_memory_help": "Ha be van kapcsolva, a DataLoader a tensorokat CUDA pinned memóriába másolja. Ez javíthatja a GPU-ra történő átvitel teljesítményét."
            },
            "text_encoder": {
                "title": "Szövegkódoló",
                "ti_title": "Textual Inversion (pivotal tuning)",
                "weighting_title": "Caption súlyozás",
                "train": "Szövegkódoló tanítása",
                "train_help": "Engedélyezi a szövegkódoló tanítását a UNet mellett, ami javíthatja a prompt követést, viszont csökkentheti a modell rugalmasságát vagy \"editálhatóságát\".",
                "clip_skip": "Clip skip",
                "clip_skip_help": "A CLIP szövegkódoló legfelső N rétegét kihagyja. SD1.5-höz tipikusan 1-et használnak, SDXL-hez általában 0-t. Rossz érték gyenge prompt értelmezéshez vezethet.",
                "train_ti": "Textual Inversion tanítása",
                "train_ti_help": "Új tokeneket tanít (Textual Inversion), hogy konkrét szavakat vagy fogalmakat adjon a modell szókincséhez.",
                "ti_frac": "TI tanítási arány",
                "ti_frac_help": "Az összes tréning epoch hány százalékában aktív a Textual Inversion tréning.",
                "te_frac": "TE tanítási arány",
                "te_frac_help": "Az összes tréning epoch hány százalékában aktív a szövegkódoló (Text Encoder) tréning.",
                "emphasis": "Alapértelmezett kiemelés",
                "emphasis_help": "Az alapértelmezett szorzó a kiemelt tagekre, amikor nincs megadva külön súly.",
                "de_emphasis": "Alapértelmezett gyengítés",
                "de_emphasis_help": "Az alapértelmezett szorzó a gyengített tagekre, amikor nincs megadva külön súly.",
                "enable_weighted": "Súlyozott captionök engedélyezése",
                "enable_weighted_help": "Engedélyezi a súlyozott szintaxist a captionökben (pl. \"(word:1.1)\"), így pontosan szabályozhatod bizonyos kifejezések fontosságát.",
                "new_tokens": "Új tokenek absztrakciónként",
                "new_tokens_help": "Hány vektort kapjon egy új token. Több vektor több részletet tud elkapni, de nehezebb hatékonyan betanítani.",
                "token_abs": "Token absztrakció",
                "token_abs_help": "A helyettesítő szöveg (pl. \"TOK\"), ami az új fogalmat jelöli a captionökben.",
                "train_text_encoder": "Szövegkódoló tanítása",
                "train_text_encoder_help": "Tanítsa-e a szövegkódolót a UNet mellett. Ajánlott jobb prompt követéshez.",
                "text_encoder_lr": "Szövegkódoló tanulási ráta",
                "text_encoder_lr_help": "A szövegkódoló learning rate-je. Általában alacsonyabb, mint a UNet learning rate."
            },
            "aug": {
                "aug_title": "Képaugmentáció",
                "enable_augmentation": "Augmentáció engedélyezése",
                "enable_augmentation_help": "Tréning közben adat-augmentációs technikákat kapcsol be, hogy a modell jobban általánosítson.",
                "aug_mode": "Augmentáció mód",
                "aug_mode_help": "Eldönti, mikor lépnek életbe az augmentációk tréning közben. <strong>Mindig:</strong> minden képet minden lépésnél augmentál — maximális változatosság, de kicsit lassíthat. <strong>Csak epochonként:</strong> az augmentációk epochonként egyszer sorsolódnak, és azon a körön belül fixek maradnak. Stabilabb, de még mindig keveri a dolgokat epochok között. <strong>Véletlen valószínűség:</strong> minden lépésnél képenként eldől, hogy legyen-e augmentáció, az egyedi beállítások alapján (pl. <a href=\"#\" class=\"xref-link\" data-xref=\"flip_aug_probability\" data-xref-label=\"Flip Aug Probability\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Flip Aug Probability</a>). A legtöbbször a \"Mindig\" vagy a \"Csak epochonként\" tökéletes.",
                "color_aug": "Szín augmentáció erőssége",
                "color_aug_help": "Mennyire rángassa meg a színeket (árnyalat, telítettség, fényerő) tréning közben. <strong>0.0:</strong> a színek pontosan ugyanazok maradnak — nincs belenyúlás. <strong>0.3-0.5:</strong> finom variáció, a legtöbb adatkészlethez szuper, és segít, hogy a modell ne a konkrét színeket magolja be. <strong>0.7-1.0:</strong> erős színeltolások. Furcsa kombók is kijöhetnek, viszont keményen tolja az általánosítást. Különösen hasznos karakter/stílus LoRÁ-knál, ha azt akarod, hogy a forma és a koncepció üljön, ne ragadjon le egy adott palettán. Kapcsold ki, ha fontos a színhűség (brand színek, konkrét ruhák, stb.).",
                "flip_aug": "Tükrözés esélye",
                "flip_aug_help": "Az esélye annak, hogy a képet vízszintesen tükrözze. <strong>0.5</strong> = 50/50, gyakorlatilag megduplázza az adatkészletedet tükrözött verziókkal. <strong>0.0</strong> = nincs tükrözés. <strong>Figyi:</strong> tedd 0-ra, ha a képeiden van: • <strong>szöveg vagy logó</strong> (visszafelé lenne) • <strong>aszimmetrikus dolog</strong> (autók, arcok jellegzetes oldallal) • <strong>irányfüggő tartalom</strong> (bal vs jobb kéz, specifikus póz). Szimmetrikus témákhoz (középre komponált portré, absztrakt, sok anime karakter) viszont remek. Kicsi adatkészleteknél az egyik legjobb trükk overfitting ellen.",
                "random_flip": "Véletlen tükrözés",
                "random_flip_help": "Véletlenszerűen tükrözi a képeket vízszintesen és/vagy függőlegesen.",
                "crop_scale": "Véletlen vágás skála",
                "crop_scale_help": "A véletlen vágás minimális zoom szintje. <strong>1.0:</strong> nincs vágás — a képek teljes méretben maradnak. <strong>0.8:</strong> 80–100% között vág, finom zoom variációkat adva. <strong>0.5:</strong> drasztikusabb — akár 2×-es belenagyítást is jelenthet, teljesen eltérő framinggel. A vágás megtanítja a modellt különböző skálákra és kompozíciókra. Szuper, ha a dataseted csupa \"tökéletesen középre komponált headshot\", de rugalmasabb kimenetet szeretnél. Hagyd 1.0-n, ha a pontos framing számít.",
                "rotation_range": "Forgatás tartomány",
                "rotation_range_help": "Véletlenszerűen elforgatja a képeket a megadott tartományon belül (fokban).",
                "width_shift_range": "Vízszintes eltolás",
                "width_shift_range_help": "Véletlenszerűen eltolja a képeket vízszintesen a megadott tartományon belül (a teljes szélesség arányában).",
                "height_shift_range": "Függőleges eltolás",
                "height_shift_range_help": "Véletlenszerűen eltolja a képeket függőlegesen a megadott tartományon belül (a teljes magasság arányában).",
                "shear_range": "Nyírás",
                "shear_range_help": "Véletlenszerűen nyírja (shear) a képeket a megadott tartományon belül (fokban).",
                "zoom_range": "Zoom tartomány",
                "zoom_range_help": "Véletlenszerűen rázoomol vagy kizóomol a képekre a megadott tartományon belül.",
                "channel_shift_range": "Csatorna eltolás",
                "channel_shift_range_help": "Véletlenszerűen eltolja a képek színcsatornáit.",
                "brightness_range": "Fényerő tartomány",
                "brightness_range_help": "Véletlenszerűen állítja a fényerőt a megadott tartományon belül.",
                "contrast_range": "Kontraszt tartomány",
                "contrast_range_help": "Véletlenszerűen állítja a kontrasztot a megadott tartományon belül.",
                "saturation_range": "Telítettség tartomány",
                "saturation_range_help": "Véletlenszerűen állítja a telítettséget a megadott tartományon belül.",
                "hue_range": "Árnyalat tartomány",
                "hue_range_help": "Véletlenszerűen állítja az árnyalatot a megadott tartományon belül.",
                "cutout_size": "CutOut méret",
                "cutout_size_help": "A CutOut augmentáció kivágott négyzeteinek mérete. 0-ra állítva kikapcsol.",
                "grid_mask_size": "Grid Mask méret",
                "grid_mask_size_help": "A Grid Mask augmentáció rácscelláinak mérete. 0-ra állítva kikapcsol.",
                "random_eraser": "Véletlen törlés",
                "random_eraser_help": "Véletlenszerű törlés (random erasing) augmentációt alkalmaz a képekre.",
                "mixup_alpha": "Mixup alfa",
                "mixup_alpha_help": "A Mixup augmentáció alfa paramétere. 0-ra állítva kikapcsol.",
                "cutmix_alpha": "CutMix alfa",
                "cutmix_alpha_help": "A CutMix augmentáció alfa paramétere. 0-ra állítva kikapcsol.",
                "label_smoothing": "Címke simítás",
                "label_smoothing_help": "Tréning közben label smoothingot alkalmaz a célcímkékre.",
                "random_crop": "Véletlen vágás",
                "random_crop_help": "Véletlenszerűen kivágja a képeket a megadott célméretre.",
                "resize": "Átméretezés",
                "resize_help": "A képeket a megadott célméretre méretezi át.",
                "normalize": "Normalizálás",
                "normalize_help": "Normalizálja a képeket (0 átlag, 1 szórás).",
                "denormalize": "Denormalizálás",
                "denormalize_help": "Denormalizálja a képeket, hogy visszaállítsa az eredeti pixelértékeket.",
                "caption_aug_title": "Caption augmentáció",
                "shuffle": "Captionök keverése",
                "shuffle_help": "Minden használatkor összekeveri a tagek/szavak sorrendjét a captionökben. Megakadályozza, hogy a modell bemagoljon fix mintákat (pl. \"a kék szem mindig a hosszú haj előtt\"), inkább azt tanulja meg, hogy ezek a tagek bárhol előfordulhatnak. Booru-stílusú, vesszővel tagolt tagekhez tökéletes, ahol a sorrend nem számít. Használd a <a href=\"#\" class=\"xref-link\" data-xref=\"keep_tokens\" data-xref-label=\"Keep Tokens\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Keep Tokens</a>-t, ha azt akarod, hogy a trigger szó fixen maradjon. <strong>Ne használd</strong> természetes nyelvű captionökhöz, ahol a szórend jelentést hordoz (\"woman holding cat\" ≠ \"cat holding woman\").",
                "keep_tokens": "Megtartott tokenek",
                "keep_tokens_help": "Hány token legyen \"szent\" a caption elején, amit nem kever. <strong>0:</strong> mindent keverhet. <strong>1:</strong> az első token (általában a trigger szó) mindig első marad. <strong>2-3:</strong> védi a \"trigger, karakter_nev\" mintát. Ha a captionjeid így néznek ki: \"mytrigger, blonde hair, blue eyes, ...\", akkor 1-re állítva a \"mytrigger\" elöl rögzül, a többi pedig keveredik. Segít, hogy a modell a trigger–koncepció kapcsolatot tanulja, ne a konkrét sorrendet. Csak akkor számít, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"shuffle_caption_checkbox\" data-xref-label=\"Shuffle Captions\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Shuffle Captions</a> be van kapcsolva.",
                "dropout": "Dropout arány",
                "dropout_help": "Annak valószínűsége, hogy egy kép captionje teljesen eldobásra kerül (mintha üres lenne). Ha a caption eltűnik, a modellnek kizárólag a vizuálisból kell kitalálnia a képet — ez az unconditional generálást tanítja. <strong>0.0:</strong> a caption mindig jelen van. <strong>0.05-0.1:</strong> enyhe dropout, finoman javíthatja az üres/gyenge promptokra adott viselkedést. <strong>0.15-0.2:</strong> agresszívebb — erősen tolja az unconditional minőséget. A dropout segíti a CFG-t inference-ben, mert a modell tényleg tudja, milyen a \"nincs prompt\" állapot. LoRA tréningnél, ahol a prompt követés a király, túl sok dropout visszaüthet. Egy pici (0.05) gyakran jó; nagyon rövid tréningnél akár hagyd is ki.",
                "dropout_epochs": "Dropout minden N. epochban",
                "dropout_epochs_help": "Nem lépésenként véletlenszerűen dob el, hanem konkrét epochokban. <strong>0:</strong> visszaáll a normál, véletlen <a href=\"#\" class=\"xref-link\" data-xref=\"caption_dropout_rate\" data-xref-label=\"Dropout Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-aug\">Dropout Rate</a> működésre. <strong>1:</strong> minden epoch caption nélküli (elég durva). <strong>5:</strong> az 5., 10., 15... epochok mennek caption nélkül. Strukturáltabb minta: többnyire normál tréning, időnként \"vak\" epochokkal. Speciális tanítási menetrendekhez hasznos, de a véletlen lépésenkénti dropout a gyakoribb.",
                "noise_title": "Zaj beállítások",
                "noise_type": "Zaj offset típusa",
                "noise_type_help": "A zaj offset számításának matematikai módszere. Az \"Original\" a standard implementáció.",
                "noise_offset": "Zaj offset erőssége",
                "noise_offset_help": "Offsetet ad a zajhoz tréning közben, ami segít jobb dinamikaátfogást elérni (mélyebb feketék, világosabb fehérek). Általában 0.05–0.1 ajánlott.",
                "adaptive_noise": "Adaptív zaj skála",
                "adaptive_noise_help": "A zajt a modell predikciós hibája alapján skálázza tréning közben.",
                "noise_random": "Zaj offset véletlen erőssége",
                "noise_random_help": "Minden tréning lépésnél véletlenszerűvé teszi a zaj offset erősségét.",
                "multires_iter": "Multires zaj iteráció",
                "multires_iter_help": "Multi-resolution zajt alkalmaz, hogy javítsa a finom textúrák és részletek tanulását.",
                "multires_discount": "Multires zaj diszkont",
                "multires_discount_help": "A multi-resolution zaj iterációkra alkalmazott diszkont tényező.",
                "edm": "EDM-stílusú tréning (SDXL)",
                "edm_help": "Az EDM (Elucidating the Design Space of Diffusion-Based Generative Models) formulációt használja, ami SDXL-nél jobb minőségű kimenetet adhat.",
                "caching_title": "Gyorsítótárazás",
                "cache_latents": "Latensek cache-elése RAM-ba",
                "cache_latents_help": "Előre kiszámolja a VAE latenseket, amivel jelentősen gyorsíthatja a tréninget (gyakran 2×–3×). Bekapcsolva minden latent RAM-ban marad tréning alatt. Ez letilt bizonyos valós idejű augmentációkat.",
                "cache_te": "Szöveg embeddingek cache-elése",
                "cache_te_help": "Előre kiszámolja a szöveg embeddingeket a gyorsabb tréninghez. Ez a beállítás nem kompatibilis az aktív szövegkódoló tréninggel.",
                "cache_disk": "Latensek cache-elése lemezre",
                "cache_disk_help": "Előre kiszámolt latenseket ment lemezre RAM spórolásért. Lassú lemez I/O ronthatja a teljesítményt. Ezeket a latenseket későbbi tréningeknél újra felhasználja. Ha változik az adatkészlet, manuálisan törölnöd kell a régi latenseket a cache-ből.",
                "vae_batch": "VAE batch méret",
                "vae_batch_help": "A VAE enkódolás batch mérete cache-eléskor. A magasabb érték gyorsít, de több VRAM kell hozzá.",
                "vae_batch_placeholder": "Auto"
            },
            "learning": {
                "optimizer": "Optimalizáló",
                "optimizer_help": "Az algoritmus, ami a gradiensek alapján ténylegesen frissíti a súlyokat. Különböző optimalizálók különböző erősségekkel bírnak. <strong>AdamW8bit:</strong> a klasszikus 8-bites, memóriatakarékos verziója, kb. ~50%-kal kevesebb optimizer state memóriát használ. Szűk VRAM mellett a legtöbb LoRA tréningnél ez a go-to. Kell hozzá a bitsandbytes. <strong>AdamW:</strong> az eredeti, full precision változat. A legstabilabb és leginkább kipróbált. Válaszd ezt, ha van VRAM tartalékod, vagy gond van a 8-bites verzióval. <strong>Lion / Lion8bit:</strong> újabb, sokszor jó, de gyakran jóval kisebb LR kell (3–10× kisebb, mint AdamW-nál). <strong>DAdaptAdam:</strong> önbeállító optimizer. Állítsd a <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Learning Rate</a>-et 1.0-ra és hagyd, hogy megtalálja az optimális értéket. <strong>Prodigy:</strong> egy másik okos, adaptív optimizer — itt is LR=1.0-t használj. Gyakran kiváló eredményt ad beállítás nélkül. <strong>CAME:</strong> kísérleti, korrekt memóriahatékonyság. <strong>Adafactor:</strong> brutál memóriabarát, eredetileg óriás nyelvi modellekhez készült — végső megoldás extrém VRAM szűkében. <strong>SGD:</strong> alap gradienscsökkenés; ritkán használják diffúziós tréningnél, de ha kísérleteznél, itt van. Finomhangold a viselkedést az <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer_args\" data-xref-label=\"Optimizer Args\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Optimizer Args</a>-szal.",
                "optimizer_adamw": "AdamW (stabil)",
                "optimizer_adamw8bit": "AdamW8bit (memóriatakarékos)",
                "optimizer_lion": "Lion (kísérleti)",
                "optimizer_prodigy": "Prodigy (haladó)",
                "optimizer_dadaptation": "DAdaptAdam (adaptív LR)",
                "optimizer_args": "Optimizer argumentumok",
                "optimizer_args_help": "Extra finomhangolások, amik közvetlenül az <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">optimizer</a>-nek mennek. Formátum: <code>key=value</code>, vesszővel elválasztva. Példák: <strong>weight_decay=0.01</strong> — L2 regularizáció overfitting ellen (gyakori AdamW-nál). <strong>betas=(0.9,0.999)</strong> — Adam momentum együtthatók. <strong>d_coef=1.0</strong> — D-koefficiens Prodigy/DAdaptAdam-hoz. <strong>eps=1e-8</strong> — numerikus stabilitáshoz. Az alapértékek a legtöbb tréninghez jók. Csak akkor nyúlj hozzá, ha követsz egy konkrét guide-ot vagy tényleg tudod, mit csinálsz.",
                "optimizer_args_placeholder": "key=value, key2=value2",
                "lr": "Learning rate",
                "lr_help": "A tréning alap learning rate-je.",
                "learning_rate": "Learning rate",
                "learning_rate_help": "A teljes tréninged egyik legfontosabb beállítása. Ez dönti el, mennyire agresszíven módosulnak a súlyok lépésenként. <strong>Túl magas</strong> (pl. 1e-3 vagy több) és \"megsütöd\" a modellt — artefaktok, zaj, akár teljes összeomlás. <strong>Túl alacsony</strong> (pl. 1e-6 vagy kevesebb) és a tréning vánszorog vagy sehová nem jut. <strong>Kiindulásnak:</strong> - <strong>LoRA/LyCORIS:</strong> 1e-4–5e-4 általában a sweet spot - <strong>Teljes finetune:</strong> jóval alacsonyabb, kb. 1e-6–1e-5 - <strong>Prodigy/DAdaptAdam:</strong> használd az 1.0-t és hagyd, hogy az optimizer belője. A \"tökéletes\" érték batch mérettől, adatkészlettől és setup-tól függ. A nagyobb effektív batch (pl. <a href=\"#\" class=\"xref-link\" data-xref=\"grad_acc\" data-xref-label=\"Gradient Accumulation\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Gradient Accumulation</a> segítségével) gyakran jobban viseli a magasabb learning rate-eket.",
                "unet_lr": "UNet LR",
                "unet_lr_help": "Felülírhatod a tanulási rátát csak az UNet-re — ez a kép-generálás fő motorja. Hagyd üresen, ha a globális <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"Learning Rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">tanulási ráta</a> beállítást akarod használni. Ha külön rátát adsz az UNet-nek és a Text Encodernek, pontosan kézben tarthatod, mi tanuljon gyorsabban. Az UNet felel a vizuálért (stílus, kompozíció, szerkezet), szóval ez közvetlenül befolyásolja, milyen gyorsan „sül be” a tréningelt témád kinézete. Őszintén: a két résznek ugyanaz a ráta általában teljesen oké — csak akkor bontsd szét, ha tudod, mit csinálsz.",
                "unet_lr_placeholder": "Opcionális",
                "te_lr": "Text Encoder LR",
                "te_lr_help": "Tanulási ráta kifejezetten a Text Encoderhez (CLIP vagy T5, az alapmodelltől függően). Ez a komponens fordítja le a szöveges promptjaidat arra, amit a kép-generátor ért. Általános ökölszabály: az UNet rátájának kb. <strong>1/10–1/2</strong>-e, vagy akár teljesen kapcsold ki (0). <strong>Ha tréningeled:</strong> segít új fogalmakat/neveket felismerni, és jobban reagálni a stílus-specifikus promptjaidra. <strong>Ha kihagyod (0):</strong> érintetlenül hagyja az alapmodell nyelvi tudását — jó, ha csak a vizuális stílus érdekel. LoRA-nál fél <a href=\"#\" class=\"xref-link\" data-xref=\"unet_lr\" data-xref-label=\"UNet LR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet LR</a> gyakran stabil kiindulópont.",
                "te_lr_placeholder": "Opcionális",
                "lr_scheduler": "LR ütemező",
                "lr_scheduler_help": "Megmondja, hogyan változzon a tanulási rátád a tréning során. Ezen simán el tud múlni az eredmény. <strong>constant:</strong> végig fix. Egyszerű, kiszámítható — rövid futásokhoz kiváló. <strong>cosine:</strong> erősen indul, majd finoman lecseng koszinusz görbével. Népszerű, mert az elején tolja a tanulást, a végén pedig szépen finomít. <strong>cosine_with_restarts:</strong> ugyanaz, csak időnként felrúgja a rátát. Segíthet kimászni lokális minimumokból. A restartok számát a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_num_cycles\" data-xref-label=\"LR Scheduler Cycles\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">LR ütemező ciklusok</a> adja. <strong>linear:</strong> egyenes vonal a kezdő LR-től nulláig. Egyszerű és hatékony. <strong>polynomial:</strong> hatványfüggvény szerint csökken. A viselkedést a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler_power\" data-xref-label=\"LR Scheduler Power\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Scheduler Power</a> hangolja. <strong>constant_with_warmup:</strong> nulláról felkúszik, aztán fixen tartja. Párosítsd a <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">warmup lépésekkel</a> a szép induláshoz. A legtöbb LoRA melóra a <strong>constant</strong> vagy a <strong>cosine</strong> bőven elég.",
                "lr_scheduler_constant": "constant",
                "lr_scheduler_linear": "linear",
                "lr_scheduler_cosine": "cosine",
                "lr_scheduler_cosine_with_restarts": "cosine_with_restarts",
                "lr_scheduler_polynomial": "polynomial",
                "lr_scheduler_constant_with_warmup": "constant_with_warmup",
                "scheduler": "LR ütemező",
                "scheduler_help": "Az a stratégia, ami tréning közben állítja a tanulási rátát.",
                "warmup": "Warmup lépések",
                "warmup_help": "Hány lépésen keresztül emelkedjen finoman a tanulási ráta nulláról a célértékig. Olyan, mint edzés előtt a bemelegítés — nem sokkolja a modellt hirtelen nagy gradiensekkel, amikor a súlyok még messze vannak az optimálistól. <strong>0</strong> = nincs warmup, azonnal teljes LR. <strong>10–100 lépés</strong> LoRA-hoz általában bőven elég. <strong>100–500 lépés</strong> segíthet teljes finetune-nál vagy nagy batch-eknél. Megadhatod arányként is a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> mezővel. Különösen hasznos magas LR-nél vagy adaptív optimalizereknél, pl. <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy vagy DAdaptAdam</a>.",
                "warmup_steps": "Warmup lépések",
                "warmup_steps_help": "Hány lépésen keresztül emelkedjen finoman a tanulási ráta nulláról a célértékig. Olyan, mint edzés előtt a bemelegítés — nem sokkolja a modellt hirtelen nagy gradiensekkel, amikor a súlyok még messze vannak az optimálistól. <strong>0</strong> = nincs warmup, azonnal teljes LR. <strong>10–100 lépés</strong> LoRA-hoz általában bőven elég. <strong>100–500 lépés</strong> segíthet teljes finetune-nál vagy nagy batch-eknél. Megadhatod arányként is a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_warmup_ratio\" data-xref-label=\"Warmup Ratio\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup Ratio</a> mezővel. Különösen hasznos magas LR-nél vagy adaptív optimalizereknél, pl. <a href=\"#\" class=\"xref-link\" data-xref=\"optimizer\" data-xref-label=\"Optimizer\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Prodigy vagy DAdaptAdam</a>.",
                "warmup_ratio": "Warmup arány",
                "warmup_ratio_help": "Alternatíva a <a href=\"#\" class=\"xref-link\" data-xref=\"warmup_steps\" data-xref-label=\"Warmup Steps\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">Warmup lépések</a> helyett — a warmupot a teljes tréning arányában adod meg, nem konkrét lépésszámmal. <strong>0.0</strong> = nincs warmup. <strong>0.05</strong> = az első 5%-ban warmup. <strong>0.1</strong> = az első 10%-ban warmup. Sokkal kényelmesebb, mert automatikusan skálázódik a tréning hosszával. Ha mindkettőt beállítod, ez nyer.",
                "cycles": "LR ciklusok",
                "cycles_help": "Hány teljes fel-le ciklust fusson, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">cosine_with_restarts</a> ütemezőt használod. Minden ciklus leviszi az LR-t minimumra, majd visszarúgja (általában kisebb csúcsra, mint előtte). <strong>1</strong> = egy ciklus az egész tréningen, kb. ugyanaz, mint a sima cosine. <strong>2–4</strong> ciklus segíthet kimozdítani a modellt lokális minimumokból és jobban feltérképezni a loss tájat. Több ciklus = több „restart” = jobb felfedezés, de a végén kevésbé stabil lehet. Csak akkor számít, ha cosine_with_restarts van.",
                "scheduler_power": "Ütemező hatvány",
                "scheduler_power_help": "A kitevő, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"lr_scheduler\" data-xref-label=\"LR Scheduler\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">polynomial ütemezőt</a> használod. Meghatározza, mennyire agresszíven essen az LR. <strong>1.0</strong> = lineáris lecsengés. <strong>> 1.0</strong> (pl. 2.0) = gyors esés az elején, lassul később. <strong>< 1.0</strong> (pl. 0.5) = lassú esés az elején, gyorsabb a végén. Csak polynomialnál számít. Az 1.0 alapból a legtöbb esetben jó.",
                "total_steps": "Összes lépés",
                "total_steps_help": "A tréning lépések teljes száma.",
                "gradient_accumulation_steps": "Gradiens felhalmozás lépései",
                "gradient_accumulation_steps_help": "Hány lépésen keresztül halmozza a gradienseket.",
                "ema_unet": "EMA az UNet-hez",
                "ema_unet_help": "Futó átlagot tart az UNet súlyaiból tréning közben. Ahelyett, hogy az utolsó lépés „nyers” súlyait használnád (ami zajos lehet), az EMA egy kisimított verziót ad, ami általában szebb és megbízhatóbban generalizál. Gondolj rá úgy, mint zajszűrés a súlyokon — az EMA checkpoint sokszor jobb a nyersnél. A bökkenő: kb. duplázza az UNet súlyok VRAM-igényét (mindkettőt tárolod). A simítás mértékét a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_decay\" data-xref-label=\"EMA Decay\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA Decay</a> vezérli. Hosszabb tréningeknél erősen ajánlott, ha a VRAM-od bírja.",
                "ema_te": "EMA a Text Encoderhez",
                "ema_te_help": "Ugyanaz az ötlet, mint a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">UNet EMA</a>, csak a text encoderre. Kevésbé kritikus, mert a text encoder súlyai LoRA tréningnél általában finomabban mozdulnak. Ettől még extra VRAM kell az extra súlykészlethez. Kapcsold be, ha tényleg tréningelsz text encodert és a lehető legjobb minőséget akarod.",
                "ema_decay": "EMA lecsengés",
                "ema_decay_help": "Mennyire „emlékezzen” az EMA a régi súlyokra az újakkal szemben: <code>ema = decay × ema + (1-decay) × current</code>. <strong>Magas (0.999–0.9999):</strong> több simítás, lassabban reagál a változásokra. Jó hosszú tréningekhez. <strong>Alacsony (0.99–0.995):</strong> kevesebb simítás, gyorsabban követi az új súlyokat. Rövid futásokhoz jobb. A <strong>0.995</strong> tipikus LoRA tréninghez (száz–pár ezer lépés) gyakran jól működik. Tízezres lépésszám? Próbáld a <strong>0.9999</strong>-et. Csak akkor számít, ha a <a href=\"#\" class=\"xref-link\" data-xref=\"ema_unet_checkbox\" data-xref-label=\"EMA for UNet\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA az UNet-hez</a> vagy az <a href=\"#\" class=\"xref-link\" data-xref=\"ema_text_encoder_checkbox\" data-xref-label=\"EMA for Text Encoder\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">EMA a Text Encoderhez</a> be van kapcsolva.",
                "mixed_precision": "Vegyes precizitás",
                "mixed_precision_help": "Használjon-e vegyes precizitású tréninget (FP16/BF16).",
                "mixed_precision_no": "no",
                "mixed_precision_fp16": "fp16",
                "mixed_precision_bf16": "bf16",
                "fp16_opt_level": "FP16 optimalizációs szint",
                "fp16_opt_level_help": "Az FP16 tréning optimalizációs szintje.",
                "loss_scale": "Loss scale",
                "loss_scale_help": "A loss scaling értéke FP16 tréninghez.",
                "clip_grad": "Gradiens vágás",
                "clip_grad_help": "Vágja-e a gradienseket tréning közben.",
                "log_interval": "Logolási gyakoriság",
                "log_interval_help": "Hány lépésenként logolja a tréning előrehaladását.",
                "save_interval": "Mentési gyakoriság",
                "save_interval_help": "Hány lépésenként mentsen checkpointot.",
                "resume_from_checkpoint": "Folytatás checkpointból",
                "resume_from_checkpoint_help": "Folytassa-e a tréninget a legutóbbi checkpointból.",
                "checkpoint_path": "Checkpoint útvonal",
                "checkpoint_path_placeholder": "pl. path/to/checkpoint",
                "checkpoint_path_help": "A checkpoint fájl útvonala, ahonnan folytatod a tréninget.",
                "use_8bit_adam": "8-bites Adam használata",
                "use_8bit_adam_help": "Használja-e a 8-bites Adam optimalizert.",
                "beta1": "Beta 1",
                "beta1_help": "A beta1 paraméter az Adam optimalizerhez.",
                "beta2": "Beta 2",
                "beta2_help": "A beta2 paraméter az Adam optimalizerhez.",
                "epsilon": "Epsilon",
                "epsilon_help": "Az epsilon paraméter az Adam optimalizerhez.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Használja-e az Adam optimalizer AMSGrad változatát.",
                "weight_decay": "Weight Decay",
                "weight_decay_help": "A weight decay (L2 büntetés) értéke az optimizerhez.",
                "max_grad_norm": "Max Grad Norm",
                "max_grad_norm_help": "A gradiensvágás maximum normája.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "A beta1 paraméter az Adam optimalizerhez.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "A beta2 paraméter az Adam optimalizerhez.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "Az epsilon paraméter az Adam optimalizerhez."
            },
            "advanced": {
                "attention": "Attention backend",
                "attention_help": "Melyik implementáció kezelje az attention mechanizmust — ez a transformerek leginkább memóriaéhes része. <strong>SDPA:</strong> PyTorch beépített, optimalizált attentionje (PyTorch 2.0+ kell). Jó balansz sebesség és memória között, extra telepítés nélkül. Ajánlott alap. <strong>xFormers:</strong> a Meta attention könyvtára, NVIDIA kártyákon gyakran a leggyorsabb. A „sima” attentionhöz képest 20–40%-kal is vághat a VRAM-on. Külön telepítés kell hozzá. <strong>None:</strong> sima PyTorch attention — többet eszik, de mindenhol működik. Csak akkor válts erre, ha az SDPA vagy xFormers problémázik. Az SDPA és az xFormers is jól együttműködik az <a href=\"#\" class=\"xref-link\" data-xref=\"enable_aggressive_memory_saving\" data-xref-label=\"Aggressive Memory Saving\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Agresszív memóriatakarékossággal</a>.",
                "use_xformers": "xFormers használata",
                "use_xformers_help": "Használja-e az xFormerst hatékonyabb attention számításhoz.",
                "xformers_memory_efficient": "Memóriatakarékos attention",
                "xformers_memory_efficient_help": "Használja-e az xFormers memóriatakarékos attention módját.",
                "gradient_checkpointing": "Gradiens checkpointing",
                "gradient_checkpointing_help": "Használja-e a gradiens checkpointinget memória spórolásra.",
                "grad_acc": "Gradiens felhalmozás",
                "grad_acc_help": "Több passzon keresztül halmozza a gradienseket, mielőtt frissítené a súlyokat — gyakorlatilag nagyobb batch-et „szimulál”. A <strong>hatékony batch = batch_size × gradient_accumulation</strong>. Tehát batch_size=2 és accumulation=4 olyan, mintha batch_size=8 lenne. Ez életmentő VRAM-szűk gépeken: nem fér be a batch=8? Csináld batch=2-vel és accumulation=4-gyel. A nagyobb effektív batch stabilabb tréninget ad, de lehet, hogy állítanod kell a <a href=\"#\" class=\"xref-link\" data-xref=\"learning_rate\" data-xref-label=\"learning rate\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-basic\">tanulási rátán</a>. Hátrány: arányosan lassít (4× accumulation = kb. 4× lassabb effektív lépésenként).",
                "mixed_precision": "Vegyes precizitás",
                "mixed_precision_help": "Számítási precizitás — közvetlenül hat a VRAM-ra és a sebességre. <strong>fp16:</strong> félprecíziós float, kb. fele memória a teljes precízióhoz képest. Jó választás GTX 10-es és RTX 20-as szérián, de SDXL-nél néha NaN-okat dobhat az fp16 korlátozott tartománya miatt. <strong>bf16:</strong> szintén 16 bit, de jobb kitevő-tartománnyal — sokkal stabilabb. RTX 30-as és újabb kártyán ezt érdemes használni. Régebbi hardveren nem megy. <strong>fp8:</strong> 8-bit, brutál VRAM spórolás. Kísérleti, ronthat minőséget, speciális hardver kell. <strong>no:</strong> teljes fp32. 2× VRAM, de numerikusan betonstabil. Csak NaN hibák debugolásához vagy ha van VRAM-od bőven.",
                "mixed_precision_training": "Vegyes precizitású tréning",
                "mixed_precision_training_help": "Használjon-e vegyes precizitású tréninget (FP16/BF16).",
                "tf32": "TF32 engedélyezése",
                "tf32_help": "TensorFloat-32 — számítási mód, ami csak RTX 30-as és újabb kártyákon van. fp32 tartományt ad, de kevesebb mantissza pontossággal (10 bit 23 helyett), így egyes műveleteknél akár 8× gyorsulást hozhat minimális minőségkülönbséggel. <strong>RTX 30/40 szérián hagyd bekapcsolva.</strong> Régebbi GPU-n (GTX 10, RTX 20) nem csinál semmit, mert nincs hozzá hardver. Csak akkor kapcsold ki, ha valamiért tényleg gyanús, hogy ront (nagyon ritka).",
                "vae_batch": "VAE batch méret",
                "vae_batch_help": "Külön batch méret csak a VAE kódoláshoz — amikor a képek latent térbe kerülnek, mielőtt az UNet látná őket. Ha ezt kisebbre állítod, mint a fő batch-ed, le tudja simítani a VRAM tüskéket kódolás közben. Hagyd üresen, ha egyezzen a tréning batch-csel. <strong>1</strong> = minimális VRAM, cserébe lassabb kódolás. Hasznos, ha kifejezetten a latent kódolási fázisban OOM-olsz.",
                "vae_batch_placeholder": "Auto",
                "max_token_length": "Max tokenhossz",
                "max_token_length_help": "A captionökben engedett maximum token (kb. szavak/al-szavak). A CLIP natív kontextusa <strong>77 token</strong> (75 használható + start/end). <strong>75:</strong> standard, a legtöbb captionnel oké. <strong>150:</strong> több hely részletes leírásra — jó komplex jelenetekhez vagy részletes karakterleíráshoz. <strong>225:</strong> nagyon hosszú, extra részletes captionök. Jelentősen több VRAM-ot eszik és lassít. A magasabb limitek caption chunking trükkökkel működnek. Emeld <strong>150</strong>-re vagy <strong>225</strong>-re, ha a captionjeid gyakran meghaladják a ~60–70 szót és a levágás fontos infót dob ki.",
                "memory_saving": "Agresszív memóriatakarékosság",
                "memory_saving_help": "Mindent bevet VRAM spórolásra: gradiens checkpointing, CPU offload, agresszív garbage collection. <strong>4–8GB-os kártyákon kötelező</strong> — olyan hardveren is lehetővé teheti az SDXL tréninget, ami amúgy megfulladna. Modelltől függően 30–50% VRAM-ot is spórolhat. Az ára: a tréning 2–4× lassabb lehet az újraszámolás és a CPU↔GPU pakolgatás miatt. Érdemes egy jó <a href=\"#\" class=\"xref-link\" data-xref=\"attention_backend\" data-xref-label=\"Attention Backend\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Attention backenddel</a> (SDPA vagy xFormers) párosítani. 16GB+ VRAM? Hagyd kikapcsolva és élvezd a gyorsabb tréninget.",
                "seed": "Tréning seed",
                "seed_help": "A tréningben használt véletlen generátorok seedje. Ha fix seedet adsz meg, kapsz <strong>reprodukálhatóságot</strong>: ugyanaz a config + ugyanaz a seed = azonos eredmény (azonos hardver/szoftver mellett). Hagyd üresen a random induláshoz — mindennapi tréninghez teljesen jó. Hasznos A/B tesztekhez, bugvadászathoz, vagy ha megosztható, ismételhető configot akarsz. Sokan emlékezetes számokat választanak, pl. <strong>42</strong>, <strong>1234</strong> vagy <strong>0</strong>.",
                "seed_placeholder": "Véletlen",
                "fp16_opt_level": "FP16 optimalizációs szint",
                "fp16_opt_level_help": "Az FP16 tréning optimalizációs szintje.",
                "loss_scale": "Loss scale",
                "loss_scale_help": "A loss scaling értéke FP16 tréninghez.",
                "adam_beta1": "Adam Beta1",
                "adam_beta1_help": "A beta1 paraméter az Adam optimalizerhez.",
                "adam_beta2": "Adam Beta2",
                "adam_beta2_help": "A beta2 paraméter az Adam optimalizerhez.",
                "adam_epsilon": "Adam Epsilon",
                "adam_epsilon_help": "Az epsilon paraméter az Adam optimalizerhez.",
                "amsgrad": "AMSGrad",
                "amsgrad_help": "Használja-e az Adam optimalizer AMSGrad változatát.",
                "weight_decay": "Weight decay",
                "weight_decay_help": "A weight decay (L2 büntetés) értéke az optimizerhez.",
                "max_grad_norm": "Max grad norm",
                "max_grad_norm_help": "A gradiensvágás maximum normája.",
                "lr_scheduler": "LR ütemező",
                "lr_scheduler_help": "A használt tanulási ráta ütemező.",
                "warmup_steps": "Warmup lépések",
                "warmup_steps_help": "Hány lépésig fusson a tanulási ráta warmup.",
                "total_steps": "Összes lépés",
                "total_steps_help": "A tréning lépések teljes száma.",
                "gradient_accumulation_steps": "Gradiens felhalmozás lépései",
                "gradient_accumulation_steps_help": "Hány lépésen keresztül halmozza a gradienseket.",
                "clip_grad": "Gradiens vágás",
                "clip_grad_help": "Vágja-e a gradienseket tréning közben.",
                "log_interval": "Logolási gyakoriság",
                "log_interval_help": "Hány lépésenként logolja a tréning előrehaladását.",
                "save_interval": "Mentési gyakoriság",
                "save_interval_help": "Hány lépésenként mentsen checkpointot.",
                "resume_from_checkpoint": "Folytatás checkpointból",
                "resume_from_checkpoint_help": "Folytassa-e a tréninget a legutóbbi checkpointból.",
                "checkpoint_path": "Checkpoint útvonal",
                "checkpoint_path_placeholder": "pl. path/to/checkpoint",
                "checkpoint_path_help": "A checkpoint fájl útvonala, ahonnan folytatod a tréninget."
            },
            "flux": {
                "title": "Flux-specifikus beállítások",
                "t5xxl": "T5XXL modell",
                "t5xxl_help": "A T5XXL text encoder útvonala. Flux tréninghez kötelező, hogy a komplex szöveges leírásokat fel tudja dolgozni.",
                "ae": "Autoencoder (VAE)",
                "ae_help": "A Flux autoencoder útvonala. A pixel tér és a latent tér közti átalakítást végzi.",
                "clip_l": "CLIP-L modell",
                "clip_l_help": "A CLIP-L text encoder útvonala, Fluxnál a T5XXL mellett dolgozik a szöveg feldolgozásán.",
                "max_seq_len": "Max szekvenciahossz",
                "max_seq_len_help": "Max tokenhossz a T5 text encoderhez (Flux / SD3). Magasabb értéknél hosszabb prompt/caption fér bele, de nő a VRAM igény és lassul a tréning. Gyakori értékek: <strong>256</strong> (alap) vagy <strong>512</strong>.",
                "guidance_scale": "Guidance skála",
                "guidance_scale_help": "Classifier-free guidance Flux és SD3 tréninghez. Azt szabályozza, mennyire ragaszkodjon a modell a szöveghez tréning közben. <strong>3.5:</strong> a Stability AI ajánlott alapja SD3-hoz, Fluxhoz is nagyon jó. <strong>Alacsony (1–2):</strong> kreatívabb/változatosabb, de könnyebben elkalandozik a prompttól. <strong>Magas (5–7):</strong> szorosabb prompt-követés, de kicsit merev lehet. SD1.5/SDXL-nél ezt általában nem használják — ott a guidance jellemzően csak inference-nél játszik.",
                "discrete_flow_shift": "Diszkrét flow shift",
                "discrete_flow_shift_help": "A flow matching shift paramétert állítja. Nagyobb érték javíthat részletességen, de több sampling lépést igényelhet.",
                "model_prediction_type": "Predikció típusa",
                "model_prediction_type_help": "Megadja, mit prediktál a modell (pl. nyers zajt vagy sebességet). Fluxnál általában \u0027raw\u0027.",
                "split_mode": "Split mód",
                "split_mode_help": "Haladó memóriamenedzsment Fluxhoz. \u0027Symmetry\u0027 az alap; \u0027Asymmetry\u0027 spórolhat VRAM-ot kis teljesítmény-árral.",
                "train_blocks": "Tréningelt blokkok",
                "train_blocks_help": "Megadja, a Flux modell mely blokkjait tréningeld. Teljes finetune-hoz az \u0027all\u0027 a standard.",
                "weighting_scheme": "Súlyozási séma",
                "weighting_scheme_help": "Hogyan mintavételezze a timestep-eket flow-matching modelleknél (Flux, SD3). Ez dönti el, mely zajszintek kapjanak több figyelmet tréning közben. <strong>None (Uniform):</strong> minden zajszint egyenlő. Biztonságos alap, ha nincs külön igényed. <strong>Logit Normal:</strong> haranggörbe eloszlás, középpontja <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a>, szélessége <a href=\"#\" class=\"xref-link\" data-xref=\"logit_std\" data-xref-label=\"Logit Std\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Std</a>. Pontosan belőheted, mely timestep-ek számítsanak. <strong>Mode:</strong> erősen egy konkrét timestep köré fókuszál, a szórást a <a href=\"#\" class=\"xref-link\" data-xref=\"mode_scale\" data-xref-label=\"Mode Scale\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode Scale</a> vezérli. Ezek csak Flux/SD3 esetén számítanak — SD1.5/SDXL-nél (epsilon-pred modellek) nem csinálnak semmit.",
                "logit_mean": "Logit mean",
                "logit_mean_help": "Hol tetőzzön a logit-normal eloszlás, ha <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> súlyozást használsz. <strong>0.0:</strong> a középső timestep köré (t=0.5) kerül a fókusz. <strong>Negatív:</strong> zajosabb timestep-ek felé tol (durva szerkezet). <strong>Pozitív:</strong> tisztább timestep-ek felé tol (finom részletek). Az alapján hangold, a generálás mely szakasza fontos neked. Csak akkor számít, ha Weighting Scheme = \"Logit Normal\".",
                "logit_std": "Logit std",
                "logit_std_help": "Mennyire legyen szétterülve a logit-normal eloszlás <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Normal</a> súlyozásnál. <strong>Alacsony (0.5–0.8):</strong> szűk fókusz a <a href=\"#\" class=\"xref-link\" data-xref=\"logit_mean\" data-xref-label=\"Logit Mean\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Logit Mean</a> körül — tényleg rákattan pár timestep-re. <strong>Magas (1.5–2.0):</strong> jobban szór, közelít az egyenletes mintavételezéshez. <strong>1.0:</strong> kiegyensúlyozott alap. Csak akkor számít, ha Weighting Scheme = \"Logit Normal\".",
                "mode_scale": "Mode scale",
                "mode_scale_help": "Mennyire szorosan csoportosuljon a mode eloszlás, ha <a href=\"#\" class=\"xref-link\" data-xref=\"weighting_scheme\" data-xref-label=\"Weighting Scheme\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Mode</a> súlyozást használsz. <strong>Alacsony:</strong> több timestep között szór. <strong>Magas:</strong> jobban összecsomósodik a módusz körül. <strong>1.29:</strong> az eredeti SD3 paper értéke. Maradhat, hacsak nem kísérletezel. Csak akkor releváns, ha Weighting Scheme = \"Mode\"."
            },
            "timestep": {
                "title": "Timestep mintavételezés",
                "type": "Mintavételezés típusa",
                "type_help": "A timestep-ek mintavételezésének módja tréning közben. Fluxhoz a \u0027sigma_distribution\u0027 ajánlott.",
                "min": "Min timestep",
                "min_help": "A mintavételezés minimális timestepje.",
                "max": "Max timestep",
                "max_help": "A mintavételezés maximális timestepje.",
                "sampling_type": "Mintavételezés típusa",
                "sampling_type_help": "Hogyan ossza el a zajt a timestep-ek között. A \u0027sigma\u0027 modernebb és általában előnyösebb Flux/SD3-hoz.",
                "sampling_type_sigma": "sigma",
                "sampling_type_timestep": "timestep",
                "timestep_spacing": "Timestep távolság",
                "timestep_spacing_help": "Hogyan legyenek elosztva a timestep-ek a diffúziós folyamatban. A \u0027linspace\u0027 a standard.",
                "timestep_spacing_linspace": "linspace",
                "timestep_spacing_leading": "leading",
                "timestep_spacing_trailing": "trailing",
                "min_timestep": "Min timestep",
                "min_timestep_help": "Alsó korlát a timestep mintavételezéshez — a tréninget a zajosabb tartományban tartja. A timestep-ek 0-tól (tiszta kép) ~1000-ig (tiszta zaj) mennek. Ha pl. <strong>100</strong>-at adsz meg, a modell nem tréningel majd majdnem tiszta képeken. Jó például: - durva szerkezetre fókuszálni, amikor a részletek nem számítanak - konkrét zajtartományokat tesztelni - furcsa, specializált ütemezésekhez Hagyd üresen a normál, teljes tartományú tréninghez. A <a href=\"#\" class=\"xref-link\" data-xref=\"max_timestep\" data-xref-label=\"Max Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Max timestep</a>-pel együtt egyedi tartományt tudsz kijelölni.",
                "max_timestep": "Max timestep",
                "max_timestep_help": "Felső korlát a timestep mintavételezéshez — távol tartja a tréninget a legzajosabb szintektől. Ha pl. <strong>800</strong>-at adsz meg, kimaradnak a maximális zajú timestep-ek. Hasznos például: - finom részletek polírozásához, amikor a durva szerkezet már stabil - finetune-hoz úgy, hogy közben megőrizd az alapmodell magas-zaj viselkedését - kísérleti tréning megközelítésekhez Hagyd üresen a normál, teljes tartományú tréninghez. A <a href=\"#\" class=\"xref-link\" data-xref=\"min_timestep\" data-xref-label=\"Min Timestep\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min timestep</a>-pel együtt a zajütemezés bármely részhalmazán tréningelhetsz."
            },
            "caption": {
                "title": "Caption beállítások",
                "extension": "Caption kiterjesztés",
                "extension_help": "A caption fájlok kiterjesztése. Alapból <strong>.txt</strong> — tehát az \"image001.png\" a captionjét az \"image001.txt\" fájlban keresi. Bármilyen kiterjesztést használhatsz (.caption, .captions, .tags, stb.). A caption fájlnak ugyanabban a mappában kell lennie, mint a képnek. Állítsd át, ha a captionelő eszközöd mást ír ki.",
                "prefix": "Caption előtag",
                "prefix_help": "Szöveg, amit tréning közben minden caption elé beszúr.",
                "suffix": "Caption utótag",
                "suffix_help": "Szöveg, amit tréning közben minden caption végére hozzáad.",
                "dropout": "Caption dropout",
                "dropout_help": "Annak esélye, hogy tréning közben a teljes caption ki legyen dobva. Segít a classifier-free guidance-ben.",
                "token_warmup": "Token warmup",
                "token_warmup_help": "A tréning elején fokozatosan vezeti be a caption tokenjeit.",
                "weighted": "Súlyozott captionök",
                "weighted_help": "A1111-stílusú súlyokat olvas a captionökből, pl. \"(fontos dolog:1.3)\" vagy \"[kevésbé fontos:0.7]\". Így tréning közben kiemelhetsz vagy visszavehetsz bizonyos szavakat. <strong>Súly > 1.0:</strong> a modell jobban figyel azokra a tokenekre. <strong>Súly < 1.0:</strong> kevésbé foglalkozik velük. Remek arra, hogy finomhangold, mi számítson a captionjeidben. Kapcsold ki, ha a captionjeidben a zárójelek szó szerintiek és nem súlyozást jelentenek.",
                "caption_dropout_rate": "Caption dropout arány",
                "caption_dropout_rate_help": "Annak valószínűsége, hogy tréning közben eldob egy captiont. Segíthet, hogy a modell ne csak a captionökre támaszkodva tanuljon.",
                "caption_dropout_every_n_epochs": "Caption dropout N epochonként",
                "caption_dropout_every_n_epochs_help": "Milyen gyakran (epochokban) alkalmazza a caption dropoutot.",
                "caption_tag_dropout_rate": "Caption tag dropout arány",
                "caption_tag_dropout_rate_help": "Annak valószínűsége, hogy a captionen belül egyes tag-eket dobjon el tréning közben."
            },
            "loss": {
                "title": "Loss \u0026 optimalizáció",
                "optimization_title": "Loss optimalizáció",
                "loss_type": "Loss típusa",
                "loss_type_help": "Az a matematikai függvény, amivel kiszámolja a hibát a prediktált és a cél zaj között. Az \u0027l2\u0027 a standard; a \u0027huber\u0027 jobban bírja a kilógó értékeket.",
                "type": "Loss típusa",
                "type_help": "Az a matematikai függvény, amivel kiszámolja a különbséget a prediktált és a cél zaj között.",
                "loss_type_l2": "l2",
                "loss_type_l1": "l1",
                "loss_type_huber": "huber",
                "huber_delta": "Huber delta",
                "huber_delta_help": "Az a küszöb, ahol a Huber loss kvadratikusból lineárisra vált.",
                "huber_c": "Huber C",
                "huber_c_help": "Küszöb paraméter a Huber lossnál. Alacsonyabb értéknél jobban hasonlít L1-re.",
                "huber_schedule": "Huber ütemezés",
                "huber_schedule_help": "Hogyan változzon a Huber delta a tréning során.",
                "huber_schedule_constant": "constant",
                "huber_schedule_exponential": "exponential",
                "huber_schedule_snr": "snr",
                "min_snr_gamma": "Min-SNR gamma",
                "min_snr_gamma_help": "Az \"Efficient Diffusion Training via Min-SNR Weighting Strategy\" paperből — megoldja azt a gondot, hogy a modellek egyes timestep-eket túltanulnak, másokat meg ignorálnak. Nélküle a minőség hajlamos szakaszonként ingadozni a denoise folyamatban. A gamma érték (tipikusan <strong>5.0</strong>) adja az SNR clipping küszöbét. <strong>Alacsony (1–3):</strong> a nagy zajú timestep-eket hangsúlyozza (korai denoise, durva szerkezet). <strong>Magas (8–20):</strong> a kis zajú timestep-eket hangsúlyozza (végső részletek, polírozás). <strong>0:</strong> teljesen kikapcsolja a Min-SNR-t. Általában érdemes bekapcsolva hagyni — nagy minőségjavulás szinte nulla teljesítményárral.",
                "min_snr": "Min SNR gamma",
                "min_snr_help": "Minimum Signal-to-Noise Ratio gamma. A zajszintek alapján súlyozza a losst, ezzel stabilizálhatja a tréninget.",
                "ip_noise_gamma": "IP noise gamma",
                "ip_noise_gamma_help": "Az Input Perturbation Noise extra véletlen zajt ad a bemenetre tréning közben (regularizáció) — kb. mint a dropout, csak képekre. Segít, hogy a modell ne memorizáljon apró részleteket, és javítja a generalizációt, főleg kisebb dataseteknél. <strong>0.05:</strong> enyhe regularizáció, általában nem zavar be. <strong>0.1 felett:</strong> ronthat minőséget, ha túl sok zajt ad. Normál tréninghez maradhat <strong>0</strong>, vagy emeld kicsit, ha azt látod, hogy konkrét képeket memorizál a modelled koncepciók helyett. Független a <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a>-től — egymásra is pakolhatók.",
                "ip_noise": "IP noise gamma",
                "ip_noise_help": "Az Input Perturbation noise erőssége. Segíthet a túlsimítás ellen.",
                "ip_noise_gamma_random_range": "IP noise random tartomány",
                "ip_noise_gamma_random_range_help": "A hozzáadott véletlen zaj tartománya, ha IP noise gammát használsz.",
                "noise_offset": "Noise offset",
                "noise_offset_help": "Tréning közben egy apró, konstans eltolást ad a zajhoz, hogy a modell ténylegesen tudjon igazi feketét és igazi fehéret generálni a kimosott szürkék helyett. A standard diffúzió szenved a nagyon sötét/világos tartományokkal, mert a zajütemezés nem fedi jól. <strong>0.035–0.05:</strong> biztonságos alapok, javítják a sötét/világos kezelést mellékhatás nélkül. <strong>0.05–0.1:</strong> agresszívebb tolás a szélsőségek felé. <strong>0.1 felett:</strong> instabilizálhat, vagy színeltolódást okozhat. A legjobban a <a href=\"#\" class=\"xref-link\" data-xref=\"zero_terminal_snr_checkbox\" data-xref-label=\"Zero Terminal SNR\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Zero Terminal SNR</a>-rel együtt működik — kiegészítik egymást. Kifejezetten fontos, ha a datasetedben sok a sötét jelenet, éjszakai kép vagy nagy kontraszt.",
                "v_pred": "V-Pred jellegű loss",
                "v_pred_help": "Sebesség-predikciós (velocity) viselkedést kever a standard zaj-predikciós tréningbe. Eredetileg v-pred modellekből jön (SD 2.x depth/inpaint) — kisimíthatja a tréninget és csökkentheti az artifactokat. <strong>0:</strong> tiszta zaj-pred, a standard. <strong>1:</strong> teljes velocity pred mód. <strong>0.1–0.2:</strong> finom v-pred hatás úgy, hogy kompatibilis marad zaj-pred alapmodellekkel. Néha segít simább gradienssel és kevesebb artifacttal. Hagyd <strong>0</strong>-n, ha amúgy jól működik, vagy ha nem vagy benne biztos.",
                "scale_v_pred": "Scale V-Pred Loss",
                "zero_terminal_snr": "Zero Terminal SNR",
                "debiased": "Debiased Estimation",
                "masked": "Masked Loss (Alpha)",
                "advanced_title": "Haladó loss opciók",
                "advanced_help": "<strong>Zero Terminal SNR:</strong> úgy állítja a noise schedulert, hogy az utolsó timestep tiszta zaj legyen (SNR = 0), ami segít a valódi feketéknél. Fontos társa a <a href=\"#\" class=\"xref-link\" data-xref=\"noise_offset_input\" data-xref-label=\"Noise Offset\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Noise Offset</a>-nek — Auto módban automatikusan bekapcsol. Nélküle a noise offset önmagában nem feltétlen javítja teljesen a sötét képes problémákat. A \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" paperből.<br><strong>Debiased Estimation:</strong> javítja a timestep mintavételezés torzítását, amikor egyesek túl vannak mintavételezve. Újrasúlyozza a losst, hogy minden timestep egyenletesen tanuljon, így összességében jobb minőséget ad. A <a href=\"#\" class=\"xref-link\" data-xref=\"snr_gamma_input\" data-xref-label=\"Min-SNR Gamma\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">Min-SNR Gamma</a> mellett is működik, de más problémát céloz.<br><strong>Scale V-Pred Loss:</strong> normalizálja a v-prediction losst, hogy hasonló nagyságrendű legyen, mint a zaj-pred loss. Főleg v-pred alapmodelleknél (SD 2.x variánsok) vagy <a href=\"#\" class=\"xref-link\" data-xref=\"v_pred_like_loss\" data-xref-label=\"V-Pred Like Loss\" data-xref-tab=\"training\" data-xref-subtab=\"cfg-advanced\">V-Pred Like Loss</a> használatakor hasznos. Standard SD 1.5/SDXL-nél (epsilon-pred) keveset számít.<br><strong>Masked Loss (Alpha):</strong> PNG alpha csatornát használ tréning maszknak — a modell csak a nem átlátszó pixelekből tanul. Tökéletes karakter/tárgy tréninghez, ha ignorálni akarod a hátteret. <strong>PNG kell rendes alpha csatornával!</strong>",
                "presets": "Loss presetek",
                "presets_help": "Gyorsan alkalmazhatsz ajánlott loss beállításokat különböző tréning célokhoz.",
                "preset_default": "Default",
                "preset_balanced": "Balanced",
                "preset_quality": "Quality",
                "preset_dark_light": "Dark/Light"
            },
            "samples": {
                "title": "Mintaképek generálása",
                "prompts_title": "Minta promptok",
                "inference_title": "Inference beállítások",
                "schedule_title": "Generálási ütemezés",
                "generated_title": "Generált minták",
                "gallery_placeholder": "A generált minták tréning közben itt fognak megjelenni...",
                "prompts": "Minta promptok",
                "prompts_help": "Adj meg promptokat mintaképek generálásához tréning közben. Soronként egy prompt.",
                "neg_prompt": "Negatív prompt",
                "neg_prompt_help": "A mintageneráláshoz használt negatív prompt.",
                "every_n_epochs": "Minden N epoch",
                "every_n_epochs_help": "Generáljon mintákat minden N. epoch után.",
                "every_n_steps": "Minden N lépés",
                "every_n_steps_help": "Generáljon mintákat minden N. lépés után.",
                "sampler": "Sampler",
                "sampler_help": "A mintaképek generálásához használt sampler.",
                "steps": "Minta lépések",
                "steps_help": "Mintavételezési lépések száma képenként.",
                "inference_steps": "Inference lépések",
                "inference_steps_help": "Denoise lépések száma mintagenerálásnál.",
                "cfg_scale": "CFG skála",
                "cfg_scale_help": "Classifier Free Guidance skála mintageneráláshoz.",
                "width": "Szélesség",
                "width_help": "A mintakép szélessége.",
                "height": "Magasság",
                "height_help": "A mintakép magassága.",
                "num_images": "Képek promptonként",
                "num_images_help": "Hány képet generáljon promptonként.",
                "seed": "Seed",
                "seed_help": "Seed reprodukálható mintageneráláshoz. -1 = véletlen.",
                "format": "Minta formátum",
                "format_help": "A mentett minták fájlformátuma.",
                "sample_every_n_steps": "Minta minden N lépés",
                "sample_every_n_steps_help": "Validációs mintákat generál minden N lépésenként, hogy vizuális visszajelzést kapj a tréning előrehaladásáról. A túl gyakori mintagenerálás kicsit növelheti az össz tréningidőt.",
                "sample_prompt": "Minta prompt",
                "sample_prompt_help": "A validációs képek promptjai, amikkel megnézed, mennyire tanulta meg a modell a cél fogalmakat. Soronként egy prompt — sampling közben mindegyik külön kerül kirenderelésre.",
                "sample_negative_prompt": "Minta negatív prompt",
                "sample_negative_prompt_help": "A mintagenerálásnál használt negatív prompt, amivel elnyomsz nem kívánt artifactokat vagy stílusokat.",
                "sample_num_images": "Mintaképek száma",
                "sample_num_images_help": "Hány mintaképet generáljon minden mintavételezési alkalommal.",
                "sample_seed": "Minta seed",
                "sample_seed_help": "Seed érték a véletlen generátorhoz mintaképek készítésekor.",
                "sample_width": "Minta szélesség",
                "sample_width_help": "A generált mintaképek szélessége.",
                "sample_height": "Minta magasság",
                "sample_height_help": "A generált mintaképek magassága.",
                "sample_guidance_scale": "Minta guidance skála",
                "sample_guidance_scale_help": "A prompt erejét szabályozó guidance skála képgenerálás közben.",
                "sample_steps": "Minta lépések",
                "sample_steps_help": "Denoise lépések száma validációs minták generálásához. 20–30 lépés általában elég értékeléshez; magasabb érték javíthat minőségen, de lassítja a generálást.",
                "sample_eta": "Minta eta",
                "sample_eta_help": "Eta paraméter, ami a diffúziós folyamat véletlenszerűségét szabályozza képgenerálásnál.",
                "sample_scheduler": "Minta scheduler",
                "sample_scheduler_help": "A validációs képek generálásához használt sampler. Az `Euler a` általában jó kompromisszum gyorsaság és vizuális minőség között, de válaszd azt, ami a workflow-dhoz passzol.",
                "sample_lora": "Minta LoRA",
                "sample_lora_help": "A mintaképek generálásához használt LoRA modell.",
                "sample_lycoris": "Minta LyCORIS",
                "sample_lycoris_help": "A mintaképek generálásához használt LyCORIS modell.",
                "sample_oft": "Minta OFT",
                "sample_oft_help": "A mintaképek generálásához használt OFT modell.",
                "sample_use_8bit_adam": "Minta 8-bites Adam",
                "sample_use_8bit_adam_help": "Használja-e a 8-bites Adam optimalizert mintaképek generálásához.",
                "sample_fp16_opt_level": "Minta FP16 opt szint",
                "sample_fp16_opt_level_help": "FP16 optimalizációs szint mintaképek generálásához.",
                "sample_loss_scale": "Minta loss scale",
                "sample_loss_scale_help": "Loss scaling érték FP16-hoz mintaképek generálásánál.",
                "sample_adam_beta1": "Minta Adam Beta1",
                "sample_adam_beta1_help": "A beta1 paraméter az Adam optimalizerhez mintagenerálásnál.",
                "sample_adam_beta2": "Minta Adam Beta2",
                "sample_adam_beta2_help": "A beta2 paraméter az Adam optimalizerhez mintagenerálásnál.",
                "sample_adam_epsilon": "Minta Adam Epsilon",
                "sample_adam_epsilon_help": "Az epsilon paraméter az Adam optimalizerhez mintagenerálásnál.",
                "sample_amsgrad": "Minta AMSGrad",
                "sample_amsgrad_help": "Használja-e az AMSGrad változatot mintagenerálásnál.",
                "sample_weight_decay": "Minta weight decay",
                "sample_weight_decay_help": "A weight decay (L2 büntetés) értéke az optimizerhez mintagenerálásnál.",
                "sample_max_grad_norm": "Minta max grad norm",
                "sample_max_grad_norm_help": "A gradiensvágás maximum normája mintagenerálásnál.",
                "sample_lr_scheduler": "Minta LR ütemező",
                "sample_lr_scheduler_help": "A mintagenerálásnál használt tanulási ráta ütemező.",
                "sample_warmup_steps": "Minta warmup lépések",
                "sample_warmup_steps_help": "Warmup lépések száma mintagenerálásnál.",
                "sample_total_steps": "Minta összes lépés",
                "sample_total_steps_help": "Összes tréning lépés (mintagenerálás kontextusában)."
            },
            "metadata": {
                "title": "Modell metaadatok (metadata)",
                "title_label": "Cím",
                "title_help": "A LoRA nyilvános neve (title).",
                "author": "Szerző (author)",
                "author_help": "A neved.",
                "desc": "Leírás (description)",
                "desc_help": "Nyilvános leírás.",
                "license": "Licenc (license)",
                "license_help": "Felhasználási licenc.",
                "tags": "Címkék (tags)",
                "tags_help": "Keresési címkék / tag-ek.",
                "comment": "Tréning megjegyzés (comment)",
                "comment_help": "Privát megjegyzések, metaadatba mentve."
            },
            "progress": {
                "title": "Tréning állapota (progress)",
                "status": "Állapot",
                "eta": "ETA",
                "vram": "VRAM",
                "steps": "Lépések (steps)",
                "epochs": "Epochok (epochs)",
                "epoch_prefix": "Epoch",
                "loss": "Loss",
                "loss_label": "LOSS",
                "start": "Tréning indítása",
                "idle": "Rendszer tétlen",
                "btn_stop": "Tréning leállítása"
            },
            "conversion": {
                "title": "Modell konvertálás (conversion)",
                "subtitle": "LoRA modellek konvertálása formátumok között (Diffusers \u0026harr; Kohya/LDM)",
                "card_title": "Konvertáló eszköz",
                "help_text": "Ezzel az eszközzel kompatibilitási gondokat javíthatsz AUTOMATIC1111/Forge alatt. \"Diffusers\" stílusú kulcsokat alakít \"Kohya/LDM\" stílusú kulcsokra és vissza.",
                "input_model": "Bemeneti modell (.safetensors)",
                "btn_refresh": "Frissítés",
                "model_architecture": "Modell architektúra",
                "architecture_help": "Megadja azt az architektúrát (architecture), ami a helyes key mappinghez kell. Ha rosszat választasz, a kimeneti modell nem fog működni.",
                "target_format": "Cél formátum",
                "target_format_kohya": "Kohya / LDM (A1111, Forge, ComfyUI)",
                "output_filename": "Kimeneti fájlnév (opcionális)",
                "output_filename_placeholder": "Hagyd üresen az automatikus névhez (pl. model_converted.safetensors)",
                "btn_convert": "Modell konvertálása",
                "success": "Konvertálás sikeres!"
            }
        },
        "console": {
            "title": "Rendszerkonzol (console)",
            "subtitle": "Valós idejű tréning logok és rendszerkimenet",
            "output_title": "Konzol kimenet",
            "clear": "Konzol törlése",
            "copy": "Logok másolása",
            "auto_scroll": "Auto-görgetés (auto-scroll)",
            "wrap_lines": "Sortördelés (wrap lines)",
            "status": "Állapot",
            "step": "Lépés",
            "loss": "Loss",
            "eta": "ETA",
            "vram": "VRAM",
            "gpu_load": "GPU terhelés (GPU load)",
            "cpu_load": "CPU terhelés (CPU load)",
            "ram": "RAM",
            "waiting": "Várakozás a tréning indítására..."
        },
        "metadata_editor": {
            "title": "Metaadat szerkesztő (metadata editor)",
            "subtitle": "Beágyazott metadata kezelése LoRA és Checkpoint fájlokhoz",
            "select_file": "Fájl kiválasztása",
            "select_placeholder": "Válassz egy fájlt...",
            "select_help": "Válassz egy modellt vagy metadata fájlt, hogy megnézd és szerkeszd a belső metaadatait.",
            "btn_load": "Metaadat betöltése",
            "load_help": "Metaadat betöltése a kiválasztott fájlból szerkesztéshez.",
            "btn_save": "Metaadat mentése",
            "load_model": "Modell betöltése",
            "save_metadata": "Metaadat mentése",
            "clear_fields": "Mezők törlése"
        },
        "modals": {
            "save_preset_title": "Preset mentése",
            "save_preset_placeholder": "Add meg a preset nevét...",
            "btn_confirm": "OK",
            "btn_cancel": "Mégse",
            "new_preset_title": "Új preset",
            "new_preset_placeholder": "Add meg az új preset nevét...",
            "preset_title": "Preset",
            "preset_name": "Preset neve",
            "preset_placeholder": "Add meg a preset nevét...",
            "btn_create": "Preset létrehozása",
            "delete_preset_title": "Preset törlése",
            "delete_preset_confirm": "Biztosan törlöd ezt a presetet?",
            "delete_title": "Törlés megerősítése",
            "delete_confirm": "Biztosan törlöd:",
            "delete_warning": "Ez a művelet nem vonható vissza. A fájl véglegesen törlődik a lemezről.",
            "btn_delete": "Törlés",
            "stop_title": "Tréning leállítása",
            "stop_confirm": "Biztosan leállítod a tréninget?",
            "stop_warning": "Minden el nem mentett haladás elveszik. Az aktuális checkpoint nem lesz elmentve.",
            "btn_stop": "Leállítás"
        },
        "notifications": {
            "success": "Siker",
            "error": "Hiba",
            "warning": "Figyelmeztetés",
            "info": "Információ"
        },
        "common": {
            "app_title": "Onika Trainer",
            "auto": "Auto",
            "select_placeholder": "Válassz egy opciót...",
            "loading": "Betöltés...",
            "searching": "Keresés...",
            "no_results": "Nincs találat",
            "all": "Mind",
            "none": "Nincs"
        }
    }
}